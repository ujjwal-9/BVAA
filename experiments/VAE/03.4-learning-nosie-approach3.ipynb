{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from torchvision.utils import save_image\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from timeit import default_timer\n",
    "from collections import defaultdict\n",
    "from tqdm import trange\n",
    "\n",
    "class Trainer():\n",
    "    def __init__(self, model, optimizer, loss_f, device, save_dir=\"results\", is_progress_bar=True):\n",
    "            self.device = device\n",
    "            self.model = model.to(device)\n",
    "            self.loss_f = loss_f\n",
    "            self.optimizer = optimizer\n",
    "            self.save_dir = save_dir\n",
    "            self.is_progress_bar = is_progress_bar\n",
    "            \n",
    "    def __call__(self, data_loader, epochs=10, checkpoint_every=10):\n",
    "        start = default_timer()\n",
    "        self.model.train()\n",
    "        for epoch in range(epochs):\n",
    "            storer = defaultdict(list)\n",
    "            mean_epoch_loss = self._train_epoch(data_loader, storer, epoch)\n",
    "            mean_epoch_loss = self._test_epoch(data_loader, storer, epoch)\n",
    "            with torch.no_grad():\n",
    "                sample = torch.randn(64, self.model.latent_dim).to(device)\n",
    "                sample = self.model.decoder(sample).cpu()  # make sure on cpu\n",
    "                save_image(sample.view(64, 1, 32, 32),\n",
    "                           './results/samples/' + str(epoch) + '.png')\n",
    "            \n",
    "    def _train_epoch(self, data_loader, storer, epoch):\n",
    "        epoch_loss = 0.\n",
    "        kwargs = dict(desc=\"Epoch {}\".format(epoch + 1), leave=False,\n",
    "                      disable=not self.is_progress_bar)\n",
    "        with trange(len(data_loader), **kwargs) as t:\n",
    "            for _, (data, _) in enumerate(data_loader):\n",
    "                iter_loss = self._train_iteration(data, storer)\n",
    "                epoch_loss += iter_loss\n",
    "                t.set_postfix(loss=iter_loss)\n",
    "                t.update()\n",
    "        mean_epoch_loss = epoch_loss / len(data_loader)\n",
    "        return mean_epoch_loss\n",
    "    \n",
    "    def _train_iteration(self, data, storer):\n",
    "        batch_size, channel, height, width = data.size()\n",
    "        data = data.to(self.device)\n",
    "        recon_batch, latent_dist, latent_sample = self.model(data)\n",
    "        loss = self.loss_f(data, recon_batch, latent_dist, self.model.training, \n",
    "                           storer, latent_sample=latent_sample)\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "            \n",
    "        return loss.item()\n",
    "    \n",
    "    def _test_epoch(self, data_loader, storer, epoch):\n",
    "        epoch_loss = 0.\n",
    "        kwargs = dict(desc=\"Epoch {}\".format(epoch + 1), leave=False,\n",
    "                      disable=not self.is_progress_bar)\n",
    "        with trange(len(data_loader), **kwargs) as t:\n",
    "            for _, (data, _) in enumerate(data_loader):\n",
    "                iter_loss = self._train_iteration(data, storer)\n",
    "                epoch_loss += iter_loss\n",
    "                t.set_postfix(loss=iter_loss)\n",
    "                t.update()\n",
    "        mean_epoch_loss = epoch_loss / len(data_loader)\n",
    "        return mean_epoch_loss\n",
    "    \n",
    "    def _test_iteration(self, data, storer):\n",
    "        batch_size, channel, height, width = data.size()\n",
    "        data = data.to(self.device)\n",
    "        recon_batch, latent_dist, latent_sample = self.model(data)\n",
    "        loss = self.loss_f(data, recon_batch, latent_dist, self.model.training, \n",
    "                               storer, latent_sample=latent_sample)\n",
    "            \n",
    "        return loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOSSES = [\"betaH\", \"betaB\"]\n",
    "RECON_DIST = [\"bernoulli\", \"laplace\", \"gaussian\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "mnist_dataset = datasets.MNIST('/home/data/bvaa', \n",
    "                   train=True, \n",
    "                   download=True, \n",
    "                   transform=transforms.Compose([\n",
    "                       transforms.Resize(32),\n",
    "                       transforms.ToTensor()\n",
    "                   ]))\n",
    "\n",
    "mnist_dataset_test = datasets.MNIST('/home/data/bvaa', train=False, download=True, transform=transforms.Compose([\n",
    "                       transforms.Resize(32),\n",
    "                       transforms.ToTensor()\n",
    "                   ]))\n",
    "\n",
    "train_loader = DataLoader(mnist_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "test_loader = DataLoader(mnist_dataset_test, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = enumerate(test_loader)\n",
    "batch_idx, (example_data, example_targets) = next(examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vae import VAE\n",
    "from encoder import Encoder\n",
    "from decoder import Decoder\n",
    "from losses import get_loss_fn\n",
    "from torch import optim\n",
    "\n",
    "latent_dim = 12\n",
    "img_size = [1,32,32]\n",
    "\n",
    "lr = 5e-4\n",
    "\n",
    "betaB_args = {\"rec_dist\": \"bernoulli\",\n",
    "              \"reg_anneal\": 10000, \n",
    "              \"betaH_B\": 4,\n",
    "              \"betaB_initC\": 0,\n",
    "              \"betaB_finC\": 25,\n",
    "              \"betaB_G\": 100\n",
    "             }\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "loss_f = get_loss_fn(\"betaB\", n_data=len(train_loader.dataset), device=device, **betaB_args)\n",
    "\n",
    "encoder = Encoder(img_size, latent_dim)\n",
    "decoder = Decoder(img_size, latent_dim)\n",
    "\n",
    "generator_model = VAE(img_size, latent_dim, encoder, decoder).to(device)\n",
    "optimizer = optim.Adam(generator_model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer = Trainer(generator_model, optimizer, loss_f, device, logger=logger,\\\n",
    "#                   save_dir=exp_dir, is_progress_bar=False)\n",
    "# epochs = 100\n",
    "# checkpoint_every = 10\n",
    "# trainer(train_loader, epochs=epochs, checkpoint_every=checkpoint_every)\n",
    "# torch.save(trainer.model.state_dict(), 'test.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generator_model.load_state_dict(torch.load('models/test.pt'))\n",
    "# generator_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Classifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Classifier, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, 3, 1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, 1)\n",
    "        self.dropout1 = nn.Dropout2d(0.25)\n",
    "        self.dropout2 = nn.Dropout2d(0.5)\n",
    "        self.fc1 = nn.Linear(9216, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = self.dropout1(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.fc2(x)\n",
    "        output = F.softmax(x, dim=1)\n",
    "        return output\n",
    "\n",
    "classifier = Classifier()\n",
    "classifier.load_state_dict(torch.load('models/mnist_cnn_non_log.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AddNoise():\n",
    "    def __init__(self, shape, device, noise=None, percent_noise=0.1):\n",
    "        self.device = device\n",
    "        self.percent_noise = percent_noise\n",
    "        if noise is None:\n",
    "            self.noise = torch.randn(shape)\n",
    "        else:\n",
    "            self.noise = noise\n",
    "\n",
    "    def noisy(self, data):\n",
    "        x = self.noise.to(self.device) * self.percent_noise + data * (1 - self.percent_noise)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Noise for Testing: \n",
      " tensor([[ 0.7344,  0.0049,  0.1108, -0.9742,  0.7806,  0.4000, -0.6167, -1.0133,\n",
      "          0.4483, -0.2564, -0.7429,  0.7341]])\n"
     ]
    }
   ],
   "source": [
    "sample_noise = torch.randn(torch.Size([1, 12]))\n",
    "backup_sample_noise = sample_noise\n",
    "print(\"Sample Noise for Testing: \\n\", sample_noise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Decoder(\n",
       "  (lin1): Linear(in_features=12, out_features=256, bias=True)\n",
       "  (lin2): Linear(in_features=256, out_features=256, bias=True)\n",
       "  (lin3): Linear(in_features=256, out_features=512, bias=True)\n",
       "  (convT1): ConvTranspose2d(32, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "  (convT2): ConvTranspose2d(32, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "  (convT3): ConvTranspose2d(32, 1, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder = generator_model.decoder\n",
    "decoder.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Classifier(\n",
       "  (conv1): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (dropout1): Dropout2d(p=0.25, inplace=False)\n",
       "  (dropout2): Dropout2d(p=0.5, inplace=False)\n",
       "  (fc1): Linear(in_features=9216, out_features=128, bias=True)\n",
       "  (fc2): Linear(in_features=128, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import exp\n",
    "\n",
    "def gaussian(window_size, sigma):\n",
    "    gauss = torch.Tensor([exp(-(x - window_size//2)**2/float(2*sigma**2)) for x in range(window_size)])\n",
    "    return gauss/gauss.sum()\n",
    "\n",
    "def create_window(window_size, channel):\n",
    "    _1D_window = gaussian(window_size, 1.5).unsqueeze(1)\n",
    "    _2D_window = _1D_window.mm(_1D_window.t()).float().unsqueeze(0).unsqueeze(0)\n",
    "    window = torch.Tensor(_2D_window.expand(channel, 1, window_size, window_size).contiguous())\n",
    "    return window\n",
    "\n",
    "def _ssim(img1, img2, window, window_size, channel, size_average = True):\n",
    "    mu1 = F.conv2d(img1, window, padding = window_size//2, groups = channel)\n",
    "    mu2 = F.conv2d(img2, window, padding = window_size//2, groups = channel)\n",
    "\n",
    "    mu1_sq = mu1.pow(2)\n",
    "    mu2_sq = mu2.pow(2)\n",
    "    mu1_mu2 = mu1*mu2\n",
    "\n",
    "    sigma1_sq = F.conv2d(img1*img1, window, padding = window_size//2, groups = channel) - mu1_sq\n",
    "    sigma2_sq = F.conv2d(img2*img2, window, padding = window_size//2, groups = channel) - mu2_sq\n",
    "    sigma12 = F.conv2d(img1*img2, window, padding = window_size//2, groups = channel) - mu1_mu2\n",
    "\n",
    "    C1 = 0.01**2\n",
    "    C2 = 0.03**2\n",
    "\n",
    "    ssim_map = ((2*mu1_mu2 + C1)*(2*sigma12 + C2))/((mu1_sq + mu2_sq + C1)*(sigma1_sq + sigma2_sq + C2))\n",
    "\n",
    "    if size_average:\n",
    "        return ssim_map.mean()\n",
    "    else:\n",
    "        return ssim_map.mean(1).mean(1).mean(1)\n",
    "\n",
    "class SSIM(nn.Module):\n",
    "    def __init__(self, window_size = 11, size_average = True):\n",
    "        super(SSIM, self).__init__()\n",
    "        self.window_size = window_size\n",
    "        self.size_average = size_average\n",
    "        self.channel = 1\n",
    "        self.window = create_window(window_size, self.channel)\n",
    "\n",
    "    def forward(self, img1, img2):\n",
    "        (_, channel, _, _) = img1.size()\n",
    "\n",
    "        if channel == self.channel and self.window.data.type() == img1.data.type():\n",
    "            window = self.window\n",
    "        else:\n",
    "            window = create_window(self.window_size, channel)\n",
    "            \n",
    "            if img1.is_cuda:\n",
    "                window = window.cuda(img1.get_device())\n",
    "            window = window.type_as(img1)\n",
    "            \n",
    "            self.window = window\n",
    "            self.channel = channel\n",
    "\n",
    "\n",
    "        return _ssim(img1, img2, window, self.window_size, channel, self.size_average)\n",
    "\n",
    "def ssim(img1, img2, window_size = 11, size_average = True):\n",
    "    (_, channel, _, _) = img1.size()\n",
    "    window = create_window(window_size, channel)\n",
    "    \n",
    "    if img1.is_cuda:\n",
    "        window = window.cuda(img1.get_device())\n",
    "    window = window.type_as(img1)\n",
    "    \n",
    "    return _ssim(img1, img2, window, window_size, channel, size_average)\n",
    "\n",
    "ssim_loss = SSIM(window_size = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constrained Translator\n",
    "class Translator(nn.Module):\n",
    "    def __init__(self, hidden_layers=[5, 10, 5], latent_dim=latent_dim):\n",
    "        super(Translator, self).__init__()\n",
    "        self.hidden_layers = hidden_layers\n",
    "        self.hidden_layers.insert(0, latent_dim)\n",
    "        self.hidden_layers.append(latent_dim)\n",
    "        self.layers = []\n",
    "        \n",
    "        for i in range(len(self.hidden_layers)-1):\n",
    "            self.layers.append(nn.Linear(self.hidden_layers[i], self.hidden_layers[i+1]))\n",
    "        \n",
    "        self.layers = nn.ModuleList(self.layers)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "translator = Translator().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from skimage.metrics import structural_similarity as ssim\n",
    "\n",
    "def create_logits(target_label, pred, confidence=0.5, device=device):\n",
    "    logits = torch.zeros(pred.shape, dtype=torch.float64)\n",
    "    pred_labels = [int(element.item()) for element in torch.argmax(pred, dim=1)]\n",
    "#     print(\"length\", len(pred_labels))\n",
    "#     print(\"pred_lables:\",pred_labels)\n",
    "#     print(logits.shape)\n",
    "    logits[range(logits.shape[0]), pred_labels] = torch.DoubleTensor([1-confidence]*pred.shape[0])\n",
    "    logits[range(logits.shape[0]), [target_label]*pred.shape[0]] += torch.DoubleTensor([confidence]*pred.shape[0])\n",
    "    return logits.to(device)\n",
    "\n",
    "def structural(org_image, noised_image):\n",
    "    batch_size, channels, width, height = org_image.shape\n",
    "    loss1 = 0\n",
    "    for b_ in range(batch_size):\n",
    "        ch_loss = 0\n",
    "        for ch_ in range(channels):\n",
    "            ch_loss += 1-ssim(org_image[b_][ch_].detach().cpu().numpy(), noised_image[b_][ch_].detach().cpu().numpy())\n",
    "        loss1 += ch_loss/channels\n",
    "    return loss1\n",
    "            \n",
    "class T_Loss(nn.Module):\n",
    "    def __init__(self, decoder=decoder, classifier=classifier):\n",
    "        super(T_Loss, self).__init__()\n",
    "        self.decoder = decoder\n",
    "        self.classifier = classifier\n",
    "    def forward(self, x, org_x, target_label):\n",
    "        \n",
    "        org_image = self.decoder(org_x)\n",
    "        noised_image = self.decoder(x)\n",
    "        batch_size, channels, width, height = org_image.shape\n",
    "        loss1 = ssim_loss(org_image, noised_image)\n",
    "#         print(type(loss1))\n",
    "#         print(loss1)\n",
    "#         loss1 = torch.Tensor(loss1 / batch_size).to(device)\n",
    "#         print(\"loss1:\", loss1)\n",
    "        preds = self.classifier(F.upsample(noised_image, (28,28), mode='bilinear', align_corners=True))\n",
    "#         print(\"preds:\",preds)\n",
    "        target = create_logits(target_label, preds)\n",
    "#         print(target)\n",
    "        loss2 = nn.BCELoss(reduction='sum')(preds, target.float())\n",
    "#         print(type(loss2))\n",
    "#         print(\"loss1:\",100*(1-loss1))\n",
    "#         print(\"loss2:\",loss2)\n",
    "        loss = 100*(1-loss1) + loss2\n",
    "        \n",
    "        \n",
    "        out_labels = preds.argmax(dim=1, keepdim=True)\n",
    "#         print(out_labels)\n",
    "#         print(torch.empty(out_labels.shape).fill_(target_label))\n",
    "#         print(preds)\n",
    "#         correct = out_labels.eq(torch.Tensor([target_label]*out_labels.shape[0]).to(device)).sum()\n",
    "        correct = out_labels.eq(torch.empty(out_labels.shape).fill_(target_label).to(device)).sum()\n",
    "#         print(out_labels.shape)\n",
    "#         print(correct)\n",
    "#         print(torch.Tensor([target_label]*out_labels.shape[0]))\n",
    "#         print(out_labels)\n",
    "        return loss, correct.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "T_Loss(\n",
       "  (decoder): Decoder(\n",
       "    (lin1): Linear(in_features=12, out_features=256, bias=True)\n",
       "    (lin2): Linear(in_features=256, out_features=256, bias=True)\n",
       "    (lin3): Linear(in_features=256, out_features=512, bias=True)\n",
       "    (convT1): ConvTranspose2d(32, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (convT2): ConvTranspose2d(32, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (convT3): ConvTranspose2d(32, 1, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "  )\n",
       "  (classifier): Classifier(\n",
       "    (conv1): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (dropout1): Dropout2d(p=0.25, inplace=False)\n",
       "    (dropout2): Dropout2d(p=0.5, inplace=False)\n",
       "    (fc1): Linear(in_features=9216, out_features=128, bias=True)\n",
       "    (fc2): Linear(in_features=128, out_features=10, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tloss = T_Loss().to(device)\n",
    "tloss.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "938"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d70d68709ddd4bed96889b737b3f10eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=10.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/bvaa/lib/python3.6/site-packages/torch/nn/functional.py:2416: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.\n",
      "  warnings.warn(\"nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0\tLoss: 105.201706\tCorrect: 45749\n",
      "Train Epoch: 1\tLoss: 190.470649\tCorrect: 20176\n",
      "Train Epoch: 2\tLoss: 225.782646\tCorrect: 783\n",
      "Train Epoch: 3\tLoss: 171.117668\tCorrect: 921\n",
      "Train Epoch: 4\tLoss: 161.041887\tCorrect: 123\n",
      "Train Epoch: 5\tLoss: 148.596235\tCorrect: 14\n",
      "Train Epoch: 6\tLoss: 144.444655\tCorrect: 18164\n",
      "Train Epoch: 7\tLoss: 152.536515\tCorrect: 13960\n",
      "Train Epoch: 8\tLoss: 177.085123\tCorrect: 0\n",
      "Train Epoch: 9\tLoss: 177.030859\tCorrect: 4\n",
      "\n"
     ]
    }
   ],
   "source": [
    "attack_log_interval = 1\n",
    "alt_target = 2\n",
    "translator.train()\n",
    "optimizer = optim.Adam(translator.parameters(), lr=1e-3)\n",
    "for epoch in tqdm(range(10)):\n",
    "    epoch_loss = 0\n",
    "    epoch_correct = 0\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data = torch.FloatTensor(data).to(device)\n",
    "        \n",
    "        _, l_dist, l_sample = generator_model(data)\n",
    "        \n",
    "        noised_sample = translator(l_sample)\n",
    "        \n",
    "        loss, correct = tloss(noised_sample, l_sample, alt_target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "#         print(correct)\n",
    "        epoch_loss += loss.item()\n",
    "        epoch_correct += correct\n",
    "        \n",
    "    if (epoch+1) % attack_log_interval == 0:\n",
    "        print('Train Epoch: {}\\tLoss: {:.6f}\\tCorrect: {}'.format(\n",
    "            epoch, epoch_loss/batch_idx, epoch_correct))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = enumerate(test_loader)\n",
    "batch_idx, (example_data, example_targets) = next(examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(i):\n",
    "    _, l_dist, l_sample = generator_model(example_data[i].unsqueeze_(0).to(device))\n",
    "    noised_sample = translator(l_sample)\n",
    "    noised_sample = 0.25 * l_sample + 0.75 * (noised_sample - noised_sample.min())/(noised_sample.max() - noised_sample.min())\n",
    "    final = generator_model.decoder(noised_sample)\n",
    "    pred = torch.argmax(classifier(F.upsample(final, (28,28), mode='bilinear', align_corners=True)))\n",
    "    print(\"Prediction: \", pred.item())\n",
    "    plt.figure(figsize=(10,10))\n",
    "    plt.subplot(1,2,1)\n",
    "    plt.imshow(example_data[i][0].detach().cpu().numpy(), cmap='gray', interpolation='none')\n",
    "    plt.subplot(1,2,2)\n",
    "    plt.imshow(final[0][0].detach().cpu().numpy(), cmap='gray', interpolation='none')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction:  0\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "test(19)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
