{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from torchvision.utils import save_image\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from timeit import default_timer\n",
    "from collections import defaultdict\n",
    "from tqdm import trange\n",
    "\n",
    "class Trainer():\n",
    "    def __init__(self, model, optimizer, loss_f, device, save_dir=\"results\", is_progress_bar=True):\n",
    "            self.device = device\n",
    "            self.model = model.to(device)\n",
    "            self.loss_f = loss_f\n",
    "            self.optimizer = optimizer\n",
    "            self.save_dir = save_dir\n",
    "            self.is_progress_bar = is_progress_bar\n",
    "            \n",
    "    def __call__(self, data_loader, epochs=10, checkpoint_every=10):\n",
    "        start = default_timer()\n",
    "        self.model.train()\n",
    "        for epoch in range(epochs):\n",
    "            storer = defaultdict(list)\n",
    "            mean_epoch_loss = self._train_epoch(data_loader, storer, epoch)\n",
    "            mean_epoch_loss = self._test_epoch(data_loader, storer, epoch)\n",
    "            with torch.no_grad():\n",
    "                sample = torch.randn(64, self.model.latent_dim).to(device)\n",
    "                sample = self.model.decoder(sample).cpu()  # make sure on cpu\n",
    "                save_image(sample.view(64, 1, 32, 32),\n",
    "                           './results/samples/' + str(epoch) + '.png')\n",
    "            \n",
    "    def _train_epoch(self, data_loader, storer, epoch):\n",
    "        epoch_loss = 0.\n",
    "        kwargs = dict(desc=\"Epoch {}\".format(epoch + 1), leave=False,\n",
    "                      disable=not self.is_progress_bar)\n",
    "        with trange(len(data_loader), **kwargs) as t:\n",
    "            for _, (data, _) in enumerate(data_loader):\n",
    "                iter_loss = self._train_iteration(data, storer)\n",
    "                epoch_loss += iter_loss\n",
    "                t.set_postfix(loss=iter_loss)\n",
    "                t.update()\n",
    "        mean_epoch_loss = epoch_loss / len(data_loader)\n",
    "        return mean_epoch_loss\n",
    "    \n",
    "    def _train_iteration(self, data, storer):\n",
    "        batch_size, channel, height, width = data.size()\n",
    "        data = data.to(self.device)\n",
    "        recon_batch, latent_dist, latent_sample = self.model(data)\n",
    "        loss = self.loss_f(data, recon_batch, latent_dist, self.model.training, \n",
    "                           storer, latent_sample=latent_sample)\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "            \n",
    "        return loss.item()\n",
    "    \n",
    "    def _test_epoch(self, data_loader, storer, epoch):\n",
    "        epoch_loss = 0.\n",
    "        kwargs = dict(desc=\"Epoch {}\".format(epoch + 1), leave=False,\n",
    "                      disable=not self.is_progress_bar)\n",
    "        with trange(len(data_loader), **kwargs) as t:\n",
    "            for _, (data, _) in enumerate(data_loader):\n",
    "                iter_loss = self._train_iteration(data, storer)\n",
    "                epoch_loss += iter_loss\n",
    "                t.set_postfix(loss=iter_loss)\n",
    "                t.update()\n",
    "        mean_epoch_loss = epoch_loss / len(data_loader)\n",
    "        return mean_epoch_loss\n",
    "    \n",
    "    def _test_iteration(self, data, storer):\n",
    "        batch_size, channel, height, width = data.size()\n",
    "        data = data.to(self.device)\n",
    "        recon_batch, latent_dist, latent_sample = self.model(data)\n",
    "        loss = self.loss_f(data, recon_batch, latent_dist, self.model.training, \n",
    "                               storer, latent_sample=latent_sample)\n",
    "            \n",
    "        return loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOSSES = [\"betaH\", \"betaB\"]\n",
    "RECON_DIST = [\"bernoulli\", \"laplace\", \"gaussian\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "mnist_dataset = datasets.MNIST('/home/data/bvaa', \n",
    "                   train=True, \n",
    "                   download=True, \n",
    "                   transform=transforms.Compose([\n",
    "                       transforms.Resize(32),\n",
    "                       transforms.ToTensor()\n",
    "                   ]))\n",
    "\n",
    "mnist_dataset_test = datasets.MNIST('/home/data/bvaa', train=False, download=True, transform=transforms.Compose([\n",
    "                       transforms.Resize(32),\n",
    "                       transforms.ToTensor()\n",
    "                   ]))\n",
    "\n",
    "train_loader = DataLoader(mnist_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "test_loader = DataLoader(mnist_dataset_test, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = enumerate(test_loader)\n",
    "batch_idx, (example_data, example_targets) = next(examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vae import VAE\n",
    "from encoder import Encoder\n",
    "from decoder import Decoder\n",
    "from losses import get_loss_fn\n",
    "from torch import optim\n",
    "\n",
    "latent_dim = 12\n",
    "img_size = [1,32,32]\n",
    "\n",
    "lr = 5e-4\n",
    "\n",
    "betaB_args = {\"rec_dist\": \"bernoulli\",\n",
    "              \"reg_anneal\": 10000, \n",
    "              \"betaH_B\": 4,\n",
    "              \"betaB_initC\": 0,\n",
    "              \"betaB_finC\": 25,\n",
    "              \"betaB_G\": 100\n",
    "             }\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "loss_f = get_loss_fn(\"betaB\", n_data=len(train_loader.dataset), device=device, **betaB_args)\n",
    "\n",
    "encoder = Encoder(img_size, latent_dim)\n",
    "decoder = Decoder(img_size, latent_dim)\n",
    "\n",
    "generator_model = VAE(img_size, latent_dim, encoder, decoder).to(device)\n",
    "optimizer = optim.Adam(generator_model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer = Trainer(generator_model, optimizer, loss_f, device, logger=logger,\\\n",
    "#                   save_dir=exp_dir, is_progress_bar=False)\n",
    "# epochs = 100\n",
    "# checkpoint_every = 10\n",
    "# trainer(train_loader, epochs=epochs, checkpoint_every=checkpoint_every)\n",
    "# torch.save(trainer.model.state_dict(), 'test.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generator_model.load_state_dict(torch.load('models/test.pt'))\n",
    "# generator_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Classifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Classifier, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, 3, 1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, 1)\n",
    "        self.dropout1 = nn.Dropout2d(0.25)\n",
    "        self.dropout2 = nn.Dropout2d(0.5)\n",
    "        self.fc1 = nn.Linear(9216, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = self.dropout1(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.fc2(x)\n",
    "        output = F.softmax(x, dim=1)\n",
    "        return output\n",
    "\n",
    "classifier = Classifier()\n",
    "classifier.load_state_dict(torch.load('models/mnist_cnn_non_log.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AddNoise():\n",
    "    def __init__(self, shape, device, noise=None, percent_noise=0.1):\n",
    "        self.device = device\n",
    "        self.percent_noise = percent_noise\n",
    "        if noise is None:\n",
    "            self.noise = torch.randn(shape)\n",
    "        else:\n",
    "            self.noise = noise\n",
    "\n",
    "    def noisy(self, data):\n",
    "        x = self.noise.to(self.device) * self.percent_noise + data * (1 - self.percent_noise)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Noise for Testing: \n",
      " tensor([[ 0.7344,  0.0049,  0.1108, -0.9742,  0.7806,  0.4000, -0.6167, -1.0133,\n",
      "          0.4483, -0.2564, -0.7429,  0.7341]])\n"
     ]
    }
   ],
   "source": [
    "sample_noise = torch.randn(torch.Size([1, 12]))\n",
    "backup_sample_noise = sample_noise\n",
    "print(\"Sample Noise for Testing: \\n\", sample_noise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Decoder(\n",
       "  (lin1): Linear(in_features=12, out_features=256, bias=True)\n",
       "  (lin2): Linear(in_features=256, out_features=256, bias=True)\n",
       "  (lin3): Linear(in_features=256, out_features=512, bias=True)\n",
       "  (convT1): ConvTranspose2d(32, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "  (convT2): ConvTranspose2d(32, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "  (convT3): ConvTranspose2d(32, 1, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder = generator_model.decoder\n",
    "decoder.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Classifier(\n",
       "  (conv1): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (dropout1): Dropout2d(p=0.25, inplace=False)\n",
       "  (dropout2): Dropout2d(p=0.5, inplace=False)\n",
       "  (fc1): Linear(in_features=9216, out_features=128, bias=True)\n",
       "  (fc2): Linear(in_features=128, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import exp\n",
    "\n",
    "def gaussian(window_size, sigma):\n",
    "    gauss = torch.Tensor([exp(-(x - window_size//2)**2/float(2*sigma**2)) for x in range(window_size)])\n",
    "    return gauss/gauss.sum()\n",
    "\n",
    "def create_window(window_size, channel):\n",
    "    _1D_window = gaussian(window_size, 1.5).unsqueeze(1)\n",
    "    _2D_window = _1D_window.mm(_1D_window.t()).float().unsqueeze(0).unsqueeze(0)\n",
    "    window = torch.Tensor(_2D_window.expand(channel, 1, window_size, window_size).contiguous())\n",
    "    return window\n",
    "\n",
    "def _ssim(img1, img2, window, window_size, channel, size_average = True):\n",
    "    mu1 = F.conv2d(img1, window, padding = window_size//2, groups = channel)\n",
    "    mu2 = F.conv2d(img2, window, padding = window_size//2, groups = channel)\n",
    "\n",
    "    mu1_sq = mu1.pow(2)\n",
    "    mu2_sq = mu2.pow(2)\n",
    "    mu1_mu2 = mu1*mu2\n",
    "\n",
    "    sigma1_sq = F.conv2d(img1*img1, window, padding = window_size//2, groups = channel) - mu1_sq\n",
    "    sigma2_sq = F.conv2d(img2*img2, window, padding = window_size//2, groups = channel) - mu2_sq\n",
    "    sigma12 = F.conv2d(img1*img2, window, padding = window_size//2, groups = channel) - mu1_mu2\n",
    "\n",
    "    C1 = 0.01**2\n",
    "    C2 = 0.03**2\n",
    "\n",
    "    ssim_map = ((2*mu1_mu2 + C1)*(2*sigma12 + C2))/((mu1_sq + mu2_sq + C1)*(sigma1_sq + sigma2_sq + C2))\n",
    "\n",
    "    if size_average:\n",
    "        return ssim_map.mean()\n",
    "    else:\n",
    "        return ssim_map.mean(1).mean(1).mean(1)\n",
    "\n",
    "class SSIM(nn.Module):\n",
    "    def __init__(self, window_size = 11, size_average = True):\n",
    "        super(SSIM, self).__init__()\n",
    "        self.window_size = window_size\n",
    "        self.size_average = size_average\n",
    "        self.channel = 1\n",
    "        self.window = create_window(window_size, self.channel)\n",
    "\n",
    "    def forward(self, img1, img2):\n",
    "        (_, channel, _, _) = img1.size()\n",
    "\n",
    "        if channel == self.channel and self.window.data.type() == img1.data.type():\n",
    "            window = self.window\n",
    "        else:\n",
    "            window = create_window(self.window_size, channel)\n",
    "            \n",
    "            if img1.is_cuda:\n",
    "                window = window.cuda(img1.get_device())\n",
    "            window = window.type_as(img1)\n",
    "            \n",
    "            self.window = window\n",
    "            self.channel = channel\n",
    "\n",
    "\n",
    "        return _ssim(img1, img2, window, self.window_size, channel, self.size_average)\n",
    "\n",
    "def ssim(img1, img2, window_size = 11, size_average = True):\n",
    "    (_, channel, _, _) = img1.size()\n",
    "    window = create_window(window_size, channel)\n",
    "    \n",
    "    if img1.is_cuda:\n",
    "        window = window.cuda(img1.get_device())\n",
    "    window = window.type_as(img1)\n",
    "    \n",
    "    return _ssim(img1, img2, window, window_size, channel, size_average)\n",
    "\n",
    "ssim_loss = SSIM(window_size = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constrained Translator\n",
    "class Translator(nn.Module):\n",
    "    def __init__(self, hidden_layers=[5, 10, 5], latent_dim=latent_dim):\n",
    "        super(Translator, self).__init__()\n",
    "        self.hidden_layers = hidden_layers\n",
    "        self.hidden_layers.insert(0, latent_dim)\n",
    "        self.hidden_layers.append(latent_dim)\n",
    "        self.layers = []\n",
    "        \n",
    "        for i in range(len(self.hidden_layers)-1):\n",
    "            self.layers.append(nn.Linear(self.hidden_layers[i], self.hidden_layers[i+1]))\n",
    "        \n",
    "        self.layers = nn.ModuleList(self.layers)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "translator = Translator().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from skimage.metrics import structural_similarity as ssim\n",
    "\n",
    "def create_logits(target_label, pred, confidence=0.53, device=device):\n",
    "    logits = torch.zeros(pred.shape, dtype=torch.float64)\n",
    "    pred_labels = [int(element.item()) for element in torch.argmax(pred, dim=1)]\n",
    "#     print(\"length\", len(pred_labels))\n",
    "#     print(\"pred_lables:\",pred_labels)\n",
    "#     print(logits.shape)\n",
    "    logits[range(logits.shape[0]), pred_labels] = torch.DoubleTensor([1-confidence]*pred.shape[0])\n",
    "    logits[range(logits.shape[0]), [target_label]*pred.shape[0]] += torch.DoubleTensor([confidence]*pred.shape[0])\n",
    "#     print(\"logits:\", logits)\n",
    "    return logits.to(device)\n",
    "\n",
    "def structural(org_image, noised_image):\n",
    "    batch_size, channels, width, height = org_image.shape\n",
    "    loss1 = 0\n",
    "    for b_ in range(batch_size):\n",
    "        ch_loss = 0\n",
    "        for ch_ in range(channels):\n",
    "            ch_loss += 1-ssim(org_image[b_][ch_].detach().cpu().numpy(), noised_image[b_][ch_].detach().cpu().numpy())\n",
    "        loss1 += ch_loss/channels\n",
    "    return loss1\n",
    "            \n",
    "class T_Loss(nn.Module):\n",
    "    def __init__(self, decoder=decoder, classifier=classifier):\n",
    "        super(T_Loss, self).__init__()\n",
    "        self.decoder = decoder\n",
    "        self.classifier = classifier\n",
    "    def forward(self, x, org_x, target_label):\n",
    "        \n",
    "        org_image = self.decoder(org_x)\n",
    "        noised_image = self.decoder(x)\n",
    "        batch_size, channels, width, height = org_image.shape\n",
    "        loss1 = ssim_loss(org_image, noised_image)\n",
    "#         print(type(loss1))\n",
    "#         print(loss1)\n",
    "#         loss1 = torch.Tensor(loss1 / batch_size).to(device)\n",
    "#         print(\"loss1:\", loss1)\n",
    "        preds = self.classifier(F.upsample(noised_image, (28,28), mode='bilinear', align_corners=True))\n",
    "#         print(\"preds:\",preds)\n",
    "        target = create_logits(target_label, preds)\n",
    "#         print(target)\n",
    "        loss2 = nn.BCELoss(reduction='sum')(preds, target.float())\n",
    "#         print(type(loss2))\n",
    "#         print(\"loss1:\",100*(1-loss1))\n",
    "#         print(\"loss2:\",loss2)\n",
    "        loss = 100*(1-loss1) + loss2\n",
    "        \n",
    "        \n",
    "        out_labels = preds.argmax(dim=1, keepdim=True)\n",
    "#         print(out_labels)\n",
    "#         print(torch.empty(out_labels.shape).fill_(target_label))\n",
    "#         print(preds)\n",
    "#         correct = out_labels.eq(torch.Tensor([target_label]*out_labels.shape[0]).to(device)).sum()\n",
    "        correct = out_labels.eq(torch.empty(out_labels.shape).fill_(target_label).to(device)).sum()\n",
    "#         print(out_labels.shape)\n",
    "#         print(correct)\n",
    "#         print(torch.Tensor([target_label]*out_labels.shape[0]))\n",
    "#         print(out_labels)\n",
    "        return loss, correct.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "T_Loss(\n",
       "  (decoder): Decoder(\n",
       "    (lin1): Linear(in_features=12, out_features=256, bias=True)\n",
       "    (lin2): Linear(in_features=256, out_features=256, bias=True)\n",
       "    (lin3): Linear(in_features=256, out_features=512, bias=True)\n",
       "    (convT1): ConvTranspose2d(32, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (convT2): ConvTranspose2d(32, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (convT3): ConvTranspose2d(32, 1, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "  )\n",
       "  (classifier): Classifier(\n",
       "    (conv1): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (dropout1): Dropout2d(p=0.25, inplace=False)\n",
       "    (dropout2): Dropout2d(p=0.5, inplace=False)\n",
       "    (fc1): Linear(in_features=9216, out_features=128, bias=True)\n",
       "    (fc2): Linear(in_features=128, out_features=10, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tloss = T_Loss().to(device)\n",
    "tloss.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "938"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95f4bd06f76e48288dca5058648933db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=10.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/bvaa/lib/python3.6/site-packages/torch/nn/functional.py:2416: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.\n",
      "  warnings.warn(\"nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0\tLoss: 248.810912\tCorrect: 14341\n",
      "Train Epoch: 1\tLoss: 216.653952\tCorrect: 12383\n",
      "Train Epoch: 2\tLoss: 197.100085\tCorrect: 15052\n",
      "Train Epoch: 3\tLoss: 117.496681\tCorrect: 54548\n",
      "Train Epoch: 4\tLoss: 107.915900\tCorrect: 59697\n",
      "Train Epoch: 5\tLoss: 101.067268\tCorrect: 59687\n",
      "Train Epoch: 6\tLoss: 103.113236\tCorrect: 59961\n",
      "Train Epoch: 7\tLoss: 109.307953\tCorrect: 59579\n",
      "Train Epoch: 8\tLoss: 108.893671\tCorrect: 58294\n",
      "Train Epoch: 9\tLoss: 110.440082\tCorrect: 59287\n",
      "\n"
     ]
    }
   ],
   "source": [
    "attack_log_interval = 1\n",
    "alt_target = 5\n",
    "translator.train()\n",
    "optimizer = optim.Adam(translator.parameters(), lr=1e-3)\n",
    "for epoch in tqdm(range(10)):\n",
    "    epoch_loss = 0\n",
    "    epoch_correct = 0\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data = torch.FloatTensor(data).to(device)\n",
    "        \n",
    "        _, l_dist, l_sample = generator_model(data)\n",
    "        \n",
    "        noised_sample = translator(l_sample)\n",
    "        \n",
    "        loss, correct = tloss(noised_sample, l_sample, alt_target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "#         print(correct)\n",
    "        epoch_loss += loss.item()\n",
    "        epoch_correct += correct\n",
    "        \n",
    "    if (epoch+1) % attack_log_interval == 0:\n",
    "        print('Train Epoch: {}\\tLoss: {:.6f}\\tCorrect: {}'.format(\n",
    "            epoch, epoch_loss/batch_idx, epoch_correct))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = enumerate(test_loader)\n",
    "batch_idx, (example_data, example_targets) = next(examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(i, conf):\n",
    "    _, l_dist, l_sample = generator_model(example_data[i].unsqueeze_(0).to(device))\n",
    "    noised_sample = translator(l_sample)\n",
    "    noised_sample = conf * l_sample + (1-conf) * (noised_sample - noised_sample.min())/(noised_sample.max() - noised_sample.min())\n",
    "    final = generator_model.decoder(noised_sample)\n",
    "    pred = torch.argmax(classifier(F.upsample(final, (28,28), mode='bilinear', align_corners=True)))\n",
    "    print(\"Actual: {}, Prediction: {}\".format(example_targets[i], pred.item()))\n",
    "    plt.figure(figsize=(10,10))\n",
    "    plt.subplot(1,2,1)\n",
    "    plt.imshow(example_data[i][0].detach().cpu().numpy(), cmap='gray', interpolation='none')\n",
    "    plt.subplot(1,2,2)\n",
    "    plt.imshow(final[0][0].detach().cpu().numpy(), cmap='gray', interpolation='none')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual: 5, Prediction: 5\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAlAAAAEdCAYAAADDzFlqAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAAflUlEQVR4nO3deWzd9Znv8c8Tx85mJ3YSsm8sAUJCx6RhSyhQGoalVLRiikAqRVU1odUgpeq0asWt7u2gVu3c3tJW6qb0ggpXvbSUsqmF3okQIzQohUD2TEKzYJIQE5tstpPY8fK9f/hE8tCc5+vvWXx+dt4vKYp9Pv75PPnZfvzk53MeWwhBAAAAGLxRlS4AAABguGGAAgAASMQABQAAkIgBCgAAIBEDFAAAQCIGKAAAgESjiznYzG6V9BNJVZL+dwjh+5G3Z2cCcO75IIRwXqWLOJuUHkb/As5JeftXwVegzKxK0s8k3SbpMkn3mtllhb4/ACPWu5Uu4GzoYQAGIW//KuZHeFdJ2h1C2BtCOC3pt5LuLOL9AcBQoocBKFgxA9RsSfsHvH4gdxsADAf0MAAFK+YxUHaW2/7mMQJmtkrSqiLuBwDKIdrD6F8A8ilmgDogae6A1+dIOvjhNwohrJG0RuJBmAAyJdrD6F8A8inmR3jrJS00s/PNrEbSPZJeKE1ZAFB29DAABSv4ClQIocfMHpT0/9T/FODHQgjbS1YZAJQRPQxAMSyEobsqzSVw4Jz0VghhWaWLKBb9Czgn5e1fbCIHAABIxAAFAACQiAEKAAAgEQMUAABAIgYoAACARAxQAAAAiRigAAAAEjFAAQAAJGKAAgAASMQABQAAkIgBCgAAIBEDFAAAQCIGKAAAgEQMUAAAAIkYoAAAABIxQAEAACRigAIAAEjEAAUAAJCIAQoAACARAxQAAEAiBigAAIBEDFAAAACJGKAAAAASja50ARgas2bNcvOlS5cWdfyGDRvcfNOmTW7e09Pj5sWaPHmymy9ZssTNY+fHq3/9+vXusVu3bnXzkydPujmAbDOzTN+/l48e7Y8Jsd4dQnDzvr4+N88yrkABAAAkYoACAABIxAAFAACQiAEKAAAgEQMUAABAIgYoAACARAxQAAAAiYraA2VmTZLaJfVK6gkhLCtFUUhXV1fn5jfccIObr1692s0XLlzo5k899ZSb/+xnP3Pzzs5ON4/tIpk2bZqbL1682M1vu+02N//Upz7l5h0dHXmzb33rW+6xu3fvdnP2QJUPPaw0itkzNBR5rH/EVFVVFXX/NTU1RR1frFj9Xn0TJ050jz1+/LibHzt2zM1j/S3Le6JKsUjz4yGED0rwfgCgEuhhAJLxIzwAAIBExQ5QQdK/mdlbZraqFAUBwBCihwEoSLE/wlsRQjhoZtMkrTWznSGEVwe+Qa4p0ZgAZJHbw+hfAPIp6gpUCOFg7u8WSc9Kuuosb7MmhLCMB2cCyJpYD6N/Acin4AHKzCaYWd2ZlyX9vaRtpSoMAMqJHgagGMX8CG+6pGdzT78cLen/hhD+XJKqAKD86GEAClbwABVC2Cvp70pYCxyTJ09286uu+pufnv4Xn/3sZ4s6vqenx81vv/12Nw8huPnp06fdfMKECW5+zTXXuPmiRYvcfNQo/2JsbE/Vrl278mYbN250jz116pSbozzoYYMX21NUzJ6hweRjxoxx89raWjefO3eum48bN87Nx48f7+ax/tHQ0ODmXV1dRb3/trY2N4+dX+/jF/vYtrS0uPn27dvdPNb7u7u73Tz2vaWcWGMAAACQiAEKAAAgEQMUAABAIgYoAACARAxQAAAAiRigAAAAEjFAAQAAJCr2d+GhRGJ7Pu666y43X7XK/3VdjY2Nbl7sLo2ZM2e6+Ze//GU3j+2Z6uvrS65poGJ3jbz99ttu/tWvfjVv9pe//MU9NvZvB0ohtsupmGOrq6vdvL6+3s3nzZvn5pdccombf/SjHy3q+Nievbq6OjePie2xKnYXXHt7u5vHvr90dHTkzQ4dOuQeu2fPHjeP1Xbs2DE3j/VH9kABAAAMIwxQAAAAiRigAAAAEjFAAQAAJGKAAgAASMQABQAAkIg1Bhkxe/ZsN7/pppvcPPY03dGj/Q917Kmq69evd/Oamho3//jHP+7mGzZscPO9e/e6eexpwO+8846bv/baa24eW2PQ0tKSN2NNAbIgtoqgqqoqbxbrH1OnTnXzxYsXu/mNN97o5itWrHDzCy+80M3HjRvn5sWseJCk3t5eN4/1x2Lff2xNS+yp/sePH8+bxfpXU1OTm48dO9bNvc+7rOMKFAAAQCIGKAAAgEQMUAAAAIkYoAAAABIxQAEAACRigAIAAEjEAAUAAJCIPVAlNGpU/nk0tufp4YcfdvPYHqUJEya4eWzP0zPPPOPmjzzyiJvHdnnMmjXLzVtbW928o6PDzWO7Srq6utz8xIkTRR0f29MCVJrXnyRpzJgxebP6+nr32CVLlrj5ypUr3Xz58uVufsEFF7h5bNdQbE/c4cOH3dzbkyTF+8/48ePdvKGhwc1j/SeWd3d3u/m7776bN9u5c6d77ObNm908toOvs7PTzWM7rCqJK1AAAACJGKAAAAASMUABAAAkYoACAABIxAAFAACQiAEKAAAgEQMUAABAougeKDN7TNIdklpCCEtyt02W9DtJCyQ1Sbo7hHC0fGUOD9XV1XmzO+64wz32+uuvd/OpU6e6+cmTJ918w4YNbr5+/Xo3X7hwoZtfeumlbn7eeee5eWxXyP79+928qanJzWN7XGLnD8MXPaxfMft0ampq3Dy2523OnDluPmPGDDeP7XmK7Yn761//6uabNm1y8yNHjrh5XV2dm0+aNMnNp0yZ4uZtbW1u/v7777t5e3u7m3t7+GI7BPft21fw+5biO6yG+x6oX0u69UO3fVPSyyGEhZJezr0OAFn0a9HDAJRYdIAKIbwq6cPj952SHs+9/LikT5e4LgAoCXoYgHIo9DFQ00MIzZKU+3ta6UoCgLKjhwEoStl/F56ZrZK0qtz3AwClRv8CkE+hV6AOmdlMScr93ZLvDUMIa0IIy0IIywq8LwAotUH1MPoXgHwKHaBekHR/7uX7JT1fmnIAYEjQwwAUJTpAmdmTktZJusTMDpjZFyV9X9LNZrZL0s251wEgc+hhAMoh+hioEMK9eaJPlLiWzJs4caKbf+IT+U/Jfffd5x47c+ZMN6+qqnLzmNgepltuucXNp03zH2Mb2+Myfvx4N4/tWTl27JibHzhwwM03btzo5q+88oqbx/bEILvoYf1i+3R6enoKyqR4f4r1ztraWjfv6+tz89iuob1797p5c3Ozm48a5V9riJ2fw4cPF3X/sT1PR4/6K8xie/C6u7sLft+xPVGdnZ1uHvvYZhmbyAEAABIxQAEAACRigAIAAEjEAAUAAJCIAQoAACARAxQAAEAiBigAAIBEZf9deCPJ5MmT3XzVqvy/Mmvp0qXusTU1NQXVdEZsT0ldXV1R+XvvvVdUHrNgwQI3X7RokZtfeeWVReUXX3yxmz/zzDNuvnbtWjcHKi22B8oT6y9jx4518+rqajeP7QI6ffp0Ufcf6y8NDQ1Fvf/Ro/1vpbE9dk1NTW4e+/4Q2/MX+/h5/bujo8M99uTJk24+nPc8xXAFCgAAIBEDFAAAQCIGKAAAgEQMUAAAAIkYoAAAABIxQAEAACRigAIAAEjEHqgBamtr3fzyyy938xUrVuTNYns8YntOWltb3XzPnj1u/vrrr7v5q6++6ubbtm1z82LF9mStXLnSzT/2sY+5+fnnn+/m9957r5vPmzfPzd95552CMknq7e11c6AUzMzNvR4V24E3a9YsN48dH+uPVVVVbj5mzBg3j+1Jiil2T1VnZ6ebz549282bm5vdPLZnq62tzc0PHjyYN1u3bp177PHjx928q6vLzXt6etw8y7gCBQAAkIgBCgAAIBEDFAAAQCIGKAAAgEQMUAAAAIkYoAAAABIxQAEAACRiD9QACxYscPO7777bzcePH583i+362bFjh5uvXbvWzV966SU337hxo5sfO3bMzcvt3XffdfMNGza4+c6dO9089rFrbGx082XLlrn5Aw88kDf7zne+4x4b29ESQnBzQIrveYrtUvJ2MU2aNMk9tq6uzs1jn8OxPUsxsX9b7NwUu+cptssols+YMcPNY3u0YvmoUf61Eq8HzZ8/3z029r1jy5Ytbt7d3e3mxX5ulBNXoAAAABIxQAEAACRigAIAAEjEAAUAAJCIAQoAACARAxQAAEAiBigAAIBE0T1QZvaYpDsktYQQluRu+7akf5TUmnuzh0IIL5aryKES26Vx+eWXu7m36+O9995zj3344Yfd/JVXXnHzSu9xKrfYnqhf/vKXbr5v3z43/8Y3vuHmV199tZt//vOfz5s9+eST7rHbt293866uLjeH71zpYbFdR96eJ0maOHFi3mzevHnusQ0NDW5+8uRJN3///ffdPPZv6+zsdPPjx4+7eWtrq5vv37/fzWNie7AuvPBCN6+vr3fz0aP9b+Wx46dOnZo3u+GGG9xjY3uafvrTn7p5bEfhqVOn3LySBnMF6teSbj3L7T8KITTm/gzrxgNgRPu16GEASiw6QIUQXpV0ZAhqAYCSo4cBKIdiHgP1oJltMbPHzMy/fgsA2UMPA1CwQgeoX0i6UFKjpGZJP8z3hma2yszeNLM3C7wvACi1QfUw+heAfAoaoEIIh0IIvSGEPkm/knSV87ZrQgjLQgj+b2MFgCEy2B5G/wKQT0EDlJnNHPDqZyRtK005AFB+9DAAxRrMGoMnJd0oaaqZHZD0PyTdaGaNkoKkJkkPlLFGACgYPQxAOUQHqBDCvWe5+dEy1FJxsX0UX/va1wrOf/zjH7vHrlu3zs3b2trc/FzX3d3t5rGP7Z///Gc3v/baa93c26HzhS98wT32e9/7npsfPHjQzeEbKT0stgtp1Cj/Bwpjx451c29XUHV1tXtsbI/T4cOH3Ty2Jym2C+idd95x86amJjc/cOCAmx89etTNY/XFdiWNGTPGzb09TZK0fPlyN//kJz/p5t6euylTphR131u3bnXz2I6+5uZmN+/t7XXzcmITOQAAQCIGKAAAgEQMUAAAAIkYoAAAABIxQAEAACRigAIAAEjEAAUAAJAougfqXHLixAk3j+1q+tKXvpQ3a21tdY+N7RGJ7UmBr6Wlxc3feOMNN4/tkWpsbMyb3XPPPe6x27b5S7Cfe+45Nz906JCbY2SI7XmK7RKaMGFCwe//vffec4+N7XmK7bGL9d6enh43P3LkiJu3t7e7eaz/nj592s1je+iKFTu/sV1xb7/9tpt//etfz5utXLnSPXby5Mlu7vVGSXrxxRfdPPa9kz1QAAAAwwgDFAAAQCIGKAAAgEQMUAAAAIkYoAAAABIxQAEAACRigAIAAEjEHqgB+vr63Dy2qySWo3K6urrcfOfOnW7+/PPPu/kVV1yRN5syZYp77EUXXeTmtbW1bs4eqJHBzNy8urrazSdNmuTm48ePd/POzs682b59+9xjOzo63Dy2hynWe4vNY3uaYnumYu+/3Hv6YruOYt97mpqa3Hz//v15s9i5qaqqcvOpU6e6+bhx49w8yzsQuQIFAACQiAEKAAAgEQMUAABAIgYoAACARAxQAAAAiRigAAAAEjFAAQAAJDqn9kDF9qDE9lF4e1Ik9kBlWWyXyAcffODmGzduLPi+Y/t9li5d6uaxPVJ79uxJrgnDT2wfWENDg5vH9kidPHkyb1bsDrxY7yz3HqhidwlVehdRrIfEdjHV1NS4eX19fd5s7Nix7rGxcx9T7A6uSuIKFAAAQCIGKAAAgEQMUAAAAIkYoAAAABIxQAEAACRigAIAAEjEAAUAAJAougfKzOZKekLSDEl9ktaEEH5iZpMl/U7SAklNku4OIRwtX6lxM2bMcPNrr73WzZcsWeLmu3fvdvOnn346b9bd3e0ei8o6ffq0m7e1tZXtvj/ykY+4+eTJk8t23yPdcOpfo0b5/5+dPn26m8+dO9fNY7uMjh8/njeL7QKK7fLp6upy83N9z1Nsj1Nsh+G0adPcfPny5W5+8cUX581Gj/bHhPb2djePfd9sbW118+G+B6pH0j+HEBZJukbSP5nZZZK+KenlEMJCSS/nXgeALKF/ASiL6AAVQmgOIWzIvdwuaYek2ZLulPR47s0el/TpchUJAIWgfwEol6THQJnZAklXSHpd0vQQQrPU36Qk+dcQAaCC6F8ASmnQvwvPzGol/UHSV0IIbbHfzTPguFWSVhVWHgAUj/4FoNQGdQXKzKrV33x+E0J4JnfzITObmctnSmo527EhhDUhhGUhhGWlKBgAUtC/AJRDdICy/v+qPSppRwjhkQHRC5Luz718v6TnS18eABSO/gWgXAbzI7wVku6TtNXMNuVue0jS9yU9ZWZflLRP0mfLUyIAFIz+BaAsogNUCOE/JOV7wMAnSltOcVasWOHmq1evdvPYPp6XX37ZzZ9/Pv9/YmN7Uiq9hwSVM3XqVDcfM2bMEFUy8gyn/hXbtxPb9ePt8pGkSZMmubm3h2rLli3usZ2dnW5+6tQpNy92T17sMW2xvNj+G9vhFfvY1tfXu/mll17q5jfccIObX3/99W4+b968vFnsY7Nr1y43f+6559x8pO+BAgAAwAAMUAAAAIkYoAAAABIxQAEAACRigAIAAEjEAAUAAJCIAQoAACDRoH8X3nAQ2wN13XXXuXlbW5ubx3aFxHaBILuqq6vdvLa2dogqwbmqqqrKzWtqatx8+vTpbt7Y2OjmU6ZMyZvFdkidOHHCzWN78I4ePVrU8TGxcxszYcIEN29oaHDzCy64wM2XLl3q5ldeeaWbX3TRRW4eq8/rf/v27XOP/f3vf+/mW7dudfPYDrEs4zs+AABAIgYoAACARAxQAAAAiRigAAAAEjFAAQAAJGKAAgAASMQABQAAkGhE7YGK7RI5cuSIm3t7UCRp8eLFbj5r1qy82d69e91ji91zcq4zMzcfPdr/VJ8zZ46bL1++PLmmwYp97Pv6+sp238iO2Mc5lk+bNs3Nvf4k+buerr76avfYsWPHuvnmzZvdvKOjw81jX7+xHXyxHVqxXUTz589389iep0svvbSo9x87v7H+d+rUKTffvXt33uzZZ591j43lra2tbj6c+xtXoAAAABIxQAEAACRigAIAAEjEAAUAAJCIAQoAACARAxQAAECiEbXG4Ac/+IGbnzx50s2/+93vuvm8efPc/Oc//3ne7IEHHnCPPXDggJufPn3azUMIbj7cxZ6GPHHiRDe/8sor3fyuu+5y89jTuIuxbt06N489DRgjQ29vr5vHPg/ef//9oo6vra3Nm82dO9c9NtYbb7755oLvW5LGjRvn5l1dXW4e65/d3d1uHusv48ePd/PYGobYKpP29nY3P3TokJtv2rTJzV966aW8Waw/xT7vYud+OOMKFAAAQCIGKAAAgEQMUAAAAIkYoAAAABIxQAEAACRigAIAAEjEAAUAAJAougfKzOZKekLSDEl9ktaEEH5iZt+W9I+SziwXeSiE8GK5Ch2Mzs5ON//jH//o5jNmzHDzBx980M2vvfbagu/7T3/6k5vHdm2sX7/ezTdv3uzmsR1ZMWbm5lOmTHHzW265xc2vu+46N1+0aJGbxz629fX1bh7b8+LtcYntUXnooYfcfMuWLW6O/IZT/4rtgdq/f7+bv/baa25+4sQJN1+8eHHebMGCBe6x06dPd/PYHqexY8e6eVVVlZvH+k/s+DFjxrh5zOHDh9382LFjbr5v3z43j+1x2rFjh5tv2LDBzZubm/NmHR0d7rGxHVYjeUfhYBZp9kj65xDCBjOrk/SWma3NZT8KIfyv8pUHAEWhfwEoi+gAFUJoltSce7ndzHZIml3uwgCgWPQvAOWS9BgoM1sg6QpJr+duetDMtpjZY2bWUOLaAKBk6F8ASmnQA5SZ1Ur6g6SvhBDaJP1C0oWSGtX/P7wf5jlulZm9aWZvlqBeAEhG/wJQaoMaoMysWv3N5zchhGckKYRwKITQG0Lok/QrSVed7dgQwpoQwrIQwrJSFQ0Ag0X/AlAO0QHK+p/e8KikHSGERwbcPnPAm31G0rbSlwcAhaN/ASiXwTwLb4Wk+yRtNbMzz6V8SNK9ZtYoKUhqkvRAWSoEgMLRvwCUhQ3ljgYzq+hCiNiukUsuucTNP/e5z7n56tWr82ajR/uzqreHQ5JOnTrl5h988IGbHzlyxM1juzxiYntYYntWZs/2nxgV2zMzceJEN6+urnbzvr4+N29tbXXzF154IW/2xBNPuMfGdrQUu6MrA94aCT8Cq3T/qqmpcfPzzjvPzWNfY+eff37ebP78+UXdd11dnZvHvj5je5xOnz7t5rEdWLE9TbH+eujQITeP7VJqaWkp6v5j77+YXU6x3ngOyNu/2EQOAACQiAEKAAAgEQMUAABAIgYoAACARAxQAAAAiRigAAAAEjFAAQAAJDqn9kDFxHYVLViwwM1vvvnmvFlsj0lsjxLKK7brpL293c03btyYN9u6dat7bG9vr5uPAOyBGgKxXXOxPXj19fUFZVJ8D1Oxe+ZGjfL/rx/7t8fuP7ZrLXZ8sf/+2PGxHhHrX0P5fX4EYg8UAABAqTBAAQAAJGKAAgAASMQABQAAkIgBCgAAIBEDFAAAQCIGKAAAgETsgQJQbuyByoDYrjlvV11sz1K59xDF3n9M7P7L/X2QPUzDGnugAAAASoUBCgAAIBEDFAAAQCIGKAAAgEQMUAAAAIkYoAAAABIxQAEAACTyl3sAAEaE2C6inp6egjLgXMUVKAAAgEQMUAAAAIkYoAAAABIxQAEAACRigAIAAEjEAAUAAJCIAQoAACBRdIAys7Fm9oaZbTaz7Wb2L7nbzzez181sl5n9zsxqyl8uAKShhwEoh8FcgeqSdFMI4e8kNUq61cyukfSvkn4UQlgo6aikL5avTAAoGD0MQMlFB6jQryP3anXuT5B0k6Snc7c/LunTZakQAIpADwNQDoN6DJSZVZnZJkktktZK2iPpWAjhzH7/A5Jml6dEACgOPQxAqQ1qgAoh9IYQGiXNkXSVpEVne7OzHWtmq8zsTTN7s/AyAaBwhfYw+heAfJKehRdCOCbp3yVdI6nezM78MuI5kg7mOWZNCGFZCGFZMYUCQLFSexj9C0A+g3kW3nlmVp97eZyklZJ2SHpF0j/k3ux+Sc+Xq0gAKBQ9DEA5jI6/iWZKetzMqtQ/cD0VQvijmf2npN+a2XckbZT0aBnrBIBC0cMAlJyFcNaHLpXnzsyG7s4AZMVbI+FHYPQv4JyUt3+xiRwAACARAxQAAEAiBigAAIBEDFAAAACJGKAAAAASMUABAAAkYoACAABINJhFmqX0gaR3B7w+NXdbVmW5vizXJmW7vizXJo28+uaXq5AhRv8qrSzXl+XapGzXl+XapBL2ryFdpPk3d272ZpYX7GW5vizXJmW7vizXJlHfcJH180B9hctybVK268tybVJp6+NHeAAAAIkYoAAAABJVeoBaU+H7j8lyfVmuTcp2fVmuTaK+4SLr54H6Cpfl2qRs15fl2qQS1lfRx0ABAAAMR5W+AgUAADDsVGSAMrNbzextM9ttZt+sRA0eM2sys61mtsnM3sxAPY+ZWYuZbRtw22QzW2tmu3J/N2Ssvm+b2Xu5c7jJzG6vUG1zzewVM9thZtvNbHXu9oqfP6e2rJy7sWb2hpltztX3L7nbzzez13Pn7ndmVlOJ+iqJHpZUC/2r8Noy278i9WXl/JW3h4UQhvSPpCpJeyRdIKlG0mZJlw11HZEamyRNrXQdA+q5XtJSSdsG3PY/JX0z9/I3Jf1rxur7tqSvZeDczZS0NPdynaS/SrosC+fPqS0r584k1eZerpb0uqRrJD0l6Z7c7b+U9OVK1zrE54UellYL/avw2jLbvyL1ZeX8lbWHVeIK1FWSdocQ9oYQTkv6raQ7K1DHsBFCeFXSkQ/dfKekx3MvPy7p00Na1AB56suEEEJzCGFD7uV2STskzVYGzp9TWyaEfh25V6tzf4KkmyQ9nbu9op97FUIPS0D/KlyW+1ekvkwodw+rxAA1W9L+Aa8fUIZOeE6Q9G9m9paZrap0MXlMDyE0S/2fxJKmVbies3nQzLbkLpFX7BL9GWa2QNIV6v9fSKbO34dqkzJy7sysysw2SWqRtFb9V16OhRB6cm+Sxa/fcqOHFS9TX395ZOJr8Iws9y/p3OxhlRig7Cy3Ze2pgCtCCEsl3Sbpn8zs+koXNAz9QtKFkholNUv6YSWLMbNaSX+Q9JUQQlsla/mws9SWmXMXQugNITRKmqP+Ky+LzvZmQ1tVxdHDRr7MfA1K2e5f0rnbwyoxQB2QNHfA63MkHaxAHXmFEA7m/m6R9Kz6T3rWHDKzmZKU+7ulwvX8FyGEQ7lP3D5Jv1IFz6GZVav/i/s3IYRncjdn4vydrbYsnbszQgjHJP27+h8/UG9mZ36PZua+focAPax4mfj6yydLX4NZ7l/56svS+TujHD2sEgPUekkLc4+Cr5F0j6QXKlDHWZnZBDOrO/OypL+XtM0/qiJekHR/7uX7JT1fwVr+xpkv7pzPqELn0MxM0qOSdoQQHhkQVfz85astQ+fuPDOrz708TtJK9T/G4RVJ/5B7s8x97g0BeljxKv7158nQ12Bm+5dED6vUI+NvV/+j9fdI+m+VqMGp7QL1P6tms6TtWahP0pPqvwzarf7//X5R0hRJL0valft7csbq+z+Stkraov4v9pkVqu069V+e3SJpU+7P7Vk4f05tWTl3H5G0MVfHNkn/PXf7BZLekLRb0u8ljanU516l/tDDkuqhfxVeW2b7V6S+rJy/svYwNpEDAAAkYhM5AABAIgYoAACARAxQAAAAiRigAAAAEjFAAQAAJGKAAgAASMQABQAAkIgBCgAAINH/BwSDW7JRTT82AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 720x720 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "test(49, 0.6909)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
