{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import codecs\n",
    "import errno\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "from PIL import Image\n",
    "import random\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.datasets.mnist\n",
    "from torchvision import transforms\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "do_learn = True\n",
    "save_frequency = 2\n",
    "batch_size = 16\n",
    "lr = 0.001\n",
    "num_epochs = 10\n",
    "weight_decay = 0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_int(b):\n",
    "    return int(codecs.encode(b, 'hex'), 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_label_file(path):\n",
    "   with open(path, 'rb') as f:\n",
    "      data = f.read()\n",
    "   assert get_int(data[:4]) == 2049\n",
    "   length = get_int(data[4:8])\n",
    "   parsed = np.frombuffer(data, dtype=np.uint8, offset=8)\n",
    "   return torch.from_numpy(parsed).view(length).long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_image_file(path):\n",
    "   with open(path, 'rb') as f:\n",
    "      data = f.read()\n",
    "   assert get_int(data[:4]) == 2051\n",
    "   length = get_int(data[4:8])\n",
    "   num_rows = get_int(data[8:12])\n",
    "   num_cols = get_int(data[12:16])\n",
    "   images = []\n",
    "   parsed = np.frombuffer(data, dtype=np.uint8, offset=16)\n",
    "   return torch.from_numpy(parsed).view(length, num_rows, num_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BalancedMNISTPair(torch.utils.data.Dataset):\n",
    "   \"\"\"Dataset that on each iteration provides two random pairs of\n",
    "   MNIST images. One pair is of the same number (positive sample), one\n",
    "   is of two different numbers (negative sample).\n",
    "   \"\"\"\n",
    "   urls = [\n",
    "      'http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz',\n",
    "      'http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz',\n",
    "      'http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz',\n",
    "      'http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz',\n",
    "   ]\n",
    "   raw_folder = 'raw'\n",
    "   processed_folder = 'processed'\n",
    "   training_file = 'training.pt'\n",
    "   test_file = 'test.pt'\n",
    "   \n",
    "   def __init__(self, root, train=True, transform=None, target_transform=None, download=False):\n",
    "      self.root = os.path.expanduser(root)\n",
    "      self.transform = transform\n",
    "      self.target_transform = target_transform\n",
    "      self.train = train # training set or test set\n",
    "      \n",
    "      if download:\n",
    "         self.download()\n",
    "         \n",
    "      if not self._check_exists():\n",
    "         raise RuntimeError('Dataset not found.' + ' You can use download=True to download it')\n",
    "         \n",
    "      if self.train:\n",
    "         self.train_data, self.train_labels = torch.load(\n",
    "            os.path.join(self.root, self.processed_folder, self.training_file))\n",
    "         \n",
    "         train_labels_class = []\n",
    "         train_data_class = []\n",
    "         for i in range(10):\n",
    "            indices = torch.squeeze((self.train_labels == i).nonzero())\n",
    "            train_labels_class.append(torch.index_select(self.train_labels, 0, indices))\n",
    "            train_data_class.append(torch.index_select(self.train_data, 0, indices))\n",
    "            \n",
    "         # generate balanced pairs\n",
    "         self.train_data = []\n",
    "         self.train_labels = []\n",
    "         lengths = [x.shape[0] for x in train_labels_class]\n",
    "         for i in range(10):\n",
    "            for j in range(500): # create 500 pairs\n",
    "               rnd_cls = random.randint(0,8) # choose random class that is not the same class\n",
    "               if rnd_cls >= i:\n",
    "                  rnd_cls = rnd_cls + 1\n",
    "\n",
    "               rnd_dist = random.randint(0, 100)\n",
    "                  \n",
    "               self.train_data.append(torch.stack([train_data_class[i][j], train_data_class[i][j+rnd_dist], train_data_class[rnd_cls][j]]))\n",
    "               self.train_labels.append([1,0])\n",
    "\n",
    "         self.train_data = torch.stack(self.train_data)\n",
    "         self.train_labels = torch.tensor(self.train_labels)\n",
    "               \n",
    "      else:\n",
    "         self.test_data, self.test_labels = torch.load(\n",
    "            os.path.join(self.root, self.processed_folder, self.test_file))\n",
    "         \n",
    "         test_labels_class = []\n",
    "         test_data_class = []\n",
    "         for i in range(10):\n",
    "            indices = torch.squeeze((self.test_labels == i).nonzero())\n",
    "            test_labels_class.append(torch.index_select(self.test_labels, 0, indices))\n",
    "            test_data_class.append(torch.index_select(self.test_data, 0, indices))\n",
    "            \n",
    "         # generate balanced pairs\n",
    "         self.test_data = []\n",
    "         self.test_labels = []\n",
    "         lengths = [x.shape[0] for x in test_labels_class]\n",
    "         for i in range(10):\n",
    "            for j in range(500): # create 500 pairs\n",
    "               rnd_cls = random.randint(0,8) # choose random class that is not the same class\n",
    "               if rnd_cls >= i:\n",
    "                  rnd_cls = rnd_cls + 1\n",
    "\n",
    "               rnd_dist = random.randint(0, 100)\n",
    "                  \n",
    "               self.test_data.append(torch.stack([test_data_class[i][j], test_data_class[i][j+rnd_dist], test_data_class[rnd_cls][j]]))\n",
    "               self.test_labels.append([1,0])\n",
    "\n",
    "         self.test_data = torch.stack(self.test_data)\n",
    "         self.test_labels = torch.tensor(self.test_labels)\n",
    "         \n",
    "   def __getitem__(self, index):\n",
    "      if self.train:\n",
    "         imgs, target = self.train_data[index], self.train_labels[index]\n",
    "      else:\n",
    "         imgs, target = self.test_data[index], self.test_labels[index]\n",
    "         \n",
    "      img_ar = []\n",
    "      for i in range(len(imgs)):\n",
    "         img = Image.fromarray(imgs[i].numpy(), mode='L')\n",
    "         if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "         img_ar.append(img)\n",
    "         \n",
    "      if self.target_transform is not None:\n",
    "         target = self.target_transform(target)\n",
    "         \n",
    "      return img_ar, target\n",
    "   \n",
    "   def __len__(self):\n",
    "      if self.train:\n",
    "         return len(self.train_data)\n",
    "      else:\n",
    "         return len(self.test_data)\n",
    "      \n",
    "   def _check_exists(self):\n",
    "      return os.path.exists(os.path.join(self.root, self.processed_folder, self.training_file)) and \\\n",
    "         os.path.exists(os.path.join(self.root, self.processed_folder, self.test_file))\n",
    "   \n",
    "   def download(self):\n",
    "      \"\"\"Download the MNIST data if it doesn't exist in processed_folder already.\"\"\"\n",
    "      from six.moves import urllib\n",
    "      import gzip\n",
    "\n",
    "      if self._check_exists():\n",
    "         return\n",
    "\n",
    "      # download files\n",
    "      try:\n",
    "         os.makedirs(os.path.join(self.root, self.raw_folder))\n",
    "         os.makedirs(os.path.join(self.root, self.processed_folder))\n",
    "      except OSError as e:\n",
    "         if e.errno == errno.EEXIST:\n",
    "            pass\n",
    "         else:\n",
    "            raise\n",
    "\n",
    "      for url in self.urls:\n",
    "         print('Downloading ' + url)\n",
    "         data = urllib.request.urlopen(url)\n",
    "         filename = url.rpartition('/')[2]\n",
    "         file_path = os.path.join(self.root, self.raw_folder, filename)\n",
    "         with open(file_path, 'wb') as f:\n",
    "            f.write(data.read())\n",
    "         with open(file_path.replace('.gz', ''), 'wb') as out_f, \\\n",
    "               gzip.GzipFile(file_path) as zip_f:\n",
    "            out_f.write(zip_f.read())\n",
    "         os.unlink(file_path)\n",
    "\n",
    "      # process and save as torch files\n",
    "      print('Processing...')\n",
    "\n",
    "      training_set = (\n",
    "         read_image_file(os.path.join(self.root, self.raw_folder, 'train-images-idx3-ubyte')),\n",
    "         read_label_file(os.path.join(self.root, self.raw_folder, 'train-labels-idx1-ubyte'))\n",
    "      )\n",
    "      test_set = (\n",
    "         read_image_file(os.path.join(self.root, self.raw_folder, 't10k-images-idx3-ubyte')),\n",
    "         read_label_file(os.path.join(self.root, self.raw_folder, 't10k-labels-idx1-ubyte'))\n",
    "      )\n",
    "      with open(os.path.join(self.root, self.processed_folder, self.training_file), 'wb') as f:\n",
    "         torch.save(training_set, f)\n",
    "      with open(os.path.join(self.root, self.processed_folder, self.test_file), 'wb') as f:\n",
    "         torch.save(test_set, f)\n",
    "\n",
    "      print('Done!')\n",
    "\n",
    "   def __repr__(self):\n",
    "      fmt_str = 'Dataset ' + self.__class__.__name__ + '\\n'\n",
    "      fmt_str += '    Number of datapoints: {}\\n'.format(self.__len__())\n",
    "      tmp = 'train' if self.train is True else 'test'\n",
    "      fmt_str += '    Split: {}\\n'.format(tmp)\n",
    "      fmt_str += '    Root Location: {}\\n'.format(self.root)\n",
    "      tmp = '    Transforms (if any): '\n",
    "      fmt_str += '{0}{1}\\n'.format(tmp, self.transform.__repr__().replace('\\n', '\\n' + ' ' * len(tmp)))\n",
    "      tmp = '    Target Transforms (if any): '\n",
    "      fmt_str += '{0}{1}'.format(tmp, self.target_transform.__repr__().replace('\\n', '\\n' + ' ' * len(tmp)))\n",
    "      return fmt_str\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, 3, 1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, 1)\n",
    "        self.dropout1 = nn.Dropout2d(0.25)\n",
    "        self.dropout2 = nn.Dropout2d(0.5)\n",
    "        self.fc1 = nn.Linear(9216, 128)\n",
    "        \n",
    "    def forward(self, data):\n",
    "        res = []\n",
    "        for i in range(2):\n",
    "            x = data[i]\n",
    "            x = self.conv1(x)\n",
    "            x = F.relu(x)\n",
    "            x = self.conv2(x)\n",
    "            x = F.max_pool2d(x, 2)\n",
    "            x = self.dropout1(x)\n",
    "            x = torch.flatten(x, 1)\n",
    "            x = self.fc1(x)\n",
    "            res.append(x)\n",
    "        res = (1/2)*(F.kl_div(res[0], (res[0]+res[1])/2)) + \\\n",
    "        (1/2)*(F.kl_div(res[1], (res[0]+res[1])/2))\n",
    "        return torch.abs(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net1(nn.Module):\n",
    "   def __init__(self):\n",
    "      super().__init__()\n",
    "      \n",
    "      self.conv1 = nn.Conv2d(1, 64, 7)\n",
    "      self.pool1 = nn.MaxPool2d(2)\n",
    "      self.conv2 = nn.Conv2d(64, 128, 5)\n",
    "      self.conv3 = nn.Conv2d(128, 256, 5)\n",
    "      self.linear1 = nn.Linear(2304, 512)\n",
    "      \n",
    "      self.linear2 = nn.Linear(512, 2)\n",
    "      \n",
    "   def forward(self, data):\n",
    "      res = []\n",
    "      for i in range(2): # Siamese nets; sharing weights\n",
    "         x = data[i]\n",
    "         x = self.conv1(x)\n",
    "         x = F.relu(x)\n",
    "         x = self.pool1(x)\n",
    "         x = self.conv2(x)\n",
    "         x = F.relu(x)\n",
    "         x = self.conv3(x)\n",
    "         x = F.relu(x)\n",
    "         \n",
    "         x = x.view(x.shape[0], -1)\n",
    "         x = self.linear1(x)\n",
    "         res.append(F.relu(x))\n",
    "         \n",
    "      res = torch.abs(res[1] - res[0])\n",
    "      res = self.linear2(res)\n",
    "      return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, device, train_loader, epoch, optimizer):\n",
    "   model.train()\n",
    "   \n",
    "   for batch_idx, (data, target) in enumerate(train_loader):\n",
    "      for i in range(len(data)):\n",
    "         data[i] = data[i].to(device)\n",
    "#       print(data[:2][0].shape)  \n",
    "      optimizer.zero_grad()\n",
    "      output_positive = model(data[:2])\n",
    "      output_negative = model(data[0:3:2])\n",
    "      \n",
    "#       print(output_positive)\n",
    "      \n",
    "#       target = target.type(torch.LongTensor).to(device)\n",
    "#       target_positive = torch.squeeze(1-target[:,0])\n",
    "#       target_negative = torch.squeeze(1+target[:,1])\n",
    "#       print(target_positive)\n",
    "      \n",
    "      loss_positive = F.mse_loss(output_positive, torch.tensor([0]).type(torch.FloatTensor).to(device))\n",
    "      loss_negative = F.mse_loss(output_negative, torch.tensor([1]).type(torch.FloatTensor).to(device))\n",
    "#       loss_positive = F.cross_entropy(output_positive, target_positive)\n",
    "#       loss_negative = F.cross_entropy(output_negative, target_negative)\n",
    "      \n",
    "      loss = loss_positive + loss_negative\n",
    "      loss.backward()\n",
    "      \n",
    "      optimizer.step()\n",
    "      if batch_idx % 10 == 0:\n",
    "         print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "            epoch, batch_idx*batch_size, len(train_loader.dataset), 100. * batch_idx*batch_size / len(train_loader.dataset),\n",
    "            loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "trans = transforms.Compose([transforms.ToTensor()])\n",
    "train_loader = torch.utils.data.DataLoader(BalancedMNISTPair('../data', train=True, download=True, transform=trans), batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "a, b = iter(train_loader).next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 0])\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2)\n",
    "# fig.suptitle('Horizontally stacked subplots')\n",
    "ax1.imshow(a[0][4].numpy().squeeze());\n",
    "ax2.imshow(a[1][4].numpy().squeeze());\n",
    "print(b[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 1, 28, 28])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[:2][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, device, test_loader):\n",
    "   model.eval()\n",
    "   \n",
    "   with torch.no_grad():\n",
    "      accurate_labels = 0\n",
    "      all_labels = 0\n",
    "      loss = 0\n",
    "      for batch_idx, (data, target) in enumerate(test_loader):\n",
    "         for i in range(len(data)):\n",
    "            data[i] = data[i].to(device)\n",
    "            \n",
    "         output_positive = model(data[:2])\n",
    "         output_negative = model(data[0:3:2])\n",
    "            \n",
    "#          target = target.type(torch.LongTensor).to(device)\n",
    "#          target_positive = torch.squeeze(1-target[:,0])\n",
    "#          target_negative = torch.squeeze(1+target[:,1])\n",
    "            \n",
    "         loss_positive = F.mse_loss(output_positive, torch.tensor([0]).type(torch.FloatTensor).to(device))\n",
    "         loss_negative = F.mse_loss(output_negative, torch.tensor([1]).type(torch.FloatTensor).to(device))\n",
    "            \n",
    "         loss = loss + loss_positive + loss_negative\n",
    "            \n",
    "         accurate_labels_positive = torch.sum(torch.argmax(output_positive, dim=1) == target_positive).cpu()\n",
    "         accurate_labels_negative = torch.sum(torch.argmax(output_negative, dim=1) == target_negative).cpu()\n",
    "            \n",
    "         accurate_labels = accurate_labels + accurate_labels_positive + accurate_labels_negative\n",
    "         all_labels = all_labels + len(target_positive) + len(target_negative)\n",
    "      \n",
    "      accuracy = 100. * accurate_labels / all_labels\n",
    "      print('Test accuracy: {}/{} ({:.3f}%)\\tLoss: {:.6f}'.format(accurate_labels, all_labels, accuracy, loss))\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/pyramid/lib/python3.6/site-packages/ipykernel_launcher.py:19: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "/opt/miniconda3/envs/pyramid/lib/python3.6/site-packages/ipykernel_launcher.py:20: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [0/5000 (0%)]\tLoss: 0.842668\n",
      "Train Epoch: 0 [160/5000 (3%)]\tLoss: 1.000000\n",
      "Train Epoch: 0 [320/5000 (6%)]\tLoss: 1.000000\n",
      "Train Epoch: 0 [480/5000 (10%)]\tLoss: 1.000000\n",
      "Train Epoch: 0 [640/5000 (13%)]\tLoss: 1.000000\n",
      "Train Epoch: 0 [800/5000 (16%)]\tLoss: 1.000000\n",
      "Train Epoch: 0 [960/5000 (19%)]\tLoss: 1.000000\n",
      "Train Epoch: 0 [1120/5000 (22%)]\tLoss: 1.000000\n",
      "Train Epoch: 0 [1280/5000 (26%)]\tLoss: 1.000000\n",
      "Train Epoch: 0 [1440/5000 (29%)]\tLoss: 1.000000\n",
      "Train Epoch: 0 [1600/5000 (32%)]\tLoss: 1.000000\n",
      "Train Epoch: 0 [1760/5000 (35%)]\tLoss: 1.000000\n",
      "Train Epoch: 0 [1920/5000 (38%)]\tLoss: 1.000000\n",
      "Train Epoch: 0 [2080/5000 (42%)]\tLoss: 1.000000\n",
      "Train Epoch: 0 [2240/5000 (45%)]\tLoss: 1.000000\n",
      "Train Epoch: 0 [2400/5000 (48%)]\tLoss: 1.000000\n",
      "Train Epoch: 0 [2560/5000 (51%)]\tLoss: 1.000000\n",
      "Train Epoch: 0 [2720/5000 (54%)]\tLoss: 1.000000\n",
      "Train Epoch: 0 [2880/5000 (58%)]\tLoss: 1.000000\n",
      "Train Epoch: 0 [3040/5000 (61%)]\tLoss: 1.000000\n",
      "Train Epoch: 0 [3200/5000 (64%)]\tLoss: 1.000000\n",
      "Train Epoch: 0 [3360/5000 (67%)]\tLoss: 1.000000\n",
      "Train Epoch: 0 [3520/5000 (70%)]\tLoss: 1.000000\n",
      "Train Epoch: 0 [3680/5000 (74%)]\tLoss: 1.000000\n",
      "Train Epoch: 0 [3840/5000 (77%)]\tLoss: 1.000000\n",
      "Train Epoch: 0 [4000/5000 (80%)]\tLoss: 1.000000\n",
      "Train Epoch: 0 [4160/5000 (83%)]\tLoss: 1.000000\n",
      "Train Epoch: 0 [4320/5000 (86%)]\tLoss: 1.000000\n",
      "Train Epoch: 0 [4480/5000 (90%)]\tLoss: 1.000000\n",
      "Train Epoch: 0 [4640/5000 (93%)]\tLoss: 1.000000\n",
      "Train Epoch: 0 [4800/5000 (96%)]\tLoss: 1.000000\n",
      "Train Epoch: 0 [4960/5000 (99%)]\tLoss: 1.000000\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "Dimension out of range (expected to be in range of [-1, 0], but got 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-f4f603323843>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m    \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-19-f4f603323843>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m       \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m          \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m          \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m          \u001b[0;32mif\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;34m&\u001b[0m \u001b[0msave_frequency\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'siamese_{:03}.pt'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-18-5ad491d7c2f9>\u001b[0m in \u001b[0;36mtest\u001b[0;34m(model, device, test_loader)\u001b[0m\n\u001b[1;32m     22\u001b[0m          \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mloss_positive\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mloss_negative\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m          \u001b[0maccurate_labels_positive\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_positive\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mtarget_positive\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m          \u001b[0maccurate_labels_negative\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_negative\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mtarget_negative\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: Dimension out of range (expected to be in range of [-1, 0], but got 1)"
     ]
    }
   ],
   "source": [
    "def oneshot(model, device, data):\n",
    "   model.eval()\n",
    "\n",
    "   with torch.no_grad():\n",
    "      for i in range(len(data)):\n",
    "            data[i] = data[i].to(device)\n",
    "      \n",
    "      output = model(data)\n",
    "      return torch.squeeze(torch.argmax(output, dim=1)).cpu().item()\n",
    "\n",
    "def main():\n",
    "   device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "   trans = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (1.0,))])\n",
    "   \n",
    "   model = Net().to(device)\n",
    "   \n",
    "   if do_learn: # training mode\n",
    "      train_loader = torch.utils.data.DataLoader(BalancedMNISTPair('../data', train=True, download=True, transform=trans), batch_size=batch_size, shuffle=True)\n",
    "      test_loader = torch.utils.data.DataLoader(BalancedMNISTPair('../data', train=False, download=True, transform=trans), batch_size=batch_size, shuffle=False)\n",
    "      \n",
    "      optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "      for epoch in range(num_epochs):\n",
    "         train(model, device, train_loader, epoch, optimizer)\n",
    "         test(model, device, test_loader)\n",
    "         if epoch & save_frequency == 0:\n",
    "            torch.save(model, 'siamese_{:03}.pt'.format(epoch))\n",
    "   else: # prediction\n",
    "      prediction_loader = torch.utils.data.DataLoader(BalancedMNISTPair('../data', train=False, download=True, transform=trans), batch_size=1, shuffle=True)\n",
    "      model.load_state_dict(torch.load(load_model_path))\n",
    "      data = []\n",
    "      data.extend(next(iter(prediction_loader))[0][:3:2])\n",
    "      same = oneshot(model, device, data)\n",
    "      if same > 0:\n",
    "         print('These two images are of the same number')\n",
    "      else:\n",
    "         print('These two images are not of the same number')\n",
    "         \n",
    "if __name__ == '__main__':\n",
    "   main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
