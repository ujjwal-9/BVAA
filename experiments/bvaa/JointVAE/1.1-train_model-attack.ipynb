{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train a JointVAE model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get MNIST data\n",
    "Build a simple JointVAE model on MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataloaders import get_mnist_dataloaders\n",
    "train_loader, test_loader = get_mnist_dataloaders(batch_size=64, path_to_data='/home/data/bvaa')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define latent distribution of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Latent distribution will be joint distribution of 10 gaussian normal distributions\n",
    "# and one 10 dimensional Gumbel Softmax distribution\n",
    "latent_spec = {'cont': 10,\n",
    "               'disc': [10]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import VAE\n",
    "\n",
    "model = VAE(latent_spec=latent_spec, img_size=(1, 32, 32), use_cuda=True).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VAE(\n",
      "  (img_to_features): Sequential(\n",
      "    (0): Conv2d(1, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "    (1): ReLU()\n",
      "    (2): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "    (3): ReLU()\n",
      "    (4): Conv2d(64, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "    (5): ReLU()\n",
      "  )\n",
      "  (features_to_hidden): Sequential(\n",
      "    (0): Linear(in_features=1024, out_features=256, bias=True)\n",
      "    (1): ReLU()\n",
      "  )\n",
      "  (fc_mean): Linear(in_features=256, out_features=10, bias=True)\n",
      "  (fc_log_var): Linear(in_features=256, out_features=10, bias=True)\n",
      "  (fc_alphas): ModuleList(\n",
      "    (0): Linear(in_features=256, out_features=10, bias=True)\n",
      "  )\n",
      "  (latent_to_features): Sequential(\n",
      "    (0): Linear(in_features=20, out_features=256, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=256, out_features=1024, bias=True)\n",
      "    (3): ReLU()\n",
      "  )\n",
      "  (features_to_img): Sequential(\n",
      "    (0): ConvTranspose2d(64, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "    (1): ReLU()\n",
      "    (2): ConvTranspose2d(32, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "    (3): ReLU()\n",
      "    (4): ConvTranspose2d(32, 1, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "    (5): Sigmoid()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import optim\n",
    "\n",
    "# Build optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=5e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from training import Trainer\n",
    "\n",
    "# Define the capacities\n",
    "# Continuous channels\n",
    "cont_capacity = [0.0, 5.0, 25000, 30.0]  # Starting at a capacity of 0.0, increase this to 5.0\n",
    "                                         # over 25000 iterations with a gamma of 30.0\n",
    "# Discrete channels\n",
    "disc_capacity = [0.0, 5.0, 25000, 30.0]  # Starting at a capacity of 0.0, increase this to 5.0\n",
    "                                         # over 25000 iterations with a gamma of 30.0\n",
    "\n",
    "# Build a trainer\n",
    "trainer = Trainer(model, optimizer,\n",
    "                  cont_capacity=cont_capacity,\n",
    "                  disc_capacity=disc_capacity, use_cuda=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a visualizer which will be passed to trainer to visualize progress during training\n",
    "from visualize import Visualizer\n",
    "\n",
    "viz = Visualizer(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/60000\tLoss: 202.803\n",
      "3200/60000\tLoss: 195.045\n",
      "6400/60000\tLoss: 194.992\n",
      "9600/60000\tLoss: 194.591\n",
      "12800/60000\tLoss: 192.635\n",
      "16000/60000\tLoss: 193.942\n",
      "19200/60000\tLoss: 196.609\n",
      "22400/60000\tLoss: 195.896\n",
      "25600/60000\tLoss: 194.811\n",
      "28800/60000\tLoss: 192.771\n",
      "32000/60000\tLoss: 193.661\n",
      "35200/60000\tLoss: 191.263\n",
      "38400/60000\tLoss: 193.223\n",
      "41600/60000\tLoss: 196.881\n",
      "44800/60000\tLoss: 194.429\n",
      "48000/60000\tLoss: 195.191\n",
      "51200/60000\tLoss: 195.297\n",
      "54400/60000\tLoss: 195.665\n",
      "57600/60000\tLoss: 193.448\n",
      "Epoch: 1 Average loss: 194.55\n",
      "0/60000\tLoss: 177.216\n",
      "3200/60000\tLoss: 195.106\n",
      "6400/60000\tLoss: 192.467\n",
      "9600/60000\tLoss: 194.127\n",
      "12800/60000\tLoss: 190.481\n",
      "16000/60000\tLoss: 194.729\n",
      "19200/60000\tLoss: 193.166\n",
      "22400/60000\tLoss: 191.908\n",
      "25600/60000\tLoss: 194.566\n",
      "28800/60000\tLoss: 192.530\n",
      "32000/60000\tLoss: 193.156\n",
      "35200/60000\tLoss: 191.442\n",
      "38400/60000\tLoss: 193.701\n",
      "41600/60000\tLoss: 192.114\n",
      "44800/60000\tLoss: 192.983\n",
      "48000/60000\tLoss: 195.897\n",
      "51200/60000\tLoss: 194.599\n",
      "54400/60000\tLoss: 194.741\n",
      "57600/60000\tLoss: 194.513\n",
      "Epoch: 2 Average loss: 193.64\n",
      "0/60000\tLoss: 190.958\n",
      "3200/60000\tLoss: 195.655\n",
      "6400/60000\tLoss: 195.515\n",
      "9600/60000\tLoss: 194.744\n",
      "12800/60000\tLoss: 195.830\n",
      "16000/60000\tLoss: 194.466\n",
      "19200/60000\tLoss: 195.450\n",
      "22400/60000\tLoss: 193.372\n",
      "25600/60000\tLoss: 194.689\n",
      "28800/60000\tLoss: 196.191\n",
      "32000/60000\tLoss: 195.323\n",
      "35200/60000\tLoss: 195.209\n",
      "38400/60000\tLoss: 192.866\n",
      "41600/60000\tLoss: 193.166\n",
      "44800/60000\tLoss: 196.114\n",
      "48000/60000\tLoss: 192.669\n",
      "51200/60000\tLoss: 193.928\n",
      "54400/60000\tLoss: 192.738\n",
      "57600/60000\tLoss: 194.011\n",
      "Epoch: 3 Average loss: 194.56\n",
      "0/60000\tLoss: 209.674\n",
      "3200/60000\tLoss: 193.216\n",
      "6400/60000\tLoss: 195.232\n",
      "9600/60000\tLoss: 191.668\n",
      "12800/60000\tLoss: 192.023\n",
      "16000/60000\tLoss: 191.856\n",
      "19200/60000\tLoss: 192.083\n",
      "22400/60000\tLoss: 192.717\n",
      "25600/60000\tLoss: 190.584\n",
      "28800/60000\tLoss: 192.951\n",
      "32000/60000\tLoss: 191.532\n",
      "35200/60000\tLoss: 192.086\n",
      "38400/60000\tLoss: 190.924\n",
      "41600/60000\tLoss: 191.910\n",
      "44800/60000\tLoss: 190.891\n",
      "48000/60000\tLoss: 192.902\n",
      "51200/60000\tLoss: 190.756\n",
      "54400/60000\tLoss: 192.583\n",
      "57600/60000\tLoss: 191.152\n",
      "Epoch: 4 Average loss: 192.14\n",
      "0/60000\tLoss: 197.926\n",
      "3200/60000\tLoss: 193.208\n",
      "6400/60000\tLoss: 190.505\n",
      "9600/60000\tLoss: 190.028\n",
      "12800/60000\tLoss: 191.153\n",
      "16000/60000\tLoss: 190.303\n",
      "19200/60000\tLoss: 190.927\n",
      "22400/60000\tLoss: 189.618\n",
      "25600/60000\tLoss: 192.238\n",
      "28800/60000\tLoss: 189.050\n",
      "32000/60000\tLoss: 191.866\n",
      "35200/60000\tLoss: 190.061\n",
      "38400/60000\tLoss: 190.337\n",
      "41600/60000\tLoss: 191.381\n",
      "44800/60000\tLoss: 188.983\n",
      "48000/60000\tLoss: 189.901\n",
      "51200/60000\tLoss: 189.825\n",
      "54400/60000\tLoss: 189.160\n",
      "57600/60000\tLoss: 190.300\n",
      "Epoch: 5 Average loss: 190.54\n",
      "0/60000\tLoss: 199.251\n",
      "3200/60000\tLoss: 190.104\n",
      "6400/60000\tLoss: 189.225\n",
      "9600/60000\tLoss: 187.877\n",
      "12800/60000\tLoss: 188.418\n",
      "16000/60000\tLoss: 188.137\n",
      "19200/60000\tLoss: 188.926\n",
      "22400/60000\tLoss: 187.564\n",
      "25600/60000\tLoss: 187.520\n",
      "28800/60000\tLoss: 188.967\n",
      "32000/60000\tLoss: 189.272\n",
      "35200/60000\tLoss: 187.369\n",
      "38400/60000\tLoss: 189.244\n",
      "41600/60000\tLoss: 187.350\n",
      "44800/60000\tLoss: 186.339\n",
      "48000/60000\tLoss: 189.752\n",
      "51200/60000\tLoss: 188.036\n",
      "54400/60000\tLoss: 189.796\n",
      "57600/60000\tLoss: 188.202\n",
      "Epoch: 6 Average loss: 188.47\n",
      "0/60000\tLoss: 185.335\n",
      "3200/60000\tLoss: 188.199\n",
      "6400/60000\tLoss: 185.772\n",
      "9600/60000\tLoss: 186.842\n",
      "12800/60000\tLoss: 186.842\n",
      "16000/60000\tLoss: 187.506\n",
      "19200/60000\tLoss: 185.999\n",
      "22400/60000\tLoss: 186.075\n",
      "25600/60000\tLoss: 185.951\n",
      "28800/60000\tLoss: 185.563\n",
      "32000/60000\tLoss: 190.171\n",
      "35200/60000\tLoss: 188.744\n",
      "38400/60000\tLoss: 187.129\n",
      "41600/60000\tLoss: 184.812\n",
      "44800/60000\tLoss: 185.609\n",
      "48000/60000\tLoss: 188.228\n",
      "51200/60000\tLoss: 187.243\n",
      "54400/60000\tLoss: 185.423\n",
      "57600/60000\tLoss: 186.285\n",
      "Epoch: 7 Average loss: 186.78\n",
      "0/60000\tLoss: 174.154\n",
      "3200/60000\tLoss: 187.787\n",
      "6400/60000\tLoss: 185.940\n",
      "9600/60000\tLoss: 184.224\n",
      "12800/60000\tLoss: 185.138\n",
      "16000/60000\tLoss: 184.069\n",
      "19200/60000\tLoss: 186.966\n",
      "22400/60000\tLoss: 186.427\n",
      "25600/60000\tLoss: 183.131\n",
      "28800/60000\tLoss: 184.712\n",
      "32000/60000\tLoss: 185.584\n",
      "35200/60000\tLoss: 185.716\n",
      "38400/60000\tLoss: 183.803\n",
      "41600/60000\tLoss: 183.202\n",
      "44800/60000\tLoss: 182.806\n",
      "48000/60000\tLoss: 183.131\n",
      "51200/60000\tLoss: 183.678\n",
      "54400/60000\tLoss: 182.480\n",
      "57600/60000\tLoss: 184.267\n",
      "Epoch: 8 Average loss: 184.71\n",
      "0/60000\tLoss: 188.893\n",
      "3200/60000\tLoss: 183.395\n",
      "6400/60000\tLoss: 184.341\n",
      "9600/60000\tLoss: 183.719\n",
      "12800/60000\tLoss: 183.970\n",
      "16000/60000\tLoss: 182.605\n",
      "19200/60000\tLoss: 185.109\n",
      "22400/60000\tLoss: 182.612\n",
      "25600/60000\tLoss: 181.818\n",
      "28800/60000\tLoss: 185.642\n",
      "32000/60000\tLoss: 182.516\n",
      "35200/60000\tLoss: 182.251\n",
      "38400/60000\tLoss: 182.258\n",
      "41600/60000\tLoss: 183.306\n",
      "44800/60000\tLoss: 180.413\n",
      "48000/60000\tLoss: 182.410\n",
      "51200/60000\tLoss: 183.715\n",
      "54400/60000\tLoss: 183.214\n",
      "57600/60000\tLoss: 182.676\n",
      "Epoch: 9 Average loss: 183.19\n",
      "0/60000\tLoss: 186.896\n",
      "3200/60000\tLoss: 182.220\n",
      "6400/60000\tLoss: 179.763\n",
      "9600/60000\tLoss: 180.757\n",
      "12800/60000\tLoss: 179.747\n",
      "16000/60000\tLoss: 180.264\n",
      "19200/60000\tLoss: 184.476\n",
      "22400/60000\tLoss: 181.992\n",
      "25600/60000\tLoss: 182.030\n",
      "28800/60000\tLoss: 180.726\n",
      "32000/60000\tLoss: 181.780\n",
      "35200/60000\tLoss: 180.315\n",
      "38400/60000\tLoss: 183.225\n",
      "41600/60000\tLoss: 179.114\n",
      "44800/60000\tLoss: 178.300\n",
      "48000/60000\tLoss: 181.994\n",
      "51200/60000\tLoss: 179.771\n",
      "54400/60000\tLoss: 182.814\n",
      "57600/60000\tLoss: 179.781\n",
      "Epoch: 10 Average loss: 181.18\n",
      "0/60000\tLoss: 175.280\n",
      "3200/60000\tLoss: 182.556\n",
      "6400/60000\tLoss: 179.855\n",
      "9600/60000\tLoss: 180.118\n",
      "12800/60000\tLoss: 179.088\n",
      "16000/60000\tLoss: 178.955\n",
      "19200/60000\tLoss: 178.996\n",
      "22400/60000\tLoss: 181.415\n",
      "25600/60000\tLoss: 181.867\n",
      "28800/60000\tLoss: 179.753\n",
      "32000/60000\tLoss: 181.480\n",
      "35200/60000\tLoss: 176.934\n",
      "38400/60000\tLoss: 180.398\n",
      "41600/60000\tLoss: 179.528\n",
      "44800/60000\tLoss: 179.452\n",
      "48000/60000\tLoss: 180.575\n",
      "51200/60000\tLoss: 179.028\n",
      "54400/60000\tLoss: 178.349\n",
      "57600/60000\tLoss: 179.938\n",
      "Epoch: 11 Average loss: 179.93\n",
      "0/60000\tLoss: 186.317\n",
      "3200/60000\tLoss: 176.282\n",
      "6400/60000\tLoss: 178.628\n",
      "9600/60000\tLoss: 178.622\n",
      "12800/60000\tLoss: 181.736\n",
      "16000/60000\tLoss: 178.095\n",
      "19200/60000\tLoss: 177.644\n",
      "22400/60000\tLoss: 177.833\n",
      "25600/60000\tLoss: 176.254\n",
      "28800/60000\tLoss: 178.955\n",
      "32000/60000\tLoss: 177.812\n",
      "35200/60000\tLoss: 179.298\n",
      "38400/60000\tLoss: 177.288\n",
      "41600/60000\tLoss: 177.661\n",
      "44800/60000\tLoss: 178.314\n",
      "48000/60000\tLoss: 176.626\n",
      "51200/60000\tLoss: 176.316\n",
      "54400/60000\tLoss: 177.781\n",
      "57600/60000\tLoss: 177.737\n",
      "Epoch: 12 Average loss: 178.01\n",
      "0/60000\tLoss: 173.243\n",
      "3200/60000\tLoss: 177.787\n",
      "6400/60000\tLoss: 176.114\n",
      "9600/60000\tLoss: 175.870\n",
      "12800/60000\tLoss: 176.926\n",
      "16000/60000\tLoss: 172.979\n",
      "19200/60000\tLoss: 175.500\n",
      "22400/60000\tLoss: 177.664\n",
      "25600/60000\tLoss: 177.365\n",
      "28800/60000\tLoss: 175.482\n",
      "32000/60000\tLoss: 178.474\n",
      "35200/60000\tLoss: 174.793\n",
      "38400/60000\tLoss: 176.142\n",
      "41600/60000\tLoss: 176.692\n",
      "44800/60000\tLoss: 179.115\n",
      "48000/60000\tLoss: 177.365\n",
      "51200/60000\tLoss: 174.992\n",
      "54400/60000\tLoss: 177.195\n",
      "57600/60000\tLoss: 176.882\n",
      "Epoch: 13 Average loss: 176.70\n",
      "0/60000\tLoss: 174.439\n",
      "3200/60000\tLoss: 174.623\n",
      "6400/60000\tLoss: 177.313\n",
      "9600/60000\tLoss: 174.496\n",
      "12800/60000\tLoss: 176.301\n",
      "16000/60000\tLoss: 176.007\n",
      "19200/60000\tLoss: 173.290\n",
      "22400/60000\tLoss: 173.477\n",
      "25600/60000\tLoss: 175.300\n",
      "28800/60000\tLoss: 174.397\n",
      "32000/60000\tLoss: 176.798\n",
      "35200/60000\tLoss: 174.332\n",
      "38400/60000\tLoss: 176.279\n",
      "41600/60000\tLoss: 173.284\n",
      "44800/60000\tLoss: 175.713\n",
      "48000/60000\tLoss: 174.130\n",
      "51200/60000\tLoss: 174.570\n",
      "54400/60000\tLoss: 171.309\n",
      "57600/60000\tLoss: 174.785\n",
      "Epoch: 14 Average loss: 174.90\n",
      "0/60000\tLoss: 177.924\n",
      "3200/60000\tLoss: 171.835\n",
      "6400/60000\tLoss: 177.223\n",
      "9600/60000\tLoss: 174.133\n",
      "12800/60000\tLoss: 174.997\n",
      "16000/60000\tLoss: 173.999\n",
      "19200/60000\tLoss: 176.253\n",
      "22400/60000\tLoss: 173.418\n",
      "25600/60000\tLoss: 173.078\n",
      "28800/60000\tLoss: 173.212\n",
      "32000/60000\tLoss: 176.493\n",
      "35200/60000\tLoss: 175.537\n",
      "38400/60000\tLoss: 176.352\n",
      "41600/60000\tLoss: 172.079\n",
      "44800/60000\tLoss: 172.098\n",
      "48000/60000\tLoss: 173.655\n",
      "51200/60000\tLoss: 173.556\n",
      "54400/60000\tLoss: 173.438\n",
      "57600/60000\tLoss: 171.199\n",
      "Epoch: 15 Average loss: 174.11\n",
      "0/60000\tLoss: 158.347\n",
      "3200/60000\tLoss: 173.330\n",
      "6400/60000\tLoss: 173.210\n",
      "9600/60000\tLoss: 173.177\n",
      "12800/60000\tLoss: 172.888\n",
      "16000/60000\tLoss: 175.274\n",
      "19200/60000\tLoss: 172.487\n",
      "22400/60000\tLoss: 171.492\n",
      "25600/60000\tLoss: 171.891\n",
      "28800/60000\tLoss: 175.606\n",
      "32000/60000\tLoss: 172.763\n",
      "35200/60000\tLoss: 173.866\n",
      "38400/60000\tLoss: 170.205\n",
      "41600/60000\tLoss: 170.322\n",
      "44800/60000\tLoss: 172.583\n",
      "48000/60000\tLoss: 170.249\n",
      "51200/60000\tLoss: 171.372\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "54400/60000\tLoss: 170.931\n",
      "57600/60000\tLoss: 172.958\n",
      "Epoch: 16 Average loss: 172.58\n",
      "0/60000\tLoss: 152.160\n",
      "3200/60000\tLoss: 170.990\n",
      "6400/60000\tLoss: 171.493\n",
      "9600/60000\tLoss: 171.743\n",
      "12800/60000\tLoss: 171.578\n",
      "16000/60000\tLoss: 170.693\n",
      "19200/60000\tLoss: 172.805\n",
      "22400/60000\tLoss: 172.402\n",
      "25600/60000\tLoss: 170.499\n",
      "28800/60000\tLoss: 170.516\n",
      "32000/60000\tLoss: 172.892\n",
      "35200/60000\tLoss: 171.441\n",
      "38400/60000\tLoss: 172.410\n",
      "41600/60000\tLoss: 173.063\n",
      "44800/60000\tLoss: 169.688\n",
      "48000/60000\tLoss: 173.703\n",
      "51200/60000\tLoss: 172.003\n",
      "54400/60000\tLoss: 170.673\n",
      "57600/60000\tLoss: 171.164\n",
      "Epoch: 17 Average loss: 171.61\n",
      "0/60000\tLoss: 177.490\n",
      "3200/60000\tLoss: 168.382\n",
      "6400/60000\tLoss: 171.973\n",
      "9600/60000\tLoss: 172.332\n",
      "12800/60000\tLoss: 169.559\n",
      "16000/60000\tLoss: 170.160\n",
      "19200/60000\tLoss: 174.196\n",
      "22400/60000\tLoss: 171.334\n",
      "25600/60000\tLoss: 169.777\n",
      "28800/60000\tLoss: 169.985\n",
      "32000/60000\tLoss: 170.498\n",
      "35200/60000\tLoss: 168.651\n",
      "38400/60000\tLoss: 168.381\n",
      "41600/60000\tLoss: 168.478\n",
      "44800/60000\tLoss: 172.977\n",
      "48000/60000\tLoss: 171.765\n",
      "51200/60000\tLoss: 171.673\n",
      "54400/60000\tLoss: 171.881\n",
      "57600/60000\tLoss: 168.607\n",
      "Epoch: 18 Average loss: 170.70\n",
      "0/60000\tLoss: 167.558\n",
      "3200/60000\tLoss: 173.346\n",
      "6400/60000\tLoss: 171.508\n",
      "9600/60000\tLoss: 170.616\n",
      "12800/60000\tLoss: 167.334\n",
      "16000/60000\tLoss: 169.124\n",
      "19200/60000\tLoss: 168.895\n",
      "22400/60000\tLoss: 169.873\n",
      "25600/60000\tLoss: 170.779\n",
      "28800/60000\tLoss: 171.097\n",
      "32000/60000\tLoss: 172.092\n",
      "35200/60000\tLoss: 168.730\n",
      "38400/60000\tLoss: 170.580\n",
      "41600/60000\tLoss: 168.412\n",
      "44800/60000\tLoss: 171.443\n",
      "48000/60000\tLoss: 170.393\n",
      "51200/60000\tLoss: 170.339\n",
      "54400/60000\tLoss: 169.613\n",
      "57600/60000\tLoss: 169.818\n",
      "Epoch: 19 Average loss: 170.26\n",
      "0/60000\tLoss: 164.294\n",
      "3200/60000\tLoss: 169.386\n",
      "6400/60000\tLoss: 168.506\n",
      "9600/60000\tLoss: 168.384\n",
      "12800/60000\tLoss: 170.082\n",
      "16000/60000\tLoss: 167.687\n",
      "19200/60000\tLoss: 170.741\n",
      "22400/60000\tLoss: 171.084\n",
      "25600/60000\tLoss: 168.647\n",
      "28800/60000\tLoss: 168.867\n",
      "32000/60000\tLoss: 169.518\n",
      "35200/60000\tLoss: 168.666\n",
      "38400/60000\tLoss: 169.729\n",
      "41600/60000\tLoss: 170.529\n",
      "44800/60000\tLoss: 170.762\n",
      "48000/60000\tLoss: 170.340\n",
      "51200/60000\tLoss: 170.224\n",
      "54400/60000\tLoss: 170.369\n",
      "57600/60000\tLoss: 170.533\n",
      "Epoch: 20 Average loss: 169.80\n",
      "0/60000\tLoss: 168.849\n",
      "3200/60000\tLoss: 169.576\n",
      "6400/60000\tLoss: 169.484\n",
      "9600/60000\tLoss: 170.626\n",
      "12800/60000\tLoss: 169.888\n",
      "16000/60000\tLoss: 169.237\n",
      "19200/60000\tLoss: 170.985\n",
      "22400/60000\tLoss: 169.519\n",
      "25600/60000\tLoss: 168.597\n",
      "28800/60000\tLoss: 169.676\n",
      "32000/60000\tLoss: 166.964\n",
      "35200/60000\tLoss: 168.092\n",
      "38400/60000\tLoss: 169.112\n",
      "41600/60000\tLoss: 169.130\n",
      "44800/60000\tLoss: 168.354\n",
      "48000/60000\tLoss: 170.002\n",
      "51200/60000\tLoss: 168.997\n",
      "54400/60000\tLoss: 169.772\n",
      "57600/60000\tLoss: 169.320\n",
      "Epoch: 21 Average loss: 169.27\n",
      "0/60000\tLoss: 175.154\n",
      "3200/60000\tLoss: 169.586\n",
      "6400/60000\tLoss: 170.245\n",
      "9600/60000\tLoss: 169.522\n",
      "12800/60000\tLoss: 169.154\n",
      "16000/60000\tLoss: 168.197\n",
      "19200/60000\tLoss: 170.168\n",
      "22400/60000\tLoss: 169.272\n",
      "25600/60000\tLoss: 168.415\n",
      "28800/60000\tLoss: 169.145\n",
      "32000/60000\tLoss: 165.973\n",
      "35200/60000\tLoss: 166.621\n",
      "38400/60000\tLoss: 168.168\n",
      "41600/60000\tLoss: 168.950\n",
      "44800/60000\tLoss: 170.743\n",
      "48000/60000\tLoss: 170.589\n",
      "51200/60000\tLoss: 169.159\n",
      "54400/60000\tLoss: 169.040\n",
      "57600/60000\tLoss: 168.257\n",
      "Epoch: 22 Average loss: 169.15\n",
      "0/60000\tLoss: 164.630\n",
      "3200/60000\tLoss: 168.118\n",
      "6400/60000\tLoss: 169.918\n",
      "9600/60000\tLoss: 169.819\n",
      "12800/60000\tLoss: 168.869\n",
      "16000/60000\tLoss: 169.005\n",
      "19200/60000\tLoss: 168.744\n",
      "22400/60000\tLoss: 167.745\n",
      "25600/60000\tLoss: 168.461\n",
      "28800/60000\tLoss: 167.572\n",
      "32000/60000\tLoss: 169.655\n",
      "35200/60000\tLoss: 168.479\n",
      "38400/60000\tLoss: 168.399\n",
      "41600/60000\tLoss: 169.566\n",
      "44800/60000\tLoss: 169.477\n",
      "48000/60000\tLoss: 169.542\n",
      "51200/60000\tLoss: 168.030\n",
      "54400/60000\tLoss: 167.745\n",
      "57600/60000\tLoss: 167.903\n",
      "Epoch: 23 Average loss: 168.72\n",
      "0/60000\tLoss: 166.118\n",
      "3200/60000\tLoss: 167.392\n",
      "6400/60000\tLoss: 166.560\n",
      "9600/60000\tLoss: 168.142\n",
      "12800/60000\tLoss: 171.032\n",
      "16000/60000\tLoss: 168.344\n",
      "19200/60000\tLoss: 168.506\n",
      "22400/60000\tLoss: 169.434\n",
      "25600/60000\tLoss: 166.879\n",
      "28800/60000\tLoss: 168.236\n",
      "32000/60000\tLoss: 169.926\n",
      "35200/60000\tLoss: 168.473\n",
      "38400/60000\tLoss: 167.685\n",
      "41600/60000\tLoss: 168.016\n",
      "44800/60000\tLoss: 168.947\n",
      "48000/60000\tLoss: 170.202\n",
      "51200/60000\tLoss: 168.473\n",
      "54400/60000\tLoss: 168.473\n",
      "57600/60000\tLoss: 167.273\n",
      "Epoch: 24 Average loss: 168.56\n",
      "0/60000\tLoss: 179.786\n",
      "3200/60000\tLoss: 167.421\n",
      "6400/60000\tLoss: 169.275\n",
      "9600/60000\tLoss: 168.099\n",
      "12800/60000\tLoss: 170.266\n",
      "16000/60000\tLoss: 168.691\n",
      "19200/60000\tLoss: 167.867\n",
      "22400/60000\tLoss: 165.245\n",
      "25600/60000\tLoss: 167.097\n",
      "28800/60000\tLoss: 169.177\n",
      "32000/60000\tLoss: 167.620\n",
      "35200/60000\tLoss: 169.807\n",
      "38400/60000\tLoss: 167.478\n",
      "41600/60000\tLoss: 165.647\n",
      "44800/60000\tLoss: 169.557\n",
      "48000/60000\tLoss: 169.853\n",
      "51200/60000\tLoss: 169.973\n",
      "54400/60000\tLoss: 170.360\n",
      "57600/60000\tLoss: 167.920\n",
      "Epoch: 25 Average loss: 168.44\n",
      "0/60000\tLoss: 159.289\n",
      "3200/60000\tLoss: 166.906\n",
      "6400/60000\tLoss: 168.078\n",
      "9600/60000\tLoss: 167.297\n",
      "12800/60000\tLoss: 171.976\n",
      "16000/60000\tLoss: 164.919\n",
      "19200/60000\tLoss: 166.787\n",
      "22400/60000\tLoss: 170.316\n",
      "25600/60000\tLoss: 169.907\n",
      "28800/60000\tLoss: 168.091\n",
      "32000/60000\tLoss: 167.178\n",
      "35200/60000\tLoss: 166.001\n",
      "38400/60000\tLoss: 168.837\n",
      "41600/60000\tLoss: 171.226\n",
      "44800/60000\tLoss: 167.715\n",
      "48000/60000\tLoss: 168.504\n",
      "51200/60000\tLoss: 169.139\n",
      "54400/60000\tLoss: 167.757\n",
      "57600/60000\tLoss: 170.385\n",
      "Epoch: 26 Average loss: 168.51\n",
      "0/60000\tLoss: 165.940\n",
      "3200/60000\tLoss: 167.008\n",
      "6400/60000\tLoss: 166.970\n",
      "9600/60000\tLoss: 169.406\n",
      "12800/60000\tLoss: 168.517\n",
      "16000/60000\tLoss: 166.082\n",
      "19200/60000\tLoss: 166.109\n",
      "22400/60000\tLoss: 167.279\n",
      "25600/60000\tLoss: 168.189\n",
      "28800/60000\tLoss: 168.335\n",
      "32000/60000\tLoss: 166.980\n",
      "35200/60000\tLoss: 166.810\n",
      "38400/60000\tLoss: 169.835\n",
      "41600/60000\tLoss: 166.870\n",
      "44800/60000\tLoss: 170.222\n",
      "48000/60000\tLoss: 167.605\n",
      "51200/60000\tLoss: 166.813\n",
      "54400/60000\tLoss: 167.845\n",
      "57600/60000\tLoss: 168.782\n",
      "Epoch: 27 Average loss: 167.89\n",
      "0/60000\tLoss: 173.208\n",
      "3200/60000\tLoss: 168.872\n",
      "6400/60000\tLoss: 166.813\n",
      "9600/60000\tLoss: 168.307\n",
      "12800/60000\tLoss: 168.737\n",
      "16000/60000\tLoss: 169.304\n",
      "19200/60000\tLoss: 167.948\n",
      "22400/60000\tLoss: 166.549\n",
      "25600/60000\tLoss: 168.761\n",
      "28800/60000\tLoss: 168.084\n",
      "32000/60000\tLoss: 166.960\n",
      "35200/60000\tLoss: 168.724\n",
      "38400/60000\tLoss: 168.381\n",
      "41600/60000\tLoss: 166.995\n",
      "44800/60000\tLoss: 167.143\n",
      "48000/60000\tLoss: 167.045\n",
      "51200/60000\tLoss: 167.874\n",
      "54400/60000\tLoss: 166.702\n",
      "57600/60000\tLoss: 170.365\n",
      "Epoch: 28 Average loss: 168.10\n",
      "0/60000\tLoss: 165.026\n",
      "3200/60000\tLoss: 166.266\n",
      "6400/60000\tLoss: 169.402\n",
      "9600/60000\tLoss: 167.958\n",
      "12800/60000\tLoss: 168.110\n",
      "16000/60000\tLoss: 168.303\n",
      "19200/60000\tLoss: 166.369\n",
      "22400/60000\tLoss: 168.016\n",
      "25600/60000\tLoss: 168.494\n",
      "28800/60000\tLoss: 168.532\n",
      "32000/60000\tLoss: 167.942\n",
      "35200/60000\tLoss: 168.784\n",
      "38400/60000\tLoss: 168.156\n",
      "41600/60000\tLoss: 167.092\n",
      "44800/60000\tLoss: 167.778\n",
      "48000/60000\tLoss: 167.728\n",
      "51200/60000\tLoss: 167.988\n",
      "54400/60000\tLoss: 168.230\n",
      "57600/60000\tLoss: 169.132\n",
      "Epoch: 29 Average loss: 168.00\n",
      "0/60000\tLoss: 174.262\n",
      "3200/60000\tLoss: 170.042\n",
      "6400/60000\tLoss: 167.364\n",
      "9600/60000\tLoss: 167.204\n",
      "12800/60000\tLoss: 169.050\n",
      "16000/60000\tLoss: 166.635\n",
      "19200/60000\tLoss: 165.441\n",
      "22400/60000\tLoss: 167.008\n",
      "25600/60000\tLoss: 167.267\n",
      "28800/60000\tLoss: 168.156\n",
      "32000/60000\tLoss: 168.209\n",
      "35200/60000\tLoss: 165.954\n",
      "38400/60000\tLoss: 168.854\n",
      "41600/60000\tLoss: 165.911\n",
      "44800/60000\tLoss: 165.772\n",
      "48000/60000\tLoss: 167.857\n",
      "51200/60000\tLoss: 167.431\n",
      "54400/60000\tLoss: 167.049\n",
      "57600/60000\tLoss: 168.839\n",
      "Epoch: 30 Average loss: 167.73\n",
      "0/60000\tLoss: 167.460\n",
      "3200/60000\tLoss: 165.618\n",
      "6400/60000\tLoss: 166.541\n",
      "9600/60000\tLoss: 167.236\n",
      "12800/60000\tLoss: 166.406\n",
      "16000/60000\tLoss: 167.806\n",
      "19200/60000\tLoss: 166.988\n",
      "22400/60000\tLoss: 167.007\n",
      "25600/60000\tLoss: 167.689\n",
      "28800/60000\tLoss: 166.025\n",
      "32000/60000\tLoss: 168.085\n",
      "35200/60000\tLoss: 167.967\n",
      "38400/60000\tLoss: 165.347\n",
      "41600/60000\tLoss: 168.038\n",
      "44800/60000\tLoss: 167.531\n",
      "48000/60000\tLoss: 168.527\n",
      "51200/60000\tLoss: 167.619\n",
      "54400/60000\tLoss: 168.461\n",
      "57600/60000\tLoss: 167.586\n",
      "Epoch: 31 Average loss: 167.38\n",
      "0/60000\tLoss: 162.016\n",
      "3200/60000\tLoss: 166.661\n",
      "6400/60000\tLoss: 167.095\n",
      "9600/60000\tLoss: 166.425\n",
      "12800/60000\tLoss: 167.484\n",
      "16000/60000\tLoss: 166.870\n",
      "19200/60000\tLoss: 167.347\n",
      "22400/60000\tLoss: 168.872\n",
      "25600/60000\tLoss: 168.889\n",
      "28800/60000\tLoss: 166.350\n",
      "32000/60000\tLoss: 166.628\n",
      "35200/60000\tLoss: 167.682\n",
      "38400/60000\tLoss: 167.297\n",
      "41600/60000\tLoss: 167.346\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44800/60000\tLoss: 167.332\n",
      "48000/60000\tLoss: 168.281\n",
      "51200/60000\tLoss: 169.387\n",
      "54400/60000\tLoss: 167.163\n",
      "57600/60000\tLoss: 165.806\n",
      "Epoch: 32 Average loss: 167.49\n",
      "0/60000\tLoss: 167.840\n",
      "3200/60000\tLoss: 168.201\n",
      "6400/60000\tLoss: 166.375\n",
      "9600/60000\tLoss: 167.321\n",
      "12800/60000\tLoss: 165.509\n",
      "16000/60000\tLoss: 167.278\n",
      "19200/60000\tLoss: 167.799\n",
      "22400/60000\tLoss: 167.636\n",
      "25600/60000\tLoss: 166.278\n",
      "28800/60000\tLoss: 168.016\n",
      "32000/60000\tLoss: 168.354\n",
      "35200/60000\tLoss: 166.112\n",
      "38400/60000\tLoss: 166.703\n",
      "41600/60000\tLoss: 167.440\n",
      "44800/60000\tLoss: 167.694\n",
      "48000/60000\tLoss: 167.059\n",
      "51200/60000\tLoss: 165.799\n",
      "54400/60000\tLoss: 166.963\n",
      "57600/60000\tLoss: 166.873\n",
      "Epoch: 33 Average loss: 167.07\n",
      "0/60000\tLoss: 174.957\n",
      "3200/60000\tLoss: 168.246\n",
      "6400/60000\tLoss: 166.767\n",
      "9600/60000\tLoss: 170.117\n",
      "12800/60000\tLoss: 166.177\n",
      "16000/60000\tLoss: 165.521\n",
      "19200/60000\tLoss: 167.533\n",
      "22400/60000\tLoss: 165.415\n",
      "25600/60000\tLoss: 166.786\n",
      "28800/60000\tLoss: 165.623\n",
      "32000/60000\tLoss: 164.728\n",
      "35200/60000\tLoss: 166.052\n",
      "38400/60000\tLoss: 170.529\n",
      "41600/60000\tLoss: 167.560\n",
      "44800/60000\tLoss: 166.828\n",
      "48000/60000\tLoss: 166.219\n",
      "51200/60000\tLoss: 166.028\n",
      "54400/60000\tLoss: 168.667\n",
      "57600/60000\tLoss: 169.357\n",
      "Epoch: 34 Average loss: 167.23\n",
      "0/60000\tLoss: 155.399\n",
      "3200/60000\tLoss: 168.692\n",
      "6400/60000\tLoss: 165.816\n",
      "9600/60000\tLoss: 164.304\n",
      "12800/60000\tLoss: 165.740\n",
      "16000/60000\tLoss: 168.366\n",
      "19200/60000\tLoss: 166.745\n",
      "22400/60000\tLoss: 166.012\n",
      "25600/60000\tLoss: 166.547\n",
      "28800/60000\tLoss: 165.880\n",
      "32000/60000\tLoss: 168.475\n",
      "35200/60000\tLoss: 167.395\n",
      "38400/60000\tLoss: 166.777\n",
      "41600/60000\tLoss: 165.506\n",
      "44800/60000\tLoss: 165.942\n",
      "48000/60000\tLoss: 166.895\n",
      "51200/60000\tLoss: 168.351\n",
      "54400/60000\tLoss: 166.656\n",
      "57600/60000\tLoss: 166.516\n",
      "Epoch: 35 Average loss: 166.84\n",
      "0/60000\tLoss: 161.154\n",
      "3200/60000\tLoss: 165.381\n",
      "6400/60000\tLoss: 166.269\n",
      "9600/60000\tLoss: 167.210\n",
      "12800/60000\tLoss: 167.763\n",
      "16000/60000\tLoss: 165.580\n",
      "19200/60000\tLoss: 169.989\n",
      "22400/60000\tLoss: 167.745\n",
      "25600/60000\tLoss: 165.844\n",
      "28800/60000\tLoss: 166.523\n",
      "32000/60000\tLoss: 168.113\n",
      "35200/60000\tLoss: 165.528\n",
      "38400/60000\tLoss: 165.791\n",
      "41600/60000\tLoss: 166.502\n",
      "44800/60000\tLoss: 165.221\n",
      "48000/60000\tLoss: 165.887\n",
      "51200/60000\tLoss: 166.711\n",
      "54400/60000\tLoss: 166.805\n",
      "57600/60000\tLoss: 164.963\n",
      "Epoch: 36 Average loss: 166.58\n",
      "0/60000\tLoss: 161.164\n",
      "3200/60000\tLoss: 167.077\n",
      "6400/60000\tLoss: 165.984\n",
      "9600/60000\tLoss: 166.107\n",
      "12800/60000\tLoss: 165.498\n",
      "16000/60000\tLoss: 167.033\n",
      "19200/60000\tLoss: 166.022\n",
      "22400/60000\tLoss: 164.791\n",
      "25600/60000\tLoss: 164.848\n",
      "28800/60000\tLoss: 166.377\n",
      "32000/60000\tLoss: 167.199\n",
      "35200/60000\tLoss: 166.601\n",
      "38400/60000\tLoss: 166.563\n",
      "41600/60000\tLoss: 166.482\n",
      "44800/60000\tLoss: 165.577\n",
      "48000/60000\tLoss: 165.723\n",
      "51200/60000\tLoss: 166.963\n",
      "54400/60000\tLoss: 167.051\n",
      "57600/60000\tLoss: 166.294\n",
      "Epoch: 37 Average loss: 166.37\n",
      "0/60000\tLoss: 167.978\n",
      "3200/60000\tLoss: 168.018\n",
      "6400/60000\tLoss: 166.610\n",
      "9600/60000\tLoss: 167.340\n",
      "12800/60000\tLoss: 166.821\n",
      "16000/60000\tLoss: 166.428\n",
      "19200/60000\tLoss: 166.508\n",
      "22400/60000\tLoss: 166.459\n",
      "25600/60000\tLoss: 166.762\n",
      "28800/60000\tLoss: 166.451\n",
      "32000/60000\tLoss: 166.825\n",
      "35200/60000\tLoss: 167.478\n",
      "38400/60000\tLoss: 166.186\n",
      "41600/60000\tLoss: 167.002\n",
      "44800/60000\tLoss: 166.096\n",
      "48000/60000\tLoss: 165.820\n",
      "51200/60000\tLoss: 166.530\n",
      "54400/60000\tLoss: 164.563\n",
      "57600/60000\tLoss: 166.718\n",
      "Epoch: 38 Average loss: 166.62\n",
      "0/60000\tLoss: 165.015\n",
      "3200/60000\tLoss: 165.929\n",
      "6400/60000\tLoss: 166.638\n",
      "9600/60000\tLoss: 166.086\n",
      "12800/60000\tLoss: 167.514\n",
      "16000/60000\tLoss: 166.748\n",
      "19200/60000\tLoss: 166.566\n",
      "22400/60000\tLoss: 168.597\n",
      "25600/60000\tLoss: 166.362\n",
      "28800/60000\tLoss: 166.698\n",
      "32000/60000\tLoss: 166.829\n",
      "35200/60000\tLoss: 167.371\n",
      "38400/60000\tLoss: 166.957\n",
      "41600/60000\tLoss: 165.531\n",
      "44800/60000\tLoss: 165.772\n",
      "48000/60000\tLoss: 166.358\n",
      "51200/60000\tLoss: 166.291\n",
      "54400/60000\tLoss: 165.825\n",
      "57600/60000\tLoss: 165.419\n",
      "Epoch: 39 Average loss: 166.52\n",
      "0/60000\tLoss: 156.406\n",
      "3200/60000\tLoss: 165.915\n",
      "6400/60000\tLoss: 164.971\n",
      "9600/60000\tLoss: 169.431\n",
      "12800/60000\tLoss: 165.066\n",
      "16000/60000\tLoss: 165.150\n",
      "19200/60000\tLoss: 166.473\n",
      "22400/60000\tLoss: 166.312\n",
      "25600/60000\tLoss: 168.441\n",
      "28800/60000\tLoss: 165.296\n",
      "32000/60000\tLoss: 165.832\n",
      "35200/60000\tLoss: 167.773\n",
      "38400/60000\tLoss: 166.880\n",
      "41600/60000\tLoss: 165.968\n",
      "44800/60000\tLoss: 166.650\n",
      "48000/60000\tLoss: 168.303\n",
      "51200/60000\tLoss: 163.037\n",
      "54400/60000\tLoss: 166.489\n",
      "57600/60000\tLoss: 166.452\n",
      "Epoch: 40 Average loss: 166.43\n",
      "0/60000\tLoss: 173.128\n",
      "3200/60000\tLoss: 166.463\n",
      "6400/60000\tLoss: 163.192\n",
      "9600/60000\tLoss: 167.274\n",
      "12800/60000\tLoss: 167.525\n",
      "16000/60000\tLoss: 167.567\n",
      "19200/60000\tLoss: 167.188\n",
      "22400/60000\tLoss: 167.610\n",
      "25600/60000\tLoss: 164.659\n",
      "28800/60000\tLoss: 165.108\n",
      "32000/60000\tLoss: 168.327\n",
      "35200/60000\tLoss: 164.556\n",
      "38400/60000\tLoss: 166.741\n",
      "41600/60000\tLoss: 165.794\n",
      "44800/60000\tLoss: 165.286\n",
      "48000/60000\tLoss: 167.453\n",
      "51200/60000\tLoss: 165.348\n",
      "54400/60000\tLoss: 167.059\n",
      "57600/60000\tLoss: 167.024\n",
      "Epoch: 41 Average loss: 166.48\n",
      "0/60000\tLoss: 169.476\n",
      "3200/60000\tLoss: 164.622\n",
      "6400/60000\tLoss: 163.272\n",
      "9600/60000\tLoss: 166.755\n",
      "12800/60000\tLoss: 166.886\n",
      "16000/60000\tLoss: 166.213\n",
      "19200/60000\tLoss: 165.250\n",
      "22400/60000\tLoss: 166.345\n",
      "25600/60000\tLoss: 166.017\n",
      "28800/60000\tLoss: 166.192\n",
      "32000/60000\tLoss: 166.185\n",
      "35200/60000\tLoss: 165.424\n",
      "38400/60000\tLoss: 166.259\n",
      "41600/60000\tLoss: 166.360\n",
      "44800/60000\tLoss: 166.184\n",
      "48000/60000\tLoss: 164.980\n",
      "51200/60000\tLoss: 167.121\n",
      "54400/60000\tLoss: 165.288\n",
      "57600/60000\tLoss: 166.062\n",
      "Epoch: 42 Average loss: 165.91\n",
      "0/60000\tLoss: 167.771\n",
      "3200/60000\tLoss: 167.298\n",
      "6400/60000\tLoss: 166.077\n",
      "9600/60000\tLoss: 165.635\n",
      "12800/60000\tLoss: 167.319\n",
      "16000/60000\tLoss: 166.173\n",
      "19200/60000\tLoss: 167.397\n",
      "22400/60000\tLoss: 165.062\n",
      "25600/60000\tLoss: 165.964\n",
      "28800/60000\tLoss: 165.689\n",
      "32000/60000\tLoss: 162.896\n",
      "35200/60000\tLoss: 165.806\n",
      "38400/60000\tLoss: 165.971\n",
      "41600/60000\tLoss: 166.354\n",
      "44800/60000\tLoss: 166.852\n",
      "48000/60000\tLoss: 168.294\n",
      "51200/60000\tLoss: 165.873\n",
      "54400/60000\tLoss: 166.408\n",
      "57600/60000\tLoss: 166.579\n",
      "Epoch: 43 Average loss: 166.27\n",
      "0/60000\tLoss: 179.079\n",
      "3200/60000\tLoss: 165.752\n",
      "6400/60000\tLoss: 166.282\n",
      "9600/60000\tLoss: 166.708\n",
      "12800/60000\tLoss: 165.580\n",
      "16000/60000\tLoss: 165.909\n",
      "19200/60000\tLoss: 168.135\n",
      "22400/60000\tLoss: 166.131\n",
      "25600/60000\tLoss: 163.463\n",
      "28800/60000\tLoss: 166.005\n",
      "32000/60000\tLoss: 168.463\n",
      "35200/60000\tLoss: 165.710\n",
      "38400/60000\tLoss: 166.648\n",
      "41600/60000\tLoss: 166.217\n",
      "44800/60000\tLoss: 166.542\n",
      "48000/60000\tLoss: 165.947\n",
      "51200/60000\tLoss: 165.936\n",
      "54400/60000\tLoss: 165.394\n",
      "57600/60000\tLoss: 166.867\n",
      "Epoch: 44 Average loss: 166.38\n",
      "0/60000\tLoss: 178.856\n",
      "3200/60000\tLoss: 165.870\n",
      "6400/60000\tLoss: 165.811\n",
      "9600/60000\tLoss: 165.244\n",
      "12800/60000\tLoss: 167.004\n",
      "16000/60000\tLoss: 166.259\n",
      "19200/60000\tLoss: 165.915\n",
      "22400/60000\tLoss: 163.469\n",
      "25600/60000\tLoss: 166.709\n",
      "28800/60000\tLoss: 168.740\n",
      "32000/60000\tLoss: 165.355\n",
      "35200/60000\tLoss: 165.003\n",
      "38400/60000\tLoss: 168.259\n",
      "41600/60000\tLoss: 167.867\n",
      "44800/60000\tLoss: 165.653\n",
      "48000/60000\tLoss: 165.497\n",
      "51200/60000\tLoss: 168.292\n",
      "54400/60000\tLoss: 165.948\n",
      "57600/60000\tLoss: 165.143\n",
      "Epoch: 45 Average loss: 166.29\n",
      "0/60000\tLoss: 170.431\n",
      "3200/60000\tLoss: 165.854\n",
      "6400/60000\tLoss: 163.773\n",
      "9600/60000\tLoss: 163.748\n",
      "12800/60000\tLoss: 165.377\n",
      "16000/60000\tLoss: 166.684\n",
      "19200/60000\tLoss: 167.408\n",
      "22400/60000\tLoss: 166.979\n",
      "25600/60000\tLoss: 167.621\n",
      "28800/60000\tLoss: 165.334\n",
      "32000/60000\tLoss: 166.033\n",
      "35200/60000\tLoss: 163.413\n",
      "38400/60000\tLoss: 165.850\n",
      "41600/60000\tLoss: 164.561\n",
      "44800/60000\tLoss: 165.921\n",
      "48000/60000\tLoss: 166.890\n",
      "51200/60000\tLoss: 165.351\n",
      "54400/60000\tLoss: 164.697\n",
      "57600/60000\tLoss: 166.177\n",
      "Epoch: 46 Average loss: 165.80\n",
      "0/60000\tLoss: 162.910\n",
      "3200/60000\tLoss: 166.262\n",
      "6400/60000\tLoss: 166.420\n",
      "9600/60000\tLoss: 164.792\n",
      "12800/60000\tLoss: 165.606\n",
      "16000/60000\tLoss: 165.378\n",
      "19200/60000\tLoss: 165.526\n",
      "22400/60000\tLoss: 163.490\n",
      "25600/60000\tLoss: 165.159\n",
      "28800/60000\tLoss: 167.088\n",
      "32000/60000\tLoss: 165.486\n",
      "35200/60000\tLoss: 166.209\n",
      "38400/60000\tLoss: 164.813\n",
      "41600/60000\tLoss: 165.499\n",
      "44800/60000\tLoss: 165.687\n",
      "48000/60000\tLoss: 167.703\n",
      "51200/60000\tLoss: 165.126\n",
      "54400/60000\tLoss: 166.000\n",
      "57600/60000\tLoss: 165.071\n",
      "Epoch: 47 Average loss: 165.73\n",
      "0/60000\tLoss: 163.753\n",
      "3200/60000\tLoss: 164.525\n",
      "6400/60000\tLoss: 164.353\n",
      "9600/60000\tLoss: 166.554\n",
      "12800/60000\tLoss: 165.516\n",
      "16000/60000\tLoss: 164.562\n",
      "19200/60000\tLoss: 164.347\n",
      "22400/60000\tLoss: 166.752\n",
      "25600/60000\tLoss: 164.482\n",
      "28800/60000\tLoss: 165.408\n",
      "32000/60000\tLoss: 163.848\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35200/60000\tLoss: 163.990\n",
      "38400/60000\tLoss: 165.383\n",
      "41600/60000\tLoss: 165.813\n",
      "44800/60000\tLoss: 166.510\n",
      "48000/60000\tLoss: 165.226\n",
      "51200/60000\tLoss: 167.323\n",
      "54400/60000\tLoss: 167.316\n",
      "57600/60000\tLoss: 165.698\n",
      "Epoch: 48 Average loss: 165.57\n",
      "0/60000\tLoss: 154.464\n",
      "3200/60000\tLoss: 163.683\n",
      "6400/60000\tLoss: 164.471\n",
      "9600/60000\tLoss: 166.537\n",
      "12800/60000\tLoss: 166.334\n",
      "16000/60000\tLoss: 164.451\n",
      "19200/60000\tLoss: 164.758\n",
      "22400/60000\tLoss: 165.397\n",
      "25600/60000\tLoss: 164.619\n",
      "28800/60000\tLoss: 167.775\n",
      "32000/60000\tLoss: 165.882\n",
      "35200/60000\tLoss: 165.415\n",
      "38400/60000\tLoss: 167.007\n",
      "41600/60000\tLoss: 163.612\n",
      "44800/60000\tLoss: 165.089\n",
      "48000/60000\tLoss: 164.384\n",
      "51200/60000\tLoss: 163.840\n",
      "54400/60000\tLoss: 166.464\n",
      "57600/60000\tLoss: 167.055\n",
      "Epoch: 49 Average loss: 165.42\n",
      "0/60000\tLoss: 176.907\n",
      "3200/60000\tLoss: 164.069\n",
      "6400/60000\tLoss: 163.122\n",
      "9600/60000\tLoss: 164.199\n",
      "12800/60000\tLoss: 165.969\n",
      "16000/60000\tLoss: 166.492\n",
      "19200/60000\tLoss: 164.642\n",
      "22400/60000\tLoss: 165.205\n",
      "25600/60000\tLoss: 165.601\n",
      "28800/60000\tLoss: 164.473\n",
      "32000/60000\tLoss: 165.651\n",
      "35200/60000\tLoss: 165.595\n",
      "38400/60000\tLoss: 166.117\n",
      "41600/60000\tLoss: 166.204\n",
      "44800/60000\tLoss: 165.820\n",
      "48000/60000\tLoss: 168.206\n",
      "51200/60000\tLoss: 165.133\n",
      "54400/60000\tLoss: 165.451\n",
      "57600/60000\tLoss: 165.520\n",
      "Epoch: 50 Average loss: 165.42\n",
      "0/60000\tLoss: 157.605\n",
      "3200/60000\tLoss: 165.033\n",
      "6400/60000\tLoss: 164.379\n",
      "9600/60000\tLoss: 165.694\n",
      "12800/60000\tLoss: 165.891\n",
      "16000/60000\tLoss: 164.568\n",
      "19200/60000\tLoss: 166.277\n",
      "22400/60000\tLoss: 162.660\n",
      "25600/60000\tLoss: 164.818\n",
      "28800/60000\tLoss: 164.679\n",
      "32000/60000\tLoss: 166.831\n",
      "35200/60000\tLoss: 167.926\n",
      "38400/60000\tLoss: 166.321\n",
      "41600/60000\tLoss: 165.095\n",
      "44800/60000\tLoss: 164.943\n",
      "48000/60000\tLoss: 165.239\n",
      "51200/60000\tLoss: 163.930\n",
      "54400/60000\tLoss: 164.162\n",
      "57600/60000\tLoss: 164.939\n",
      "Epoch: 51 Average loss: 165.43\n",
      "0/60000\tLoss: 172.803\n",
      "3200/60000\tLoss: 165.823\n",
      "6400/60000\tLoss: 165.609\n",
      "9600/60000\tLoss: 163.813\n",
      "12800/60000\tLoss: 166.451\n",
      "16000/60000\tLoss: 165.452\n",
      "19200/60000\tLoss: 166.647\n",
      "22400/60000\tLoss: 165.098\n",
      "25600/60000\tLoss: 163.641\n",
      "28800/60000\tLoss: 165.744\n",
      "32000/60000\tLoss: 166.338\n",
      "35200/60000\tLoss: 166.766\n",
      "38400/60000\tLoss: 164.255\n",
      "41600/60000\tLoss: 167.316\n",
      "44800/60000\tLoss: 164.129\n",
      "48000/60000\tLoss: 165.098\n",
      "51200/60000\tLoss: 164.907\n",
      "54400/60000\tLoss: 165.362\n",
      "57600/60000\tLoss: 164.603\n",
      "Epoch: 52 Average loss: 165.39\n",
      "0/60000\tLoss: 151.780\n",
      "3200/60000\tLoss: 165.317\n",
      "6400/60000\tLoss: 165.301\n",
      "9600/60000\tLoss: 165.644\n",
      "12800/60000\tLoss: 164.722\n",
      "16000/60000\tLoss: 164.048\n",
      "19200/60000\tLoss: 164.274\n",
      "22400/60000\tLoss: 165.337\n",
      "25600/60000\tLoss: 164.478\n",
      "28800/60000\tLoss: 165.068\n",
      "32000/60000\tLoss: 166.858\n",
      "35200/60000\tLoss: 165.133\n",
      "38400/60000\tLoss: 162.405\n",
      "41600/60000\tLoss: 164.757\n",
      "44800/60000\tLoss: 164.968\n",
      "48000/60000\tLoss: 168.355\n",
      "51200/60000\tLoss: 164.704\n",
      "54400/60000\tLoss: 162.524\n",
      "57600/60000\tLoss: 165.000\n",
      "Epoch: 53 Average loss: 165.05\n",
      "0/60000\tLoss: 154.600\n",
      "3200/60000\tLoss: 165.597\n",
      "6400/60000\tLoss: 166.493\n",
      "9600/60000\tLoss: 163.620\n",
      "12800/60000\tLoss: 168.277\n",
      "16000/60000\tLoss: 166.758\n",
      "19200/60000\tLoss: 165.786\n",
      "22400/60000\tLoss: 165.924\n",
      "25600/60000\tLoss: 163.849\n",
      "28800/60000\tLoss: 166.063\n",
      "32000/60000\tLoss: 164.849\n",
      "35200/60000\tLoss: 165.148\n",
      "38400/60000\tLoss: 165.471\n",
      "41600/60000\tLoss: 165.902\n",
      "44800/60000\tLoss: 163.822\n",
      "48000/60000\tLoss: 165.129\n",
      "51200/60000\tLoss: 165.645\n",
      "54400/60000\tLoss: 164.761\n",
      "57600/60000\tLoss: 162.376\n",
      "Epoch: 54 Average loss: 165.30\n",
      "0/60000\tLoss: 164.159\n",
      "3200/60000\tLoss: 165.719\n",
      "6400/60000\tLoss: 165.501\n",
      "9600/60000\tLoss: 166.973\n",
      "12800/60000\tLoss: 164.341\n",
      "16000/60000\tLoss: 164.789\n",
      "19200/60000\tLoss: 165.134\n",
      "22400/60000\tLoss: 166.120\n",
      "25600/60000\tLoss: 168.485\n",
      "28800/60000\tLoss: 163.458\n",
      "32000/60000\tLoss: 164.753\n",
      "35200/60000\tLoss: 164.370\n",
      "38400/60000\tLoss: 163.363\n",
      "41600/60000\tLoss: 164.221\n",
      "44800/60000\tLoss: 164.242\n",
      "48000/60000\tLoss: 164.646\n",
      "51200/60000\tLoss: 162.479\n",
      "54400/60000\tLoss: 165.452\n",
      "57600/60000\tLoss: 166.730\n",
      "Epoch: 55 Average loss: 165.09\n",
      "0/60000\tLoss: 169.320\n",
      "3200/60000\tLoss: 165.361\n",
      "6400/60000\tLoss: 163.993\n",
      "9600/60000\tLoss: 163.956\n",
      "12800/60000\tLoss: 164.828\n",
      "16000/60000\tLoss: 165.292\n",
      "19200/60000\tLoss: 165.017\n",
      "22400/60000\tLoss: 164.328\n",
      "25600/60000\tLoss: 164.640\n",
      "28800/60000\tLoss: 162.956\n",
      "32000/60000\tLoss: 165.558\n",
      "35200/60000\tLoss: 166.961\n",
      "38400/60000\tLoss: 166.653\n",
      "41600/60000\tLoss: 164.695\n",
      "44800/60000\tLoss: 166.510\n",
      "48000/60000\tLoss: 165.493\n",
      "51200/60000\tLoss: 164.297\n",
      "54400/60000\tLoss: 163.610\n",
      "57600/60000\tLoss: 163.878\n",
      "Epoch: 56 Average loss: 165.02\n",
      "0/60000\tLoss: 163.879\n",
      "3200/60000\tLoss: 165.073\n",
      "6400/60000\tLoss: 163.465\n",
      "9600/60000\tLoss: 163.988\n",
      "12800/60000\tLoss: 165.510\n",
      "16000/60000\tLoss: 164.774\n",
      "19200/60000\tLoss: 164.881\n",
      "22400/60000\tLoss: 163.912\n",
      "25600/60000\tLoss: 163.277\n",
      "28800/60000\tLoss: 167.016\n",
      "32000/60000\tLoss: 165.422\n",
      "35200/60000\tLoss: 163.281\n",
      "38400/60000\tLoss: 165.443\n",
      "41600/60000\tLoss: 164.336\n",
      "44800/60000\tLoss: 164.145\n",
      "48000/60000\tLoss: 166.238\n",
      "51200/60000\tLoss: 163.967\n",
      "54400/60000\tLoss: 165.409\n",
      "57600/60000\tLoss: 165.696\n",
      "Epoch: 57 Average loss: 164.80\n",
      "0/60000\tLoss: 159.753\n",
      "3200/60000\tLoss: 165.785\n",
      "6400/60000\tLoss: 166.048\n",
      "9600/60000\tLoss: 163.906\n",
      "12800/60000\tLoss: 165.131\n",
      "16000/60000\tLoss: 162.907\n",
      "19200/60000\tLoss: 165.905\n",
      "22400/60000\tLoss: 164.789\n",
      "25600/60000\tLoss: 165.182\n",
      "28800/60000\tLoss: 164.974\n",
      "32000/60000\tLoss: 166.022\n",
      "35200/60000\tLoss: 165.037\n",
      "38400/60000\tLoss: 165.449\n",
      "41600/60000\tLoss: 163.380\n",
      "44800/60000\tLoss: 164.974\n",
      "48000/60000\tLoss: 163.854\n",
      "51200/60000\tLoss: 164.462\n",
      "54400/60000\tLoss: 164.184\n",
      "57600/60000\tLoss: 165.884\n",
      "Epoch: 58 Average loss: 164.97\n",
      "0/60000\tLoss: 155.407\n",
      "3200/60000\tLoss: 163.341\n",
      "6400/60000\tLoss: 166.405\n",
      "9600/60000\tLoss: 167.564\n",
      "12800/60000\tLoss: 164.555\n",
      "16000/60000\tLoss: 165.176\n",
      "19200/60000\tLoss: 163.545\n",
      "22400/60000\tLoss: 164.825\n",
      "25600/60000\tLoss: 164.306\n",
      "28800/60000\tLoss: 163.714\n",
      "32000/60000\tLoss: 166.169\n",
      "35200/60000\tLoss: 165.292\n",
      "38400/60000\tLoss: 163.996\n",
      "41600/60000\tLoss: 165.702\n",
      "44800/60000\tLoss: 164.699\n",
      "48000/60000\tLoss: 165.571\n",
      "51200/60000\tLoss: 162.937\n",
      "54400/60000\tLoss: 164.830\n",
      "57600/60000\tLoss: 163.975\n",
      "Epoch: 59 Average loss: 165.00\n",
      "0/60000\tLoss: 164.225\n",
      "3200/60000\tLoss: 162.947\n",
      "6400/60000\tLoss: 165.272\n",
      "9600/60000\tLoss: 163.487\n",
      "12800/60000\tLoss: 163.295\n",
      "16000/60000\tLoss: 166.125\n",
      "19200/60000\tLoss: 163.857\n",
      "22400/60000\tLoss: 166.076\n",
      "25600/60000\tLoss: 164.985\n",
      "28800/60000\tLoss: 164.022\n",
      "32000/60000\tLoss: 165.082\n",
      "35200/60000\tLoss: 164.312\n",
      "38400/60000\tLoss: 165.363\n",
      "41600/60000\tLoss: 163.789\n",
      "44800/60000\tLoss: 166.170\n",
      "48000/60000\tLoss: 164.177\n",
      "51200/60000\tLoss: 166.978\n",
      "54400/60000\tLoss: 165.857\n",
      "57600/60000\tLoss: 163.657\n",
      "Epoch: 60 Average loss: 164.78\n",
      "0/60000\tLoss: 160.894\n",
      "3200/60000\tLoss: 164.526\n",
      "6400/60000\tLoss: 165.059\n",
      "9600/60000\tLoss: 163.897\n",
      "12800/60000\tLoss: 164.383\n",
      "16000/60000\tLoss: 163.245\n",
      "19200/60000\tLoss: 163.956\n",
      "22400/60000\tLoss: 165.053\n",
      "25600/60000\tLoss: 165.499\n",
      "28800/60000\tLoss: 163.792\n",
      "32000/60000\tLoss: 165.226\n",
      "35200/60000\tLoss: 165.558\n",
      "38400/60000\tLoss: 165.905\n",
      "41600/60000\tLoss: 165.344\n",
      "44800/60000\tLoss: 164.761\n",
      "48000/60000\tLoss: 165.888\n",
      "51200/60000\tLoss: 164.599\n",
      "54400/60000\tLoss: 163.773\n",
      "57600/60000\tLoss: 164.810\n",
      "Epoch: 61 Average loss: 164.77\n",
      "0/60000\tLoss: 165.456\n",
      "3200/60000\tLoss: 164.668\n",
      "6400/60000\tLoss: 164.760\n",
      "9600/60000\tLoss: 164.575\n",
      "12800/60000\tLoss: 165.663\n",
      "16000/60000\tLoss: 164.958\n",
      "19200/60000\tLoss: 163.277\n",
      "22400/60000\tLoss: 163.878\n",
      "25600/60000\tLoss: 165.256\n",
      "28800/60000\tLoss: 163.186\n",
      "32000/60000\tLoss: 164.635\n",
      "35200/60000\tLoss: 165.764\n",
      "38400/60000\tLoss: 164.564\n",
      "41600/60000\tLoss: 167.086\n",
      "44800/60000\tLoss: 164.743\n",
      "48000/60000\tLoss: 165.437\n",
      "51200/60000\tLoss: 162.924\n",
      "54400/60000\tLoss: 163.689\n",
      "57600/60000\tLoss: 165.203\n",
      "Epoch: 62 Average loss: 164.72\n",
      "0/60000\tLoss: 156.302\n",
      "3200/60000\tLoss: 164.904\n",
      "6400/60000\tLoss: 164.150\n",
      "9600/60000\tLoss: 164.954\n",
      "12800/60000\tLoss: 163.870\n",
      "16000/60000\tLoss: 166.146\n",
      "19200/60000\tLoss: 163.936\n",
      "22400/60000\tLoss: 165.117\n",
      "25600/60000\tLoss: 165.138\n",
      "28800/60000\tLoss: 163.681\n",
      "32000/60000\tLoss: 162.298\n",
      "35200/60000\tLoss: 163.928\n",
      "38400/60000\tLoss: 164.676\n",
      "41600/60000\tLoss: 164.493\n",
      "44800/60000\tLoss: 163.402\n",
      "48000/60000\tLoss: 165.169\n",
      "51200/60000\tLoss: 163.743\n",
      "54400/60000\tLoss: 164.859\n",
      "57600/60000\tLoss: 164.244\n",
      "Epoch: 63 Average loss: 164.52\n",
      "0/60000\tLoss: 158.506\n",
      "3200/60000\tLoss: 163.588\n",
      "6400/60000\tLoss: 163.939\n",
      "9600/60000\tLoss: 163.935\n",
      "12800/60000\tLoss: 163.814\n",
      "16000/60000\tLoss: 165.927\n",
      "19200/60000\tLoss: 163.981\n",
      "22400/60000\tLoss: 165.048\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25600/60000\tLoss: 164.775\n",
      "28800/60000\tLoss: 164.067\n",
      "32000/60000\tLoss: 164.387\n",
      "35200/60000\tLoss: 163.359\n",
      "38400/60000\tLoss: 163.981\n",
      "41600/60000\tLoss: 164.030\n",
      "44800/60000\tLoss: 166.366\n",
      "48000/60000\tLoss: 166.181\n",
      "51200/60000\tLoss: 162.920\n",
      "54400/60000\tLoss: 163.979\n",
      "57600/60000\tLoss: 164.959\n",
      "Epoch: 64 Average loss: 164.61\n",
      "0/60000\tLoss: 170.651\n",
      "3200/60000\tLoss: 163.851\n",
      "6400/60000\tLoss: 164.430\n",
      "9600/60000\tLoss: 163.711\n",
      "12800/60000\tLoss: 162.623\n",
      "16000/60000\tLoss: 164.928\n",
      "19200/60000\tLoss: 163.730\n",
      "22400/60000\tLoss: 163.773\n",
      "25600/60000\tLoss: 164.964\n",
      "28800/60000\tLoss: 164.389\n",
      "32000/60000\tLoss: 165.643\n",
      "35200/60000\tLoss: 164.987\n",
      "38400/60000\tLoss: 166.237\n",
      "41600/60000\tLoss: 165.338\n",
      "44800/60000\tLoss: 164.217\n",
      "48000/60000\tLoss: 164.517\n",
      "51200/60000\tLoss: 164.803\n",
      "54400/60000\tLoss: 164.394\n",
      "57600/60000\tLoss: 164.017\n",
      "Epoch: 65 Average loss: 164.66\n",
      "0/60000\tLoss: 173.599\n",
      "3200/60000\tLoss: 166.792\n",
      "6400/60000\tLoss: 163.652\n",
      "9600/60000\tLoss: 164.897\n",
      "12800/60000\tLoss: 168.702\n",
      "16000/60000\tLoss: 164.877\n",
      "19200/60000\tLoss: 163.941\n",
      "22400/60000\tLoss: 167.512\n",
      "25600/60000\tLoss: 165.010\n",
      "28800/60000\tLoss: 164.167\n",
      "32000/60000\tLoss: 162.633\n",
      "35200/60000\tLoss: 162.762\n",
      "38400/60000\tLoss: 163.869\n",
      "41600/60000\tLoss: 162.116\n",
      "44800/60000\tLoss: 163.408\n",
      "48000/60000\tLoss: 163.692\n",
      "51200/60000\tLoss: 163.162\n",
      "54400/60000\tLoss: 163.816\n",
      "57600/60000\tLoss: 164.950\n",
      "Epoch: 66 Average loss: 164.47\n",
      "0/60000\tLoss: 153.653\n",
      "3200/60000\tLoss: 163.916\n",
      "6400/60000\tLoss: 163.588\n",
      "9600/60000\tLoss: 163.067\n",
      "12800/60000\tLoss: 162.994\n",
      "16000/60000\tLoss: 164.209\n",
      "19200/60000\tLoss: 162.648\n",
      "22400/60000\tLoss: 166.160\n",
      "25600/60000\tLoss: 163.790\n",
      "28800/60000\tLoss: 165.357\n",
      "32000/60000\tLoss: 163.849\n",
      "35200/60000\tLoss: 165.021\n",
      "38400/60000\tLoss: 165.692\n",
      "41600/60000\tLoss: 164.716\n",
      "44800/60000\tLoss: 165.846\n",
      "48000/60000\tLoss: 163.952\n",
      "51200/60000\tLoss: 163.861\n",
      "54400/60000\tLoss: 165.611\n",
      "57600/60000\tLoss: 163.993\n",
      "Epoch: 67 Average loss: 164.49\n",
      "0/60000\tLoss: 173.407\n",
      "3200/60000\tLoss: 165.334\n",
      "6400/60000\tLoss: 163.367\n",
      "9600/60000\tLoss: 166.236\n",
      "12800/60000\tLoss: 164.486\n",
      "16000/60000\tLoss: 166.127\n",
      "19200/60000\tLoss: 166.474\n",
      "22400/60000\tLoss: 164.456\n",
      "25600/60000\tLoss: 163.390\n",
      "28800/60000\tLoss: 162.248\n",
      "32000/60000\tLoss: 165.125\n",
      "35200/60000\tLoss: 163.401\n",
      "38400/60000\tLoss: 166.169\n",
      "41600/60000\tLoss: 163.811\n",
      "44800/60000\tLoss: 162.332\n",
      "48000/60000\tLoss: 162.783\n",
      "51200/60000\tLoss: 164.333\n",
      "54400/60000\tLoss: 164.116\n",
      "57600/60000\tLoss: 165.088\n",
      "Epoch: 68 Average loss: 164.50\n",
      "0/60000\tLoss: 160.029\n",
      "3200/60000\tLoss: 165.140\n",
      "6400/60000\tLoss: 162.360\n",
      "9600/60000\tLoss: 162.956\n",
      "12800/60000\tLoss: 163.997\n",
      "16000/60000\tLoss: 164.680\n",
      "19200/60000\tLoss: 164.016\n",
      "22400/60000\tLoss: 164.423\n",
      "25600/60000\tLoss: 165.531\n",
      "28800/60000\tLoss: 164.777\n",
      "32000/60000\tLoss: 165.376\n",
      "35200/60000\tLoss: 162.875\n",
      "38400/60000\tLoss: 167.026\n",
      "41600/60000\tLoss: 162.891\n",
      "44800/60000\tLoss: 165.738\n",
      "48000/60000\tLoss: 162.821\n",
      "51200/60000\tLoss: 165.053\n",
      "54400/60000\tLoss: 163.258\n",
      "57600/60000\tLoss: 163.560\n",
      "Epoch: 69 Average loss: 164.31\n",
      "0/60000\tLoss: 151.439\n",
      "3200/60000\tLoss: 165.013\n",
      "6400/60000\tLoss: 165.496\n",
      "9600/60000\tLoss: 163.814\n",
      "12800/60000\tLoss: 164.609\n",
      "16000/60000\tLoss: 163.528\n",
      "19200/60000\tLoss: 163.672\n",
      "22400/60000\tLoss: 164.613\n",
      "25600/60000\tLoss: 165.585\n",
      "28800/60000\tLoss: 164.524\n",
      "32000/60000\tLoss: 164.639\n",
      "35200/60000\tLoss: 164.551\n",
      "38400/60000\tLoss: 163.409\n",
      "41600/60000\tLoss: 163.236\n",
      "44800/60000\tLoss: 162.253\n",
      "48000/60000\tLoss: 162.414\n",
      "51200/60000\tLoss: 163.377\n",
      "54400/60000\tLoss: 162.839\n",
      "57600/60000\tLoss: 163.597\n",
      "Epoch: 70 Average loss: 164.08\n",
      "0/60000\tLoss: 174.558\n",
      "3200/60000\tLoss: 163.440\n",
      "6400/60000\tLoss: 163.906\n",
      "9600/60000\tLoss: 165.068\n",
      "12800/60000\tLoss: 164.886\n",
      "16000/60000\tLoss: 162.768\n",
      "19200/60000\tLoss: 162.425\n",
      "22400/60000\tLoss: 163.715\n",
      "25600/60000\tLoss: 164.719\n",
      "28800/60000\tLoss: 163.650\n",
      "32000/60000\tLoss: 162.111\n",
      "35200/60000\tLoss: 163.187\n",
      "38400/60000\tLoss: 164.447\n",
      "41600/60000\tLoss: 163.805\n",
      "44800/60000\tLoss: 167.747\n",
      "48000/60000\tLoss: 163.958\n",
      "51200/60000\tLoss: 164.875\n",
      "54400/60000\tLoss: 164.970\n",
      "57600/60000\tLoss: 163.848\n",
      "Epoch: 71 Average loss: 164.07\n",
      "0/60000\tLoss: 168.532\n",
      "3200/60000\tLoss: 167.316\n",
      "6400/60000\tLoss: 164.551\n",
      "9600/60000\tLoss: 164.573\n",
      "12800/60000\tLoss: 164.102\n",
      "16000/60000\tLoss: 165.241\n",
      "19200/60000\tLoss: 164.222\n",
      "22400/60000\tLoss: 164.528\n",
      "25600/60000\tLoss: 164.929\n",
      "28800/60000\tLoss: 165.061\n",
      "32000/60000\tLoss: 163.381\n",
      "35200/60000\tLoss: 164.872\n",
      "38400/60000\tLoss: 162.302\n",
      "41600/60000\tLoss: 165.872\n",
      "44800/60000\tLoss: 161.275\n",
      "48000/60000\tLoss: 163.658\n",
      "51200/60000\tLoss: 164.412\n",
      "54400/60000\tLoss: 162.032\n",
      "57600/60000\tLoss: 162.943\n",
      "Epoch: 72 Average loss: 164.31\n",
      "0/60000\tLoss: 160.802\n",
      "3200/60000\tLoss: 163.410\n",
      "6400/60000\tLoss: 166.893\n",
      "9600/60000\tLoss: 164.034\n",
      "12800/60000\tLoss: 161.940\n",
      "16000/60000\tLoss: 163.689\n",
      "19200/60000\tLoss: 163.265\n",
      "22400/60000\tLoss: 164.291\n",
      "25600/60000\tLoss: 163.469\n",
      "28800/60000\tLoss: 162.466\n",
      "32000/60000\tLoss: 163.104\n",
      "35200/60000\tLoss: 163.215\n",
      "38400/60000\tLoss: 163.394\n",
      "41600/60000\tLoss: 163.469\n",
      "44800/60000\tLoss: 164.180\n",
      "48000/60000\tLoss: 165.759\n",
      "51200/60000\tLoss: 164.826\n",
      "54400/60000\tLoss: 165.371\n",
      "57600/60000\tLoss: 165.541\n",
      "Epoch: 73 Average loss: 164.09\n",
      "0/60000\tLoss: 160.618\n",
      "3200/60000\tLoss: 162.426\n",
      "6400/60000\tLoss: 164.753\n",
      "9600/60000\tLoss: 163.981\n",
      "12800/60000\tLoss: 162.593\n",
      "16000/60000\tLoss: 163.376\n",
      "19200/60000\tLoss: 164.180\n",
      "22400/60000\tLoss: 162.269\n",
      "25600/60000\tLoss: 165.486\n",
      "28800/60000\tLoss: 162.947\n",
      "32000/60000\tLoss: 165.585\n",
      "35200/60000\tLoss: 161.924\n",
      "38400/60000\tLoss: 161.299\n",
      "41600/60000\tLoss: 164.152\n",
      "44800/60000\tLoss: 163.629\n",
      "48000/60000\tLoss: 164.809\n",
      "51200/60000\tLoss: 163.854\n",
      "54400/60000\tLoss: 165.670\n",
      "57600/60000\tLoss: 162.834\n",
      "Epoch: 74 Average loss: 163.78\n",
      "0/60000\tLoss: 162.083\n",
      "3200/60000\tLoss: 164.563\n",
      "6400/60000\tLoss: 163.808\n",
      "9600/60000\tLoss: 164.149\n",
      "12800/60000\tLoss: 164.505\n",
      "16000/60000\tLoss: 162.860\n",
      "19200/60000\tLoss: 164.355\n",
      "22400/60000\tLoss: 164.467\n",
      "25600/60000\tLoss: 165.825\n",
      "28800/60000\tLoss: 162.504\n",
      "32000/60000\tLoss: 162.300\n",
      "35200/60000\tLoss: 161.633\n",
      "38400/60000\tLoss: 164.747\n",
      "41600/60000\tLoss: 165.700\n",
      "44800/60000\tLoss: 161.685\n",
      "48000/60000\tLoss: 163.856\n",
      "51200/60000\tLoss: 162.269\n",
      "54400/60000\tLoss: 163.393\n",
      "57600/60000\tLoss: 164.365\n",
      "Epoch: 75 Average loss: 163.83\n",
      "0/60000\tLoss: 158.791\n",
      "3200/60000\tLoss: 163.675\n",
      "6400/60000\tLoss: 165.065\n",
      "9600/60000\tLoss: 163.533\n",
      "12800/60000\tLoss: 164.608\n",
      "16000/60000\tLoss: 164.893\n",
      "19200/60000\tLoss: 164.222\n",
      "22400/60000\tLoss: 164.258\n",
      "25600/60000\tLoss: 162.889\n",
      "28800/60000\tLoss: 162.546\n",
      "32000/60000\tLoss: 164.531\n",
      "35200/60000\tLoss: 165.325\n",
      "38400/60000\tLoss: 164.090\n",
      "41600/60000\tLoss: 163.247\n",
      "44800/60000\tLoss: 163.195\n",
      "48000/60000\tLoss: 164.519\n",
      "51200/60000\tLoss: 164.911\n",
      "54400/60000\tLoss: 162.677\n",
      "57600/60000\tLoss: 164.431\n",
      "Epoch: 76 Average loss: 164.12\n",
      "0/60000\tLoss: 166.456\n",
      "3200/60000\tLoss: 161.390\n",
      "6400/60000\tLoss: 162.925\n",
      "9600/60000\tLoss: 164.480\n",
      "12800/60000\tLoss: 163.842\n",
      "16000/60000\tLoss: 162.572\n",
      "19200/60000\tLoss: 162.903\n",
      "22400/60000\tLoss: 165.815\n",
      "25600/60000\tLoss: 162.044\n",
      "28800/60000\tLoss: 164.415\n",
      "32000/60000\tLoss: 163.350\n",
      "35200/60000\tLoss: 164.285\n",
      "38400/60000\tLoss: 163.636\n",
      "41600/60000\tLoss: 166.067\n",
      "44800/60000\tLoss: 163.490\n",
      "48000/60000\tLoss: 163.638\n",
      "51200/60000\tLoss: 164.244\n",
      "54400/60000\tLoss: 163.488\n",
      "57600/60000\tLoss: 163.791\n",
      "Epoch: 77 Average loss: 163.73\n",
      "0/60000\tLoss: 161.417\n",
      "3200/60000\tLoss: 162.781\n",
      "6400/60000\tLoss: 163.350\n",
      "9600/60000\tLoss: 162.510\n",
      "12800/60000\tLoss: 164.725\n",
      "16000/60000\tLoss: 164.667\n",
      "19200/60000\tLoss: 163.074\n",
      "22400/60000\tLoss: 164.168\n",
      "25600/60000\tLoss: 162.059\n",
      "28800/60000\tLoss: 162.442\n",
      "32000/60000\tLoss: 161.829\n",
      "35200/60000\tLoss: 163.812\n",
      "38400/60000\tLoss: 164.090\n",
      "41600/60000\tLoss: 163.771\n",
      "44800/60000\tLoss: 165.629\n",
      "48000/60000\tLoss: 162.380\n",
      "51200/60000\tLoss: 164.758\n",
      "54400/60000\tLoss: 164.175\n",
      "57600/60000\tLoss: 164.079\n",
      "Epoch: 78 Average loss: 163.66\n",
      "0/60000\tLoss: 181.539\n",
      "3200/60000\tLoss: 162.856\n",
      "6400/60000\tLoss: 162.448\n",
      "9600/60000\tLoss: 164.655\n",
      "12800/60000\tLoss: 163.695\n",
      "16000/60000\tLoss: 164.256\n",
      "19200/60000\tLoss: 163.423\n",
      "22400/60000\tLoss: 164.456\n",
      "25600/60000\tLoss: 164.905\n",
      "28800/60000\tLoss: 164.025\n",
      "32000/60000\tLoss: 164.859\n",
      "35200/60000\tLoss: 162.707\n",
      "38400/60000\tLoss: 163.632\n",
      "41600/60000\tLoss: 163.764\n",
      "44800/60000\tLoss: 164.615\n",
      "48000/60000\tLoss: 162.741\n",
      "51200/60000\tLoss: 165.797\n",
      "54400/60000\tLoss: 163.536\n",
      "57600/60000\tLoss: 162.802\n",
      "Epoch: 79 Average loss: 164.00\n",
      "0/60000\tLoss: 167.539\n",
      "3200/60000\tLoss: 165.082\n",
      "6400/60000\tLoss: 162.407\n",
      "9600/60000\tLoss: 163.003\n",
      "12800/60000\tLoss: 163.012\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16000/60000\tLoss: 163.120\n",
      "19200/60000\tLoss: 163.668\n",
      "22400/60000\tLoss: 164.455\n",
      "25600/60000\tLoss: 162.869\n",
      "28800/60000\tLoss: 164.016\n",
      "32000/60000\tLoss: 162.822\n",
      "35200/60000\tLoss: 161.987\n",
      "38400/60000\tLoss: 166.158\n",
      "41600/60000\tLoss: 163.043\n",
      "44800/60000\tLoss: 165.263\n",
      "48000/60000\tLoss: 162.941\n",
      "51200/60000\tLoss: 163.719\n",
      "54400/60000\tLoss: 163.097\n",
      "57600/60000\tLoss: 165.377\n",
      "Epoch: 80 Average loss: 163.72\n",
      "0/60000\tLoss: 154.377\n",
      "3200/60000\tLoss: 164.231\n",
      "6400/60000\tLoss: 163.099\n",
      "9600/60000\tLoss: 163.337\n",
      "12800/60000\tLoss: 162.931\n",
      "16000/60000\tLoss: 164.747\n",
      "19200/60000\tLoss: 163.489\n",
      "22400/60000\tLoss: 165.827\n",
      "25600/60000\tLoss: 165.080\n",
      "28800/60000\tLoss: 162.729\n",
      "32000/60000\tLoss: 161.671\n",
      "35200/60000\tLoss: 163.440\n",
      "38400/60000\tLoss: 163.402\n",
      "41600/60000\tLoss: 161.673\n",
      "44800/60000\tLoss: 163.714\n",
      "48000/60000\tLoss: 164.240\n",
      "51200/60000\tLoss: 164.642\n",
      "54400/60000\tLoss: 163.940\n",
      "57600/60000\tLoss: 163.126\n",
      "Epoch: 81 Average loss: 163.68\n",
      "0/60000\tLoss: 154.611\n",
      "3200/60000\tLoss: 163.319\n",
      "6400/60000\tLoss: 162.980\n",
      "9600/60000\tLoss: 163.365\n",
      "12800/60000\tLoss: 164.023\n",
      "16000/60000\tLoss: 163.749\n",
      "19200/60000\tLoss: 164.144\n",
      "22400/60000\tLoss: 161.289\n",
      "25600/60000\tLoss: 164.683\n",
      "28800/60000\tLoss: 164.742\n",
      "32000/60000\tLoss: 161.291\n",
      "35200/60000\tLoss: 163.720\n",
      "38400/60000\tLoss: 162.503\n",
      "41600/60000\tLoss: 164.280\n",
      "44800/60000\tLoss: 164.185\n",
      "48000/60000\tLoss: 164.985\n",
      "51200/60000\tLoss: 161.931\n",
      "54400/60000\tLoss: 162.622\n",
      "57600/60000\tLoss: 163.348\n",
      "Epoch: 82 Average loss: 163.51\n",
      "0/60000\tLoss: 170.012\n",
      "3200/60000\tLoss: 164.369\n",
      "6400/60000\tLoss: 166.235\n",
      "9600/60000\tLoss: 166.086\n",
      "12800/60000\tLoss: 165.496\n",
      "16000/60000\tLoss: 162.821\n",
      "19200/60000\tLoss: 161.321\n",
      "22400/60000\tLoss: 164.069\n",
      "25600/60000\tLoss: 164.030\n",
      "28800/60000\tLoss: 163.635\n",
      "32000/60000\tLoss: 163.056\n",
      "35200/60000\tLoss: 161.763\n",
      "38400/60000\tLoss: 162.940\n",
      "41600/60000\tLoss: 163.902\n",
      "44800/60000\tLoss: 163.820\n",
      "48000/60000\tLoss: 164.809\n",
      "51200/60000\tLoss: 162.799\n",
      "54400/60000\tLoss: 164.355\n",
      "57600/60000\tLoss: 162.629\n",
      "Epoch: 83 Average loss: 163.85\n",
      "0/60000\tLoss: 159.041\n",
      "3200/60000\tLoss: 163.201\n",
      "6400/60000\tLoss: 164.109\n",
      "9600/60000\tLoss: 164.764\n",
      "12800/60000\tLoss: 166.773\n",
      "16000/60000\tLoss: 163.038\n",
      "19200/60000\tLoss: 162.376\n",
      "22400/60000\tLoss: 164.099\n",
      "25600/60000\tLoss: 163.136\n",
      "28800/60000\tLoss: 164.022\n",
      "32000/60000\tLoss: 165.283\n",
      "35200/60000\tLoss: 162.238\n",
      "38400/60000\tLoss: 163.892\n",
      "41600/60000\tLoss: 163.676\n",
      "44800/60000\tLoss: 164.138\n",
      "48000/60000\tLoss: 164.110\n",
      "51200/60000\tLoss: 164.143\n",
      "54400/60000\tLoss: 164.472\n",
      "57600/60000\tLoss: 164.666\n",
      "Epoch: 84 Average loss: 164.02\n",
      "0/60000\tLoss: 158.141\n",
      "3200/60000\tLoss: 161.615\n",
      "6400/60000\tLoss: 162.514\n",
      "9600/60000\tLoss: 168.416\n",
      "12800/60000\tLoss: 163.219\n",
      "16000/60000\tLoss: 166.241\n",
      "19200/60000\tLoss: 163.830\n",
      "22400/60000\tLoss: 162.851\n",
      "25600/60000\tLoss: 163.859\n",
      "28800/60000\tLoss: 164.921\n",
      "32000/60000\tLoss: 162.801\n",
      "35200/60000\tLoss: 163.564\n",
      "38400/60000\tLoss: 163.516\n",
      "41600/60000\tLoss: 163.871\n",
      "44800/60000\tLoss: 163.711\n",
      "48000/60000\tLoss: 164.751\n",
      "51200/60000\tLoss: 164.543\n",
      "54400/60000\tLoss: 165.557\n",
      "57600/60000\tLoss: 162.080\n",
      "Epoch: 85 Average loss: 164.04\n",
      "0/60000\tLoss: 162.397\n",
      "3200/60000\tLoss: 163.814\n",
      "6400/60000\tLoss: 162.855\n",
      "9600/60000\tLoss: 162.798\n",
      "12800/60000\tLoss: 164.840\n",
      "16000/60000\tLoss: 162.564\n",
      "19200/60000\tLoss: 162.537\n",
      "22400/60000\tLoss: 165.087\n",
      "25600/60000\tLoss: 162.095\n",
      "28800/60000\tLoss: 163.495\n",
      "32000/60000\tLoss: 164.394\n",
      "35200/60000\tLoss: 164.563\n",
      "38400/60000\tLoss: 163.084\n",
      "41600/60000\tLoss: 162.460\n",
      "44800/60000\tLoss: 162.571\n",
      "48000/60000\tLoss: 163.706\n",
      "51200/60000\tLoss: 162.374\n",
      "54400/60000\tLoss: 163.108\n",
      "57600/60000\tLoss: 164.231\n",
      "Epoch: 86 Average loss: 163.46\n",
      "0/60000\tLoss: 155.559\n",
      "3200/60000\tLoss: 164.569\n",
      "6400/60000\tLoss: 163.520\n",
      "9600/60000\tLoss: 162.924\n",
      "12800/60000\tLoss: 161.940\n",
      "16000/60000\tLoss: 162.599\n",
      "19200/60000\tLoss: 162.094\n",
      "22400/60000\tLoss: 163.700\n",
      "25600/60000\tLoss: 161.659\n",
      "28800/60000\tLoss: 162.098\n",
      "32000/60000\tLoss: 164.322\n",
      "35200/60000\tLoss: 163.765\n",
      "38400/60000\tLoss: 162.046\n",
      "41600/60000\tLoss: 163.229\n",
      "44800/60000\tLoss: 162.527\n",
      "48000/60000\tLoss: 164.252\n",
      "51200/60000\tLoss: 166.292\n",
      "54400/60000\tLoss: 163.661\n",
      "57600/60000\tLoss: 164.761\n",
      "Epoch: 87 Average loss: 163.44\n",
      "0/60000\tLoss: 168.533\n",
      "3200/60000\tLoss: 162.952\n",
      "6400/60000\tLoss: 163.913\n",
      "9600/60000\tLoss: 160.941\n",
      "12800/60000\tLoss: 162.064\n",
      "16000/60000\tLoss: 165.003\n",
      "19200/60000\tLoss: 164.142\n",
      "22400/60000\tLoss: 163.894\n",
      "25600/60000\tLoss: 165.016\n",
      "28800/60000\tLoss: 162.268\n",
      "32000/60000\tLoss: 165.011\n",
      "35200/60000\tLoss: 163.280\n",
      "38400/60000\tLoss: 162.085\n",
      "41600/60000\tLoss: 163.917\n",
      "44800/60000\tLoss: 162.634\n",
      "48000/60000\tLoss: 165.027\n",
      "51200/60000\tLoss: 163.335\n",
      "54400/60000\tLoss: 163.658\n",
      "57600/60000\tLoss: 162.169\n",
      "Epoch: 88 Average loss: 163.50\n",
      "0/60000\tLoss: 157.808\n",
      "3200/60000\tLoss: 163.747\n",
      "6400/60000\tLoss: 163.979\n",
      "9600/60000\tLoss: 161.732\n",
      "12800/60000\tLoss: 163.007\n",
      "16000/60000\tLoss: 164.934\n",
      "19200/60000\tLoss: 163.105\n",
      "22400/60000\tLoss: 163.284\n",
      "25600/60000\tLoss: 164.326\n",
      "28800/60000\tLoss: 164.228\n",
      "32000/60000\tLoss: 165.143\n",
      "35200/60000\tLoss: 162.142\n",
      "38400/60000\tLoss: 162.455\n",
      "41600/60000\tLoss: 163.165\n",
      "44800/60000\tLoss: 163.122\n",
      "48000/60000\tLoss: 161.791\n",
      "51200/60000\tLoss: 162.927\n",
      "54400/60000\tLoss: 163.242\n",
      "57600/60000\tLoss: 163.309\n",
      "Epoch: 89 Average loss: 163.34\n",
      "0/60000\tLoss: 157.493\n",
      "3200/60000\tLoss: 163.419\n",
      "6400/60000\tLoss: 163.665\n",
      "9600/60000\tLoss: 163.902\n",
      "12800/60000\tLoss: 163.791\n",
      "16000/60000\tLoss: 162.955\n",
      "19200/60000\tLoss: 163.838\n",
      "22400/60000\tLoss: 163.770\n",
      "25600/60000\tLoss: 163.213\n",
      "28800/60000\tLoss: 162.711\n",
      "32000/60000\tLoss: 163.947\n",
      "35200/60000\tLoss: 161.595\n",
      "38400/60000\tLoss: 163.882\n",
      "41600/60000\tLoss: 161.630\n",
      "44800/60000\tLoss: 163.213\n",
      "48000/60000\tLoss: 162.667\n",
      "51200/60000\tLoss: 163.379\n",
      "54400/60000\tLoss: 162.907\n",
      "57600/60000\tLoss: 165.948\n",
      "Epoch: 90 Average loss: 163.45\n",
      "0/60000\tLoss: 159.320\n",
      "3200/60000\tLoss: 162.189\n",
      "6400/60000\tLoss: 164.687\n",
      "9600/60000\tLoss: 161.999\n",
      "12800/60000\tLoss: 163.793\n",
      "16000/60000\tLoss: 162.525\n",
      "19200/60000\tLoss: 164.013\n",
      "22400/60000\tLoss: 163.942\n",
      "25600/60000\tLoss: 161.293\n",
      "28800/60000\tLoss: 162.973\n",
      "32000/60000\tLoss: 163.558\n",
      "35200/60000\tLoss: 162.161\n",
      "38400/60000\tLoss: 164.154\n",
      "41600/60000\tLoss: 162.741\n",
      "44800/60000\tLoss: 163.632\n",
      "48000/60000\tLoss: 163.665\n",
      "51200/60000\tLoss: 163.480\n",
      "54400/60000\tLoss: 162.521\n",
      "57600/60000\tLoss: 163.580\n",
      "Epoch: 91 Average loss: 163.18\n",
      "0/60000\tLoss: 166.734\n",
      "3200/60000\tLoss: 163.289\n",
      "6400/60000\tLoss: 162.375\n",
      "9600/60000\tLoss: 162.587\n",
      "12800/60000\tLoss: 163.142\n",
      "16000/60000\tLoss: 164.988\n",
      "19200/60000\tLoss: 162.767\n",
      "22400/60000\tLoss: 163.648\n",
      "25600/60000\tLoss: 162.116\n",
      "28800/60000\tLoss: 163.450\n",
      "32000/60000\tLoss: 163.385\n",
      "35200/60000\tLoss: 163.758\n",
      "38400/60000\tLoss: 161.204\n",
      "41600/60000\tLoss: 162.850\n",
      "44800/60000\tLoss: 162.796\n",
      "48000/60000\tLoss: 164.764\n",
      "51200/60000\tLoss: 163.841\n",
      "54400/60000\tLoss: 164.078\n",
      "57600/60000\tLoss: 163.564\n",
      "Epoch: 92 Average loss: 163.33\n",
      "0/60000\tLoss: 164.191\n",
      "3200/60000\tLoss: 162.964\n",
      "6400/60000\tLoss: 162.074\n",
      "9600/60000\tLoss: 163.442\n",
      "12800/60000\tLoss: 163.704\n",
      "16000/60000\tLoss: 161.933\n",
      "19200/60000\tLoss: 163.170\n",
      "22400/60000\tLoss: 161.450\n",
      "25600/60000\tLoss: 161.548\n",
      "28800/60000\tLoss: 163.205\n",
      "32000/60000\tLoss: 163.140\n",
      "35200/60000\tLoss: 162.953\n",
      "38400/60000\tLoss: 162.878\n",
      "41600/60000\tLoss: 163.092\n",
      "44800/60000\tLoss: 163.541\n",
      "48000/60000\tLoss: 164.586\n",
      "51200/60000\tLoss: 166.417\n",
      "54400/60000\tLoss: 162.391\n",
      "57600/60000\tLoss: 163.350\n",
      "Epoch: 93 Average loss: 163.14\n",
      "0/60000\tLoss: 166.800\n",
      "3200/60000\tLoss: 164.407\n",
      "6400/60000\tLoss: 163.155\n",
      "9600/60000\tLoss: 162.138\n",
      "12800/60000\tLoss: 162.526\n",
      "16000/60000\tLoss: 162.639\n",
      "19200/60000\tLoss: 163.629\n",
      "22400/60000\tLoss: 163.381\n",
      "25600/60000\tLoss: 160.767\n",
      "28800/60000\tLoss: 162.913\n",
      "32000/60000\tLoss: 162.258\n",
      "35200/60000\tLoss: 164.615\n",
      "38400/60000\tLoss: 162.938\n",
      "41600/60000\tLoss: 161.887\n",
      "44800/60000\tLoss: 163.099\n",
      "48000/60000\tLoss: 162.226\n",
      "51200/60000\tLoss: 164.578\n",
      "54400/60000\tLoss: 162.909\n",
      "57600/60000\tLoss: 164.621\n",
      "Epoch: 94 Average loss: 163.15\n",
      "0/60000\tLoss: 164.930\n",
      "3200/60000\tLoss: 162.911\n",
      "6400/60000\tLoss: 163.225\n",
      "9600/60000\tLoss: 162.255\n",
      "12800/60000\tLoss: 161.921\n",
      "16000/60000\tLoss: 163.184\n",
      "19200/60000\tLoss: 163.064\n",
      "22400/60000\tLoss: 163.964\n",
      "25600/60000\tLoss: 163.758\n",
      "28800/60000\tLoss: 163.950\n",
      "32000/60000\tLoss: 163.511\n",
      "35200/60000\tLoss: 160.685\n",
      "38400/60000\tLoss: 161.291\n",
      "41600/60000\tLoss: 162.964\n",
      "44800/60000\tLoss: 164.825\n",
      "48000/60000\tLoss: 162.938\n",
      "51200/60000\tLoss: 162.509\n",
      "54400/60000\tLoss: 161.441\n",
      "57600/60000\tLoss: 162.510\n",
      "Epoch: 95 Average loss: 162.86\n",
      "0/60000\tLoss: 155.505\n",
      "3200/60000\tLoss: 163.202\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6400/60000\tLoss: 163.273\n",
      "9600/60000\tLoss: 162.679\n",
      "12800/60000\tLoss: 162.635\n",
      "16000/60000\tLoss: 160.683\n",
      "19200/60000\tLoss: 162.395\n",
      "22400/60000\tLoss: 162.585\n",
      "25600/60000\tLoss: 163.342\n",
      "28800/60000\tLoss: 164.467\n",
      "32000/60000\tLoss: 163.659\n",
      "35200/60000\tLoss: 162.630\n",
      "38400/60000\tLoss: 163.480\n",
      "41600/60000\tLoss: 163.722\n",
      "44800/60000\tLoss: 164.213\n",
      "48000/60000\tLoss: 163.579\n",
      "51200/60000\tLoss: 162.330\n",
      "54400/60000\tLoss: 163.425\n",
      "57600/60000\tLoss: 163.476\n",
      "Epoch: 96 Average loss: 163.19\n",
      "0/60000\tLoss: 171.721\n",
      "3200/60000\tLoss: 164.232\n",
      "6400/60000\tLoss: 163.798\n",
      "9600/60000\tLoss: 162.980\n",
      "12800/60000\tLoss: 162.233\n",
      "16000/60000\tLoss: 163.057\n",
      "19200/60000\tLoss: 162.701\n",
      "22400/60000\tLoss: 162.920\n",
      "25600/60000\tLoss: 163.799\n",
      "28800/60000\tLoss: 160.633\n",
      "32000/60000\tLoss: 163.456\n",
      "35200/60000\tLoss: 164.777\n",
      "38400/60000\tLoss: 162.571\n",
      "41600/60000\tLoss: 163.532\n",
      "44800/60000\tLoss: 163.394\n",
      "48000/60000\tLoss: 162.187\n",
      "51200/60000\tLoss: 162.186\n",
      "54400/60000\tLoss: 164.837\n",
      "57600/60000\tLoss: 162.749\n",
      "Epoch: 97 Average loss: 163.19\n",
      "0/60000\tLoss: 150.354\n",
      "3200/60000\tLoss: 161.038\n",
      "6400/60000\tLoss: 163.685\n",
      "9600/60000\tLoss: 163.313\n",
      "12800/60000\tLoss: 162.567\n",
      "16000/60000\tLoss: 163.811\n",
      "19200/60000\tLoss: 162.409\n",
      "22400/60000\tLoss: 163.113\n",
      "25600/60000\tLoss: 163.660\n",
      "28800/60000\tLoss: 163.504\n",
      "32000/60000\tLoss: 164.737\n",
      "35200/60000\tLoss: 162.393\n",
      "38400/60000\tLoss: 163.590\n",
      "41600/60000\tLoss: 161.001\n",
      "44800/60000\tLoss: 163.672\n",
      "48000/60000\tLoss: 162.393\n",
      "51200/60000\tLoss: 162.952\n",
      "54400/60000\tLoss: 163.697\n",
      "57600/60000\tLoss: 162.288\n",
      "Epoch: 98 Average loss: 162.99\n",
      "0/60000\tLoss: 164.740\n",
      "3200/60000\tLoss: 163.062\n",
      "6400/60000\tLoss: 163.146\n",
      "9600/60000\tLoss: 163.219\n",
      "12800/60000\tLoss: 161.977\n",
      "16000/60000\tLoss: 162.062\n",
      "19200/60000\tLoss: 164.006\n",
      "22400/60000\tLoss: 162.650\n",
      "25600/60000\tLoss: 161.841\n",
      "28800/60000\tLoss: 164.017\n",
      "32000/60000\tLoss: 162.279\n",
      "35200/60000\tLoss: 162.614\n",
      "38400/60000\tLoss: 161.756\n",
      "41600/60000\tLoss: 162.263\n",
      "44800/60000\tLoss: 163.279\n",
      "48000/60000\tLoss: 163.917\n",
      "51200/60000\tLoss: 162.559\n",
      "54400/60000\tLoss: 164.356\n",
      "57600/60000\tLoss: 162.304\n",
      "Epoch: 99 Average loss: 162.86\n",
      "0/60000\tLoss: 153.887\n",
      "3200/60000\tLoss: 162.457\n",
      "6400/60000\tLoss: 162.106\n",
      "9600/60000\tLoss: 162.982\n",
      "12800/60000\tLoss: 161.369\n",
      "16000/60000\tLoss: 165.522\n",
      "19200/60000\tLoss: 162.127\n",
      "22400/60000\tLoss: 163.359\n",
      "25600/60000\tLoss: 163.546\n",
      "28800/60000\tLoss: 163.055\n",
      "32000/60000\tLoss: 163.267\n",
      "35200/60000\tLoss: 164.047\n",
      "38400/60000\tLoss: 161.909\n",
      "41600/60000\tLoss: 161.116\n",
      "44800/60000\tLoss: 161.113\n",
      "48000/60000\tLoss: 163.591\n",
      "51200/60000\tLoss: 164.029\n",
      "54400/60000\tLoss: 161.353\n",
      "57600/60000\tLoss: 163.792\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Lossy conversion from float32 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
      "Lossy conversion from float32 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
      "Lossy conversion from float32 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
      "Lossy conversion from float32 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
      "Lossy conversion from float32 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
      "Lossy conversion from float32 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
      "Lossy conversion from float32 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
      "Lossy conversion from float32 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
      "Lossy conversion from float32 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
      "Lossy conversion from float32 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
      "Lossy conversion from float32 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
      "Lossy conversion from float32 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
      "Lossy conversion from float32 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
      "Lossy conversion from float32 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
      "Lossy conversion from float32 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 100 Average loss: 162.93\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Lossy conversion from float32 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
      "Lossy conversion from float32 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
      "Lossy conversion from float32 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
      "Lossy conversion from float32 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
      "Lossy conversion from float32 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
      "Lossy conversion from float32 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
      "Lossy conversion from float32 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
      "Lossy conversion from float32 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
      "Lossy conversion from float32 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
      "Lossy conversion from float32 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
      "Lossy conversion from float32 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
      "Lossy conversion from float32 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
      "Lossy conversion from float32 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
      "Lossy conversion from float32 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
      "Lossy conversion from float32 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
      "Lossy conversion from float32 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
      "Lossy conversion from float32 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
      "Lossy conversion from float32 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
      "Lossy conversion from float32 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
      "Lossy conversion from float32 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
      "Lossy conversion from float32 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
      "Lossy conversion from float32 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
      "Lossy conversion from float32 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
      "Lossy conversion from float32 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
      "Lossy conversion from float32 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
      "Lossy conversion from float32 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
      "Lossy conversion from float32 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
      "Lossy conversion from float32 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
      "Lossy conversion from float32 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
      "Lossy conversion from float32 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
      "Lossy conversion from float32 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
      "Lossy conversion from float32 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
      "Lossy conversion from float32 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
      "Lossy conversion from float32 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
      "Lossy conversion from float32 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
      "Lossy conversion from float32 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
      "Lossy conversion from float32 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
      "Lossy conversion from float32 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
      "Lossy conversion from float32 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
      "Lossy conversion from float32 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
      "Lossy conversion from float32 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
      "Lossy conversion from float32 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
      "Lossy conversion from float32 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
      "Lossy conversion from float32 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
      "Lossy conversion from float32 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
      "Lossy conversion from float32 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
      "Lossy conversion from float32 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
      "Lossy conversion from float32 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
      "Lossy conversion from float32 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
      "Lossy conversion from float32 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
      "Lossy conversion from float32 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
      "Lossy conversion from float32 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
      "Lossy conversion from float32 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
      "Lossy conversion from float32 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
      "Lossy conversion from float32 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
      "Lossy conversion from float32 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
      "Lossy conversion from float32 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
      "Lossy conversion from float32 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
      "Lossy conversion from float32 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
      "Lossy conversion from float32 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
      "Lossy conversion from float32 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
      "Lossy conversion from float32 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
      "Lossy conversion from float32 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
      "Lossy conversion from float32 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
      "Lossy conversion from float32 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
      "Lossy conversion from float32 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
      "Lossy conversion from float32 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
      "Lossy conversion from float32 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
      "Lossy conversion from float32 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Lossy conversion from float32 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
      "Lossy conversion from float32 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
      "Lossy conversion from float32 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
      "Lossy conversion from float32 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
      "Lossy conversion from float32 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
      "Lossy conversion from float32 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
      "Lossy conversion from float32 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
      "Lossy conversion from float32 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
      "Lossy conversion from float32 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
      "Lossy conversion from float32 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
      "Lossy conversion from float32 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
      "Lossy conversion from float32 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
      "Lossy conversion from float32 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
      "Lossy conversion from float32 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
      "Lossy conversion from float32 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
      "Lossy conversion from float32 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n"
     ]
    }
   ],
   "source": [
    "# Train model for 10 epochs\n",
    "# Note this should really be a 100 epochs and trained on a GPU, but this is just to demo\n",
    "\n",
    "trainer.train(train_loader, epochs=100, save_training_gif=('./training.gif', viz))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = enumerate(test_loader)\n",
    "batch_idx, (example_data, example_targets) = next(examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([64, 1, 32, 32]), torch.Size([64]))"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_data.shape, example_targets.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_img = example_data[0,:,:,:].unsqueeze(0).cuda()\n",
    "output, l_dist = model(example_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "l_sample = model.reparameterize(l_dist)\n",
    "new_l_sample = l_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 20])"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l_sample.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_l_sample = l_sample\n",
    "new_l_sample = 0.25 * l_sample\n",
    "new_output = model.decode(new_l_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fb613c1cef0>"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(new_output[0][0].detach().cpu().numpy(), cmap='gray', interpolation='none')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fb613d19c88>"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(output[0][0].detach().cpu().numpy(), cmap='gray', interpolation='none')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fb613cf37f0>"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(example_img[0][0].detach().cpu().numpy(), cmap='gray', interpolation='none')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1, device='cuda:0')"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.argmax(classifier(F.upsample(example_img, (28,28), mode='bilinear', align_corners=True)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check(i, j, alpha, beta):\n",
    "    im1 = example_data[i,:,:,:].unsqueeze(0).cuda()\n",
    "    im2 = example_data[j,:,:,:].unsqueeze(0).cuda()\n",
    "    out1, l_dist1 = model(im1)\n",
    "    out2, l_dist2 = model(im2)\n",
    "    l_sample1 = model.reparameterize(l_dist1)\n",
    "    l_sample2 = model.reparameterize(l_dist2)\n",
    "    l_sample = alpha*l_sample1 + beta*l_sample2\n",
    "    new_out = model.decode(l_sample)\n",
    "#     new_out1 = model.decode(l_sample1)\n",
    "#     new_out2 = model.decode(l_sample2)\n",
    "    plt.figure(figsize=(10,15))\n",
    "    plt.subplot(1,3,1)\n",
    "    plt.imshow(example_data[i][0].detach().cpu().numpy(), cmap='gray', interpolation='none')\n",
    "    plt.subplot(1,3,2)\n",
    "    plt.imshow(example_data[j][0].detach().cpu().numpy(), cmap='gray', interpolation='none')\n",
    "    plt.subplot(1,3,3)\n",
    "    plt.imshow(new_out[0][0].detach().cpu().numpy(), cmap='gray', interpolation='none')\n",
    "    print(torch.argmax(classifier(F.upsample(new_out, (28,28), mode='bilinear', align_corners=True))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0, device='cuda:0')\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x1080 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "alpha=1\n",
    "beta = 1.2\n",
    "check(19,11, alpha, beta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Classifier, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, 3, 1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, 1)\n",
    "        self.dropout1 = nn.Dropout2d(0.25)\n",
    "        self.dropout2 = nn.Dropout2d(0.5)\n",
    "        self.fc1 = nn.Linear(9216, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = self.dropout1(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.fc2(x)\n",
    "        output = F.softmax(x, dim=1)\n",
    "        return output\n",
    "\n",
    "classifier = Classifier().cuda()\n",
    "classifier.load_state_dict(torch.load('../models/mnist_cnn_non_log.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import exp\n",
    "\n",
    "def gaussian(window_size, sigma):\n",
    "    gauss = torch.Tensor([exp(-(x - window_size//2)**2/float(2*sigma**2)) for x in range(window_size)])\n",
    "    return gauss/gauss.sum()\n",
    "\n",
    "def create_window(window_size, channel):\n",
    "    _1D_window = gaussian(window_size, 1.5).unsqueeze(1)\n",
    "    _2D_window = _1D_window.mm(_1D_window.t()).float().unsqueeze(0).unsqueeze(0)\n",
    "    window = torch.Tensor(_2D_window.expand(channel, 1, window_size, window_size).contiguous())\n",
    "    return window\n",
    "\n",
    "def _ssim(img1, img2, window, window_size, channel, size_average = True):\n",
    "    mu1 = F.conv2d(img1, window, padding = window_size//2, groups = channel)\n",
    "    mu2 = F.conv2d(img2, window, padding = window_size//2, groups = channel)\n",
    "\n",
    "    mu1_sq = mu1.pow(2)\n",
    "    mu2_sq = mu2.pow(2)\n",
    "    mu1_mu2 = mu1*mu2\n",
    "\n",
    "    sigma1_sq = F.conv2d(img1*img1, window, padding = window_size//2, groups = channel) - mu1_sq\n",
    "    sigma2_sq = F.conv2d(img2*img2, window, padding = window_size//2, groups = channel) - mu2_sq\n",
    "    sigma12 = F.conv2d(img1*img2, window, padding = window_size//2, groups = channel) - mu1_mu2\n",
    "\n",
    "    C1 = 0.01**2\n",
    "    C2 = 0.03**2\n",
    "\n",
    "    ssim_map = ((2*mu1_mu2 + C1)*(2*sigma12 + C2))/((mu1_sq + mu2_sq + C1)*(sigma1_sq + sigma2_sq + C2))\n",
    "\n",
    "    if size_average:\n",
    "        return ssim_map.mean()\n",
    "    else:\n",
    "        return ssim_map.mean(1).mean(1).mean(1)\n",
    "\n",
    "class SSIM(nn.Module):\n",
    "    def __init__(self, window_size = 11, size_average = True):\n",
    "        super(SSIM, self).__init__()\n",
    "        self.window_size = window_size\n",
    "        self.size_average = size_average\n",
    "        self.channel = 1\n",
    "        self.window = create_window(window_size, self.channel)\n",
    "\n",
    "    def forward(self, img1, img2):\n",
    "        (_, channel, _, _) = img1.size()\n",
    "\n",
    "        if channel == self.channel and self.window.data.type() == img1.data.type():\n",
    "            window = self.window\n",
    "        else:\n",
    "            window = create_window(self.window_size, channel)\n",
    "            \n",
    "            if img1.is_cuda:\n",
    "                window = window.cuda(img1.get_device())\n",
    "            window = window.type_as(img1)\n",
    "            \n",
    "            self.window = window\n",
    "            self.channel = channel\n",
    "\n",
    "\n",
    "        return _ssim(img1, img2, window, self.window_size, channel, self.size_average)\n",
    "\n",
    "def ssim(img1, img2, window_size = 11, size_average = True):\n",
    "    (_, channel, _, _) = img1.size()\n",
    "    window = create_window(window_size, channel)\n",
    "    \n",
    "    if img1.is_cuda:\n",
    "        window = window.cuda(img1.get_device())\n",
    "    window = window.type_as(img1)\n",
    "    \n",
    "    return _ssim(img1, img2, window, window_size, channel, size_average)\n",
    "\n",
    "ssim_loss = SSIM(window_size = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cuda = True\n",
    "device = torch.device('cuda' if use_cuda else 'cpu')\n",
    "# Constrained Translator\n",
    "class Translator(nn.Module):\n",
    "    def __init__(self, hidden_layers=[5, 10, 5], latent_dim=20):\n",
    "        super(Translator, self).__init__()\n",
    "        self.hidden_layers = hidden_layers\n",
    "        self.hidden_layers.insert(0, latent_dim)\n",
    "        self.hidden_layers.append(latent_dim)\n",
    "        self.layers = []\n",
    "        \n",
    "        for i in range(len(self.hidden_layers)-1):\n",
    "            self.layers.append(nn.Linear(self.hidden_layers[i], self.hidden_layers[i+1]))\n",
    "        \n",
    "        self.layers = nn.ModuleList(self.layers)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        l_sample = x\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        noised_sample = x\n",
    "        noised_sample = 1.2 * l_sample + 4.9e-7 * noised_sample\n",
    "        return x\n",
    "\n",
    "translator = Translator().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from skimage.metrics import structural_similarity as ssim\n",
    "\n",
    "def create_logits(target_label, pred, confidence=0.5, device=device):\n",
    "    logits = torch.zeros(pred.shape, dtype=torch.float64)\n",
    "    pred_labels = [int(element.item()) for element in torch.argmax(pred, dim=1)]\n",
    "#     print(\"length\", len(pred_labels))\n",
    "#     print(\"pred_lables:\",pred_labels)\n",
    "#     print(logits.shape)\n",
    "    logits[range(logits.shape[0]), pred_labels] = torch.DoubleTensor([1-confidence]*pred.shape[0])\n",
    "    logits[range(logits.shape[0]), [target_label]*pred.shape[0]] += torch.DoubleTensor([confidence]*pred.shape[0])\n",
    "    return logits.to(device)\n",
    "\n",
    "def structural(org_image, noised_image):\n",
    "    batch_size, channels, width, height = org_image.shape\n",
    "    loss1 = 0\n",
    "    for b_ in range(batch_size):\n",
    "        ch_loss = 0\n",
    "        for ch_ in range(channels):\n",
    "            ch_loss += 1-ssim(org_image[b_][ch_].detach().cpu().numpy(), noised_image[b_][ch_].detach().cpu().numpy())\n",
    "        loss1 += ch_loss/channels\n",
    "    return loss1\n",
    "            \n",
    "class T_Loss(nn.Module):\n",
    "    def __init__(self, decoder=model.decode, classifier=classifier):\n",
    "        super(T_Loss, self).__init__()\n",
    "        self.decoder = decoder\n",
    "        self.classifier = classifier\n",
    "    def forward(self, x, org_x, target_label):\n",
    "        \n",
    "        org_image = self.decoder(org_x)\n",
    "        noised_image = self.decoder(x)\n",
    "        batch_size, channels, width, height = org_image.shape\n",
    "        loss1 = ssim_loss(org_image, noised_image)\n",
    "#         print(type(loss1))\n",
    "#         print(loss1)\n",
    "#         loss1 = torch.Tensor(loss1 / batch_size).to(device)\n",
    "#         print(\"loss1:\", loss1)\n",
    "        preds = self.classifier(F.upsample(noised_image, (28,28), mode='bilinear', align_corners=True))\n",
    "#         print(\"preds:\",preds)\n",
    "        target = create_logits(target_label, preds)\n",
    "#         print(target)\n",
    "        loss2 = nn.BCELoss(reduction='sum')(preds, target.float())\n",
    "#         print(type(loss2))\n",
    "#         print(\"loss1:\",100*(1-loss1))\n",
    "#         print(\"loss2:\",loss2)\n",
    "        loss = 200*(1-loss1) + loss2\n",
    "        \n",
    "        \n",
    "        out_labels = preds.argmax(dim=1, keepdim=True)\n",
    "#         print(out_labels)\n",
    "#         print(torch.empty(out_labels.shape).fill_(target_label))\n",
    "#         print(preds)\n",
    "#         correct = out_labels.eq(torch.Tensor([target_label]*out_labels.shape[0]).to(device)).sum()\n",
    "        correct = out_labels.eq(torch.empty(out_labels.shape).fill_(target_label).to(device)).sum()\n",
    "#         print(out_labels.shape)\n",
    "#         print(correct)\n",
    "#         print(torch.Tensor([target_label]*out_labels.shape[0]))\n",
    "#         print(out_labels)\n",
    "        return loss, correct.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "T_Loss(\n",
       "  (classifier): Classifier(\n",
       "    (conv1): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (dropout1): Dropout2d(p=0.25, inplace=False)\n",
       "    (dropout2): Dropout2d(p=0.5, inplace=False)\n",
       "    (fc1): Linear(in_features=9216, out_features=128, bias=True)\n",
       "    (fc2): Linear(in_features=128, out_features=10, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 300,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tloss = T_Loss().to(device)\n",
    "tloss.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VAE(\n",
       "  (img_to_features): Sequential(\n",
       "    (0): Conv2d(1, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (1): ReLU()\n",
       "    (2): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (3): ReLU()\n",
       "    (4): Conv2d(64, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (5): ReLU()\n",
       "  )\n",
       "  (features_to_hidden): Sequential(\n",
       "    (0): Linear(in_features=1024, out_features=256, bias=True)\n",
       "    (1): ReLU()\n",
       "  )\n",
       "  (fc_mean): Linear(in_features=256, out_features=10, bias=True)\n",
       "  (fc_log_var): Linear(in_features=256, out_features=10, bias=True)\n",
       "  (fc_alphas): ModuleList(\n",
       "    (0): Linear(in_features=256, out_features=10, bias=True)\n",
       "  )\n",
       "  (latent_to_features): Sequential(\n",
       "    (0): Linear(in_features=20, out_features=256, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=256, out_features=1024, bias=True)\n",
       "    (3): ReLU()\n",
       "  )\n",
       "  (features_to_img): Sequential(\n",
       "    (0): ConvTranspose2d(64, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (1): ReLU()\n",
       "    (2): ConvTranspose2d(32, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (3): ReLU()\n",
       "    (4): ConvTranspose2d(32, 1, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (5): Sigmoid()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 301,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "metadata": {},
   "outputs": [],
   "source": [
    "translator = Translator().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59701c1ec0094fffa74ed3e8800b233c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=20.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0\tLoss: 355.146811\tCorrect: 240\n",
      "Train Epoch: 1\tLoss: 155.563796\tCorrect: 55199\n",
      "Train Epoch: 2\tLoss: 108.466862\tCorrect: 60000\n",
      "Train Epoch: 3\tLoss: 81.091524\tCorrect: 59922\n",
      "Train Epoch: 4\tLoss: 58.762144\tCorrect: 60000\n",
      "Train Epoch: 5\tLoss: 55.665399\tCorrect: 60000\n",
      "Train Epoch: 6\tLoss: 54.086596\tCorrect: 60000\n",
      "Train Epoch: 7\tLoss: 52.544739\tCorrect: 60000\n",
      "Train Epoch: 8\tLoss: 51.672013\tCorrect: 60000\n",
      "Train Epoch: 9\tLoss: 51.175964\tCorrect: 60000\n",
      "Train Epoch: 10\tLoss: 50.793209\tCorrect: 60000\n",
      "Train Epoch: 11\tLoss: 50.632449\tCorrect: 60000\n",
      "Train Epoch: 12\tLoss: 50.446061\tCorrect: 60000\n",
      "Train Epoch: 13\tLoss: 50.355388\tCorrect: 60000\n",
      "Train Epoch: 14\tLoss: 50.269844\tCorrect: 60000\n",
      "Train Epoch: 15\tLoss: 50.243272\tCorrect: 60000\n",
      "Train Epoch: 16\tLoss: 50.258003\tCorrect: 60000\n",
      "Train Epoch: 17\tLoss: 50.282607\tCorrect: 60000\n",
      "Train Epoch: 18\tLoss: 50.286113\tCorrect: 60000\n",
      "Train Epoch: 19\tLoss: 50.258231\tCorrect: 59999\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "attack_log_interval = 1\n",
    "alt_target = 0\n",
    "translator.train()\n",
    "optimizer = optim.Adam(translator.parameters(), lr=1e-5)\n",
    "for epoch in tqdm(range(20)):\n",
    "    epoch_loss = 0\n",
    "    epoch_correct = 0\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data = torch.FloatTensor(data).to(device)\n",
    "        \n",
    "        _, l_dist = model(data)\n",
    "        l_sample = model.reparameterize(l_dist)\n",
    "        \n",
    "        noised_sample = translator(l_sample)\n",
    "        loss, correct = tloss(noised_sample, l_sample, alt_target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "#         print(correct)\n",
    "        epoch_loss += loss.item()\n",
    "        epoch_correct += correct\n",
    "        \n",
    "    if (epoch+1) % attack_log_interval == 0:\n",
    "        print('Train Epoch: {}\\tLoss: {:.6f}\\tCorrect: {}'.format(\n",
    "            epoch+1, epoch_loss/batch_idx, epoch_correct))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 450,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7207\n",
      "Accuracy:  72.07000000000001\n"
     ]
    }
   ],
   "source": [
    "total_correct = 0\n",
    "total_test = 0\n",
    "for batch_idx, (data, target) in enumerate(test_loader):\n",
    "    total_test += data.shape[0]\n",
    "    data = torch.FloatTensor(data).to(device)\n",
    "\n",
    "    _, l_dist = model(data)\n",
    "    l_sample = model.reparameterize(l_dist)\n",
    "\n",
    "    noised_sample = translator(l_sample)\n",
    "#     noised_sample = 1 * l_sample + 6e-7 * noised_sample #2\n",
    "    noised_sample = 1 * l_sample + 2e-2 * noised_sample #0\n",
    "    loss, correct = tloss(noised_sample, l_sample, alt_target)\n",
    "    total_correct += correct\n",
    "#     print(correct)\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "#         print(correct)\n",
    "#         epoch_loss += loss.item()\n",
    "    \n",
    "\n",
    "#     if (epoch+1) % attack_log_interval == 0:\n",
    "#         print('Train Epoch: \\tCorrect: {}'.format(\n",
    "#             epoch, epoch_correct))\n",
    "print(total_correct)\n",
    "print(\"Accuracy: \", 100*(total_correct/total_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 451,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(i):\n",
    "    _, l_dist = model(example_data[i].unsqueeze_(0).to(device))\n",
    "    l_sample = model.reparameterize(l_dist)\n",
    "    noised_sample = translator(l_sample)\n",
    "    print(noised_sample)\n",
    "    print(l_sample)\n",
    "#     noised_sample = 1 * ((l_sample - l_sample.min())/(l_sample.max() - l_sample.min())) + 1e-2 * ((noised_sample - noised_sample.min())/(noised_sample.max() - noised_sample.min()))\n",
    "    noised_sample = 1 * l_sample + 2e-2 * noised_sample\n",
    "#     noised_sample = l_sample + 1e-7 * noised_sample\n",
    "    final = model.decode(noised_sample)\n",
    "    pred_org = torch.argmax(classifier(F.upsample(example_data[i,:,:,:].unsqueeze(0).cuda(), (28,28), mode='bilinear', align_corners=True)))\n",
    "    pred = torch.argmax(classifier(F.upsample(final, (28,28), mode='bilinear', align_corners=True)))\n",
    "    print(\"Prediction: {}, {}\".format(pred_org.item(), pred.item()))\n",
    "    plt.figure(figsize=(10,10))\n",
    "    plt.subplot(1,2,1)\n",
    "    plt.imshow(example_data[i][0].detach().cpu().numpy(), cmap='gray', interpolation='none')\n",
    "    plt.subplot(1,2,2)\n",
    "    plt.imshow(final[0][0].detach().cpu().numpy(), cmap='gray', interpolation='none')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 462,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-3.6366, -3.1751,  6.1095,  1.6212,  0.8919, -5.7888, -5.0468, -4.4371,\n",
      "          6.0394, -1.3505,  3.0333, -5.3019,  2.9255,  4.8683,  1.7715,  2.1769,\n",
      "          0.6416,  3.2473,  5.5838, -5.2517]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward>)\n",
      "tensor([[ 0.0280,  0.4251,  0.0480, -0.0193, -0.0128, -0.5147,  0.7122,  0.6263,\n",
      "         -0.0048,  1.1076,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  1.0000,  0.0000]], device='cuda:0',\n",
      "       grad_fn=<CatBackward>)\n",
      "Prediction: 3, 0\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "test(23)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
