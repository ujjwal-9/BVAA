{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from torchvision.utils import save_image\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from timeit import default_timer\n",
    "from collections import defaultdict\n",
    "from tqdm import trange\n",
    "\n",
    "class Trainer():\n",
    "    def __init__(self, model, optimizer, loss_f, device, save_dir=\"results\", is_progress_bar=True):\n",
    "            self.device = device\n",
    "            self.model = model.to(device)\n",
    "            self.loss_f = loss_f\n",
    "            self.optimizer = optimizer\n",
    "            self.save_dir = save_dir\n",
    "            self.is_progress_bar = is_progress_bar\n",
    "            \n",
    "    def __call__(self, data_loader, epochs=10, checkpoint_every=10):\n",
    "        start = default_timer()\n",
    "        self.model.train()\n",
    "        for epoch in range(epochs):\n",
    "            storer = defaultdict(list)\n",
    "            mean_epoch_loss = self._train_epoch(data_loader, storer, epoch)\n",
    "            mean_epoch_loss = self._test_epoch(data_loader, storer, epoch)\n",
    "            with torch.no_grad():\n",
    "                sample = torch.randn(64, self.model.latent_dim).to(device)\n",
    "                sample = self.model.decoder(sample).cpu()  # make sure on cpu\n",
    "                save_image(sample.view(64, 1, 32, 32),\n",
    "                           './results/samples/' + str(epoch) + '.png')\n",
    "            \n",
    "    def _train_epoch(self, data_loader, storer, epoch):\n",
    "        epoch_loss = 0.\n",
    "        kwargs = dict(desc=\"Epoch {}\".format(epoch + 1), leave=False,\n",
    "                      disable=not self.is_progress_bar)\n",
    "        with trange(len(data_loader), **kwargs) as t:\n",
    "            for _, (data, _) in enumerate(data_loader):\n",
    "                iter_loss = self._train_iteration(data, storer)\n",
    "                epoch_loss += iter_loss\n",
    "                t.set_postfix(loss=iter_loss)\n",
    "                t.update()\n",
    "        mean_epoch_loss = epoch_loss / len(data_loader)\n",
    "        return mean_epoch_loss\n",
    "    \n",
    "    def _train_iteration(self, data, storer):\n",
    "        batch_size, channel, height, width = data.size()\n",
    "        data = data.to(self.device)\n",
    "        recon_batch, latent_dist, latent_sample = self.model(data)\n",
    "        loss = self.loss_f(data, recon_batch, latent_dist, self.model.training, \n",
    "                           storer, latent_sample=latent_sample)\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "            \n",
    "        return loss.item()\n",
    "    \n",
    "    def _test_epoch(self, data_loader, storer, epoch):\n",
    "        epoch_loss = 0.\n",
    "        kwargs = dict(desc=\"Epoch {}\".format(epoch + 1), leave=False,\n",
    "                      disable=not self.is_progress_bar)\n",
    "        with trange(len(data_loader), **kwargs) as t:\n",
    "            for _, (data, _) in enumerate(data_loader):\n",
    "                iter_loss = self._train_iteration(data, storer)\n",
    "                epoch_loss += iter_loss\n",
    "                t.set_postfix(loss=iter_loss)\n",
    "                t.update()\n",
    "        mean_epoch_loss = epoch_loss / len(data_loader)\n",
    "        return mean_epoch_loss\n",
    "    \n",
    "    def _test_iteration(self, data, storer):\n",
    "        batch_size, channel, height, width = data.size()\n",
    "        data = data.to(self.device)\n",
    "        recon_batch, latent_dist, latent_sample = self.model(data)\n",
    "        loss = self.loss_f(data, recon_batch, latent_dist, self.model.training, \n",
    "                               storer, latent_sample=latent_sample)\n",
    "            \n",
    "        return loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOSSES = [\"betaH\", \"betaB\"]\n",
    "RECON_DIST = [\"bernoulli\", \"laplace\", \"gaussian\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "mnist_dataset = datasets.MNIST('/home/data/bvaa', \n",
    "                   train=True, \n",
    "                   download=True, \n",
    "                   transform=transforms.Compose([\n",
    "                       transforms.Resize(32),\n",
    "                       transforms.ToTensor()\n",
    "                   ]))\n",
    "\n",
    "mnist_dataset_test = datasets.MNIST('/home/data/bvaa', train=False, download=True, transform=transforms.Compose([\n",
    "                       transforms.Resize(32),\n",
    "                       transforms.ToTensor()\n",
    "                   ]))\n",
    "\n",
    "train_loader = DataLoader(mnist_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "test_loader = DataLoader(mnist_dataset_test, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = enumerate(test_loader)\n",
    "batch_idx, (example_data, example_targets) = next(examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vae import VAE\n",
    "from encoder import Encoder\n",
    "from decoder import Decoder\n",
    "from losses import get_loss_fn\n",
    "from torch import optim\n",
    "\n",
    "latent_dim = 12\n",
    "img_size = [1,32,32]\n",
    "\n",
    "lr = 5e-4\n",
    "\n",
    "betaB_args = {\"rec_dist\": \"bernoulli\",\n",
    "              \"reg_anneal\": 10000, \n",
    "              \"betaH_B\": 4,\n",
    "              \"betaB_initC\": 0,\n",
    "              \"betaB_finC\": 25,\n",
    "              \"betaB_G\": 100\n",
    "             }\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "loss_f = get_loss_fn(\"betaB\", n_data=len(train_loader.dataset), device=device, **betaB_args)\n",
    "\n",
    "encoder = Encoder(img_size, latent_dim)\n",
    "decoder = Decoder(img_size, latent_dim)\n",
    "\n",
    "generator_model = VAE(img_size, latent_dim, encoder, decoder).to(device)\n",
    "optimizer = optim.Adam(generator_model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer = Trainer(generator_model, optimizer, loss_f, device, logger=logger,\\\n",
    "#                   save_dir=exp_dir, is_progress_bar=False)\n",
    "# epochs = 100\n",
    "# checkpoint_every = 10\n",
    "# trainer(train_loader, epochs=epochs, checkpoint_every=checkpoint_every)\n",
    "# torch.save(trainer.model.state_dict(), 'test.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generator_model.load_state_dict(torch.load('models/test.pt'))\n",
    "# generator_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Classifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Classifier, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, 3, 1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, 1)\n",
    "        self.dropout1 = nn.Dropout2d(0.25)\n",
    "        self.dropout2 = nn.Dropout2d(0.5)\n",
    "        self.fc1 = nn.Linear(9216, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = self.dropout1(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.fc2(x)\n",
    "        output = F.softmax(x, dim=1)\n",
    "        return output\n",
    "\n",
    "classifier = Classifier()\n",
    "classifier.load_state_dict(torch.load('models/mnist_cnn_non_log.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AddNoise():\n",
    "    def __init__(self, shape, device, noise=None, percent_noise=0.1):\n",
    "        self.device = device\n",
    "        self.percent_noise = percent_noise\n",
    "        if noise is None:\n",
    "            self.noise = torch.randn(shape)\n",
    "        else:\n",
    "            self.noise = noise\n",
    "\n",
    "    def noisy(self, data):\n",
    "        x = self.noise.to(self.device) * self.percent_noise + data * (1 - self.percent_noise)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Noise for Testing: \n",
      " tensor([[ 0.7344,  0.0049,  0.1108, -0.9742,  0.7806,  0.4000, -0.6167, -1.0133,\n",
      "          0.4483, -0.2564, -0.7429,  0.7341]])\n"
     ]
    }
   ],
   "source": [
    "sample_noise = torch.randn(torch.Size([1, 12]))\n",
    "backup_sample_noise = sample_noise\n",
    "print(\"Sample Noise for Testing: \\n\", sample_noise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Decoder(\n",
       "  (lin1): Linear(in_features=12, out_features=256, bias=True)\n",
       "  (lin2): Linear(in_features=256, out_features=256, bias=True)\n",
       "  (lin3): Linear(in_features=256, out_features=512, bias=True)\n",
       "  (convT1): ConvTranspose2d(32, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "  (convT2): ConvTranspose2d(32, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "  (convT3): ConvTranspose2d(32, 1, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder = generator_model.decoder\n",
    "decoder.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Classifier(\n",
       "  (conv1): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (dropout1): Dropout2d(p=0.25, inplace=False)\n",
       "  (dropout2): Dropout2d(p=0.5, inplace=False)\n",
       "  (fc1): Linear(in_features=9216, out_features=128, bias=True)\n",
       "  (fc2): Linear(in_features=128, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import exp\n",
    "\n",
    "def gaussian(window_size, sigma):\n",
    "    gauss = torch.Tensor([exp(-(x - window_size//2)**2/float(2*sigma**2)) for x in range(window_size)])\n",
    "    return gauss/gauss.sum()\n",
    "\n",
    "def create_window(window_size, channel):\n",
    "    _1D_window = gaussian(window_size, 1.5).unsqueeze(1)\n",
    "    _2D_window = _1D_window.mm(_1D_window.t()).float().unsqueeze(0).unsqueeze(0)\n",
    "    window = torch.Tensor(_2D_window.expand(channel, 1, window_size, window_size).contiguous())\n",
    "    return window\n",
    "\n",
    "def _ssim(img1, img2, window, window_size, channel, size_average = True):\n",
    "    mu1 = F.conv2d(img1, window, padding = window_size//2, groups = channel)\n",
    "    mu2 = F.conv2d(img2, window, padding = window_size//2, groups = channel)\n",
    "\n",
    "    mu1_sq = mu1.pow(2)\n",
    "    mu2_sq = mu2.pow(2)\n",
    "    mu1_mu2 = mu1*mu2\n",
    "\n",
    "    sigma1_sq = F.conv2d(img1*img1, window, padding = window_size//2, groups = channel) - mu1_sq\n",
    "    sigma2_sq = F.conv2d(img2*img2, window, padding = window_size//2, groups = channel) - mu2_sq\n",
    "    sigma12 = F.conv2d(img1*img2, window, padding = window_size//2, groups = channel) - mu1_mu2\n",
    "\n",
    "    C1 = 0.01**2\n",
    "    C2 = 0.03**2\n",
    "\n",
    "    ssim_map = ((2*mu1_mu2 + C1)*(2*sigma12 + C2))/((mu1_sq + mu2_sq + C1)*(sigma1_sq + sigma2_sq + C2))\n",
    "\n",
    "    if size_average:\n",
    "        return ssim_map.mean()\n",
    "    else:\n",
    "        return ssim_map.mean(1).mean(1).mean(1)\n",
    "\n",
    "class SSIM(nn.Module):\n",
    "    def __init__(self, window_size = 11, size_average = True):\n",
    "        super(SSIM, self).__init__()\n",
    "        self.window_size = window_size\n",
    "        self.size_average = size_average\n",
    "        self.channel = 1\n",
    "        self.window = create_window(window_size, self.channel)\n",
    "\n",
    "    def forward(self, img1, img2):\n",
    "        (_, channel, _, _) = img1.size()\n",
    "\n",
    "        if channel == self.channel and self.window.data.type() == img1.data.type():\n",
    "            window = self.window\n",
    "        else:\n",
    "            window = create_window(self.window_size, channel)\n",
    "            \n",
    "            if img1.is_cuda:\n",
    "                window = window.cuda(img1.get_device())\n",
    "            window = window.type_as(img1)\n",
    "            \n",
    "            self.window = window\n",
    "            self.channel = channel\n",
    "\n",
    "\n",
    "        return _ssim(img1, img2, window, self.window_size, channel, self.size_average)\n",
    "\n",
    "def ssim(img1, img2, window_size = 11, size_average = True):\n",
    "    (_, channel, _, _) = img1.size()\n",
    "    window = create_window(window_size, channel)\n",
    "    \n",
    "    if img1.is_cuda:\n",
    "        window = window.cuda(img1.get_device())\n",
    "    window = window.type_as(img1)\n",
    "    \n",
    "    return _ssim(img1, img2, window, window_size, channel, size_average)\n",
    "\n",
    "ssim_loss = SSIM(window_size = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constrained Translator\n",
    "class Translator(nn.Module):\n",
    "    def __init__(self, hidden_layers=[5, 10, 5], latent_dim=latent_dim):\n",
    "        super(Translator, self).__init__()\n",
    "        self.hidden_layers = hidden_layers\n",
    "        self.hidden_layers.insert(0, latent_dim)\n",
    "        self.hidden_layers.append(latent_dim)\n",
    "        self.layers = []\n",
    "        \n",
    "        for i in range(len(self.hidden_layers)-1):\n",
    "            self.layers.append(nn.Linear(self.hidden_layers[i], self.hidden_layers[i+1]))\n",
    "        \n",
    "        self.layers = nn.ModuleList(self.layers)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "translator = Translator().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from skimage.metrics import structural_similarity as ssim\n",
    "\n",
    "def create_logits(target_label, pred, confidence=0.5, device=device):\n",
    "    logits = torch.zeros(pred.shape, dtype=torch.float64)\n",
    "    pred_labels = [int(element.item()) for element in torch.argmax(pred, dim=1)]\n",
    "#     print(\"length\", len(pred_labels))\n",
    "#     print(\"pred_lables:\",pred_labels)\n",
    "#     print(logits.shape)\n",
    "    logits[range(logits.shape[0]), pred_labels] = torch.DoubleTensor([1-confidence]*pred.shape[0])\n",
    "    logits[range(logits.shape[0]), [target_label]*pred.shape[0]] += torch.DoubleTensor([confidence]*pred.shape[0])\n",
    "    return logits.to(device)\n",
    "\n",
    "def structural(org_image, noised_image):\n",
    "    batch_size, channels, width, height = org_image.shape\n",
    "    loss1 = 0\n",
    "    for b_ in range(batch_size):\n",
    "        ch_loss = 0\n",
    "        for ch_ in range(channels):\n",
    "            ch_loss += 1-ssim(org_image[b_][ch_].detach().cpu().numpy(), noised_image[b_][ch_].detach().cpu().numpy())\n",
    "        loss1 += ch_loss/channels\n",
    "    return loss1\n",
    "            \n",
    "class T_Loss(nn.Module):\n",
    "    def __init__(self, decoder=decoder, classifier=classifier):\n",
    "        super(T_Loss, self).__init__()\n",
    "        self.decoder = decoder\n",
    "        self.classifier = classifier\n",
    "    def forward(self, x, org_x, target_label):\n",
    "        \n",
    "        org_image = self.decoder(org_x)\n",
    "        noised_image = self.decoder(x)\n",
    "        batch_size, channels, width, height = org_image.shape\n",
    "        loss1 = ssim_loss(org_image, noised_image)\n",
    "#         print(type(loss1))\n",
    "#         print(loss1)\n",
    "#         loss1 = torch.Tensor(loss1 / batch_size).to(device)\n",
    "#         print(\"loss1:\", loss1)\n",
    "        preds = self.classifier(F.upsample(noised_image, (28,28), mode='bilinear', align_corners=True))\n",
    "#         print(\"preds:\",preds)\n",
    "        target = create_logits(target_label, preds)\n",
    "#         print(target)\n",
    "        loss2 = nn.BCELoss(reduction='sum')(preds, target.float())\n",
    "#         print(type(loss2))\n",
    "#         print(\"loss1:\",100*(1-loss1))\n",
    "#         print(\"loss2:\",loss2)\n",
    "        loss = 100*(1-loss1) + loss2\n",
    "        \n",
    "        \n",
    "        out_labels = preds.argmax(dim=1, keepdim=True)\n",
    "#         print(out_labels)\n",
    "#         print(torch.empty(out_labels.shape).fill_(target_label))\n",
    "#         print(preds)\n",
    "#         correct = out_labels.eq(torch.Tensor([target_label]*out_labels.shape[0]).to(device)).sum()\n",
    "        correct = out_labels.eq(torch.empty(out_labels.shape).fill_(target_label).to(device)).sum()\n",
    "#         print(out_labels.shape)\n",
    "#         print(correct)\n",
    "#         print(torch.Tensor([target_label]*out_labels.shape[0]))\n",
    "#         print(out_labels)\n",
    "        return loss, correct.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "T_Loss(\n",
       "  (decoder): Decoder(\n",
       "    (lin1): Linear(in_features=12, out_features=256, bias=True)\n",
       "    (lin2): Linear(in_features=256, out_features=256, bias=True)\n",
       "    (lin3): Linear(in_features=256, out_features=512, bias=True)\n",
       "    (convT1): ConvTranspose2d(32, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (convT2): ConvTranspose2d(32, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (convT3): ConvTranspose2d(32, 1, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "  )\n",
       "  (classifier): Classifier(\n",
       "    (conv1): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (dropout1): Dropout2d(p=0.25, inplace=False)\n",
       "    (dropout2): Dropout2d(p=0.5, inplace=False)\n",
       "    (fc1): Linear(in_features=9216, out_features=128, bias=True)\n",
       "    (fc2): Linear(in_features=128, out_features=10, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tloss = T_Loss().to(device)\n",
    "tloss.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "938"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ddcfc5a254024e948df98fc5dc2f4617",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=10.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/bvaa/lib/python3.6/site-packages/torch/nn/functional.py:2416: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.\n",
      "  warnings.warn(\"nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0\tLoss: 101.755137\tCorrect: 45149\n",
      "Train Epoch: 1\tLoss: 218.742583\tCorrect: 16975\n",
      "Train Epoch: 2\tLoss: 312.483392\tCorrect: 4249\n",
      "Train Epoch: 3\tLoss: 335.262390\tCorrect: 256\n",
      "Train Epoch: 4\tLoss: 192.567708\tCorrect: 8216\n",
      "Train Epoch: 5\tLoss: 145.945556\tCorrect: 21787\n",
      "Train Epoch: 6\tLoss: 155.799779\tCorrect: 10544\n",
      "Train Epoch: 7\tLoss: 162.216870\tCorrect: 5\n",
      "Train Epoch: 8\tLoss: 173.021897\tCorrect: 1\n",
      "Train Epoch: 9\tLoss: 201.857514\tCorrect: 1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "attack_log_interval = 1\n",
    "alt_target = 2\n",
    "translator.train()\n",
    "optimizer = optim.Adam(translator.parameters(), lr=1e-3)\n",
    "for epoch in tqdm(range(10)):\n",
    "    epoch_loss = 0\n",
    "    epoch_correct = 0\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data = torch.FloatTensor(data).to(device)\n",
    "        \n",
    "        _, l_dist, l_sample = generator_model(data)\n",
    "        \n",
    "        noised_sample = translator(l_sample)\n",
    "        \n",
    "        loss, correct = tloss(noised_sample, l_sample, alt_target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "#         print(correct)\n",
    "        epoch_loss += loss.item()\n",
    "        epoch_correct += correct\n",
    "        \n",
    "    if (epoch+1) % attack_log_interval == 0:\n",
    "        print('Train Epoch: {}\\tLoss: {:.6f}\\tCorrect: {}'.format(\n",
    "            epoch, epoch_loss/batch_idx, epoch_correct))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = enumerate(test_loader)\n",
    "batch_idx, (example_data, example_targets) = next(examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(i):\n",
    "    _, l_dist, l_sample = generator_model(example_data[i].unsqueeze_(0).to(device))\n",
    "    noised_sample = translator(l_sample)\n",
    "    print(noised_sample)\n",
    "    print(l_sample)\n",
    "    noised_sample = 2 * ((l_sample - l_sample.min())/(l_sample.max() - l_sample.min())) + 1e-3 * ((noised_sample - noised_sample.min())/(noised_sample.max() - noised_sample.min()))\n",
    "    final = generator_model.decoder(noised_sample)\n",
    "    pred = torch.argmax(classifier(F.upsample(final, (28,28), mode='bilinear', align_corners=True)))\n",
    "    print(\"Prediction: \", pred.item())\n",
    "    plt.figure(figsize=(10,10))\n",
    "    plt.subplot(1,2,1)\n",
    "    plt.imshow(example_data[i][0].detach().cpu().numpy(), cmap='gray', interpolation='none')\n",
    "    plt.subplot(1,2,2)\n",
    "    plt.imshow(final[0][0].detach().cpu().numpy(), cmap='gray', interpolation='none')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-341542.0000, -327714.2188, -337407.4375,   34821.4727,  -18526.1055,\n",
      "         -292513.2500, -313890.8125,  178696.0156,   16251.2051,   -2629.7827,\n",
      "         -331779.0938, -308876.1875]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward>)\n",
      "tensor([[ 0.5444,  0.9835,  0.4785,  0.0258, -0.8371, -0.0832, -0.7022,  2.0096,\n",
      "          0.6236, -0.0749,  0.7555, -0.0840]], device='cuda:0',\n",
      "       grad_fn=<AddBackward0>)\n",
      "Prediction:  2\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "test(11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-568883.8750, -546580.5625, -561910.6250,   60184.6016,  -30571.0684,\n",
      "         -485188.1250, -524854.6875,  307574.6562,   25205.1230,   -4981.4038,\n",
      "         -552835.5625, -515649.2500]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward>)\n",
      "tensor([[ 0.6024,  0.1721,  0.3362, -0.6550,  0.5297,  0.0613, -0.6694, -0.2037,\n",
      "          0.0037,  0.2079, -1.4497, -0.4065]], device='cuda:0',\n",
      "       grad_fn=<AddBackward0>)\n",
      "Prediction:  1\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAlAAAAEdCAYAAADDzFlqAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAAbfUlEQVR4nO3dfYzW9bnn8c/FMDM8SEEUgYJdQHApPiEBarQ5pVVPPI0pmFBT057YxIjZ2HSbnP3DdpPdnmb/8LTVtmk2buhK9Wy6rV1pA1mND7UY2JaIwgHBYilQVMqjPD8OMHPtH3OTTO38ru9874e5fzPzfiVkZu7P/Zu5+DH3NRe/uecac3cBAACg74Y1uwAAAICBhgEKAAAgEwMUAABAJgYoAACATAxQAAAAmRigAAAAMg2v5WAzu0fSjyS1SPqf7v544v7sTACGng/dfUKzi+hNTg+jfwFDUmH/qvoKlJm1SPrvkv5B0hxJD5jZnGrfH4BB671mF9AbehiAPijsX7V8C2+hpJ3uvtvdL0j6haTFNbw/AOhP9DAAVatlgJoi6YMeb++t3AYAAwE9DEDVankOlPVy2988R8DMlklaVsPHAYBGSPYw+heAIrUMUHslXdvj7amS9n30Tu6+XNJyiSdhAiiVZA+jfwEoUsu38N6UNMvMpptZm6QvSVpdn7IAoOHoYQCqVvUVKHe/ZGZfk/Syun8EeIW7v1O3ygCggehhAGph7v13VZpL4MCQtNHd5ze7iFrRv4AhqbB/sYkcAAAgEwMUAABAJgYoAACATAxQAAAAmRigAAAAMjFAAQAAZGKAAgAAyMQABQAAkIkBCgAAIBMDFAAAQCYGKAAAgEwMUAAAAJkYoAAAADIxQAEAAGRigAIAAMjEAAUAAJCJAQoAACATAxQAAEAmBigAAIBMDFAAAACZGKAAAAAyMUABAABkYoACAADIxAAFAACQiQEKAAAgEwMUAABAJgYoAACATAxQAAAAmRigAAAAMjFAAQAAZGKAAgAAyDS8loPNbI+kU5I6JV1y9/n1KAr5hg2LZ+Hx48eH+cSJE8O8s7MzzA8fPhzm58+fD/Nx48aF+ZgxY8J8+PCaPpWT2trawjw6f6ljU/bv3x/me/bsCfMjR46EeerfdjCjhwGoVj2+6nzW3T+sw/sBgGaghwHIxrfwAAAAMtU6QLmkV8xso5ktq0dBANCP6GEAqlLrt/DucPd9ZnaNpFfN7F13X9vzDpWmRGMCUEZhD6N/AShS0xUod99XeXlI0q8lLezlPsvdfT5PzgRQNqkeRv8CUKTqAcrMRpvZmMuvS/p7SdvqVRgANBI9DEAtavkW3kRJvzazy+/nf7v7S3WpCgAajx4GoGpVD1DuvlvSLXWsBYHUnqdJkyaF+Z133llT3tHREebr168P89QuotmzZ4f5zJkzw/yKK64I81ql9lDdcccdhdmVV14ZHlv5Al5ozZo1Yf7MM8+E+SuvvBLmBw4cCPPBih42eKQeQ42W6s9dXV1h7u71LOdvpM5PtEfv0qVL4bGNrr3MWGMAAACQiQEKAAAgEwMUAABAJgYoAACATAxQAAAAmRigAAAAMjFAAQAAZKr1d+Ghn4wdOzbMH3jggTB/+OGHw/z666/Prinn/aek9pQM5l0jqb/bokWLwjx17k6cOBHmq1atCnOg0WrZUyRJ48aNC/OWlpYwT+25S/Xf1GN43759YX7x4sUwr1Vra2uYjx8/vjA7fPhweGxnZ2dVNQ0GXIECAADIxAAFAACQiQEKAAAgEwMUAABAJgYoAACATAxQAAAAmVhjUBLt7e1h/sgjj4T5V7/61TC/7rrrckvCAJH6MeNUDgx0qR+lHz16dJin1iSk8gMHDoT5pUuXwrzRUmsYli5dWpj99re/DY/ds2dPmJ89ezbMBzKuQAEAAGRigAIAAMjEAAUAAJCJAQoAACATAxQAAEAmBigAAIBMDFAAAACZ2ANVEgsWLAjzu+++O8ynTZsW5i0tLbkl9auurq4wP3PmTJhfuHChnuVku3jxYmH20ksvhceePn06zI8cORLma9asCfMtW7aEOdBs7h7m0eNLko4dOxbmJ0+erOnjp/pT6vhmO3HiRJhPmTKlMHvyySfDY3/605+G+cqVK8O82TuyasEVKAAAgEwMUAAAAJkYoAAAADIxQAEAAGRigAIAAMjEAAUAAJCJAQoAACBTcg+Uma2QdK+kQ+5+Y+W28ZKekzRN0h5J97t7vIhjCDCzwmzMmDHhsY888kiYz507N8zb2trC/Pz582H+/vvvh/mbb74Z5ocOHQrzlI6OjjB/5513wjy1K6nRoj0xf/nLX8JjUztuUucm9XdP7dAa7Ohhg19qD9NA3jVUD6keE+2JWrRoUXjs6NGjwzy1By+1o6rM+nIF6hlJ93zktsckvebusyS9VnkbAMroGdHDANRZcoBy97WSjn7k5sWSnq28/qykJXWuCwDqgh4GoBGqfQ7URHffL0mVl9fUryQAaDh6GICaNPx34ZnZMknLGv1xAKDe6F8AilR7BeqgmU2WpMrLwmcQu/tyd5/v7vOr/FgAUG996mH0LwBFqh2gVkt6sPL6g5JW1accAOgX9DAANUkOUGb2c0nrJf17M9trZg9JelzS3Wb2J0l3V94GgNKhhwFohORzoNz9gYLozjrXMuC1tLQUZqk9TgsWLAjzcePGVVXTZTt37gzz5557LsxffvnlMD916lR2TT2l9rQcOHAgzIf6riMUo4dhqEvtydqyZUthtnfv3vDY559/Psxr/dpQZmwiBwAAyMQABQAAkIkBCgAAIBMDFAAAQCYGKAAAgEwMUAAAAJkYoAAAADI1/HfhDSXDhhXPozNmzAiPHTlyZJibWVU1XbZjx44wf+GFF8J827ZtYd7Z2Rnm0Y6svhyf2mMCAOhd6uvH+vXrC7OHHnooPDbaISVJXV1dYT6QcQUKAAAgEwMUAABAJgYoAACATAxQAAAAmRigAAAAMjFAAQAAZGKAAgAAyMQeqDqKdh2l9kC1t7fXu5y/Mnbs2DC/9dZbw3z69Olhfvjw4TCfMGFCmO/bty/MOzo6wvz8+fNhfuLEiTA/fvx4mJ87dy7MAaBRoh2DfclTu5jOnDlTmP3hD3+o+tjBjitQAAAAmRigAAAAMjFAAQAAZGKAAgAAyMQABQAAkIkBCgAAIBMDFAAAQCb2QNXR8OHFp/Ouu+4Kj/3Yxz5W73L+yp133llTntr1sXXr1jC/4YYbwvzNN98M85MnT4b5kSNHwjy1yyT18VN/v9SeKQAoEn3tkKRRo0aFeVtbW5in9uRFewivvPLK8Nhjx46F+WDGFSgAAIBMDFAAAACZGKAAAAAyMUABAABkYoACAADIxAAFAACQiQEKAAAgU3IPlJmtkHSvpEPufmPltm9LeljS4crdvuXuLzaqyIHC3Quz1B6Ozs7OepdTV6NHjw7z2267rab3n9pDFZ3bvujq6grzLVu2hPn3v//9MF+5cmVhduHChfBYNBY9DGYW5sOGxdcSWlpawjy1x6nW95/qf6m/3yc+8YkwX7BgQWG2a9eu8NhUPpj15QrUM5Lu6eX2H7j73MofGg+AsnpG9DAAdZYcoNx9raSj/VALANQdPQxAI9TyHKivmdnbZrbCzOJd7wBQPvQwAFWrdoB6StJ1kuZK2i/piaI7mtkyM3vLzN6q8mMBQL31qYfRvwAUqWqAcveD7t7p7l2SfiJpYXDf5e4+393nV1skANRTX3sY/QtAkaoGKDOb3OPN+yRtq085ANB49DAAterLGoOfS1ok6Woz2yvpv0paZGZzJbmkPZIeaWCNAFA1ehiARkgOUO7+QC83P92AWga8jo6OwuyJJwqfJiZJ+uEPfxjmM2bMCPPUHpChLrWH5cYbbwzzb37zm2F+8eLFwmz16tXhseyJaqzB0sNSu4ZSu+Rq3aVWZqnHd2qP3fXXXx/mx48fD/NU/03toUu9/7Nnz4Z5ao/UzJkzw/ymm24qzFI78lJ/t8GMTeQAAACZGKAAAAAyMUABAABkYoACAADIxAAFAACQiQEKAAAgEwMUAABApuQeKPTdpUuXCrO1a9eGxy5dujTMFy1aFOZTp04N80ZL7aDZtWtXmG/dujXMR44cGeaf/exnw3zx4sVhHu1BkaRJkyaF+ac//enC7IUXXgiPBSRp7ty5YX7LLbeE+euvvx7m77//fpgP5D1RqV1E58+fD/MzZ86E+f79+8M86v1Sur5UXuu/zZEjR8J83bp1hdmOHTvCYwfy502tuAIFAACQiQEKAAAgEwMUAABAJgYoAACATAxQAAAAmRigAAAAMjFAAQAAZGIPVB1F+zBOnToVHrtt27YwP3ToUJiPGjUqzBsttQsk9fc/duxYmLe0tIT57t27w7y1tTXMZ8+eHebt7e1hPmXKlMKsra0tPDa1o2Yo71kZTFKfw9/73vfC/Oqrrw7zmTNnhvl3v/vdME89Rgey1J6m1I6sc+fOhXmjH6NmFuapHpM6fvv27YVZR0dHeOxQxhUoAACATAxQAAAAmRigAAAAMjFAAQAAZGKAAgAAyMQABQAAkIkBCgAAIBN7oEqis7MzzPfv399PlZRTao/Le++9F+apPS9dXV1hPmLEiDCfM2dOYTZmzJjw2NOnT4d56nMDA0NqV8+MGTPCPPU5vnnz5jBP7RsbzFJ7mpq95ykltUNs1qxZYR7tqZPi/pjqjUMZV6AAAAAyMUABAABkYoACAADIxAAFAACQiQEKAAAgEwMUAABAJgYoAACATMk9UGZ2raR/lTRJUpek5e7+IzMbL+k5SdMk7ZF0v7sfa1ypzTdy5Mgwj/a4pHb57NmzJ8yH8g6XvkjtWho7dmyYDxsW/1+itbU1zCdNmlSYtbe3h8eaWZijemXqXzfccEOYjx49Osw//vGPh/nw4XE7b/Yuo6Es9RhP9Z9Uf0t9bhw7Fn9qHzx4sDDj86ZYX65AXZL0T+7+SUm3SXrUzOZIekzSa+4+S9JrlbcBoEzoXwAaIjlAuft+d99Uef2UpO2SpkhaLOnZyt2elbSkUUUCQDXoXwAaJes5UGY2TdKtkt6QNNHd90vdTUrSNfUuDgDqhf4FoJ76/LvwzOwKSSslfcPdT/b1eRtmtkzSsurKA4Da0b8A1FufrkCZWau6m8/P3P1XlZsPmtnkSj5Z0qHejnX35e4+393n16NgAMhB/wLQCMkByrr/q/a0pO3u/mSPaLWkByuvPyhpVf3LA4Dq0b8ANEpfvoV3h6R/lLTVzDZXbvuWpMcl/dLMHpL0vqQvNqZEAKga/QtAQyQHKHf/f5KKnjBwZ33Laa7UHpabbropzL/85S8XZpcuXQqPXb16dZhv2rQpzE+cOBHmA11qj9O8efPC/Oabbw7zlpaWML9w4UKY7927tzBL7fBiz0rj9Gf/Sj2v6vbbbw/zCRMmhPnEiRPDfOHChWH+4osvhvmpU6fCvMxSj99ad7GNGDEizNva2sJ81KhRYX7VVVeFeWqHWOr4LVu2hHnq6xN6xyZyAACATAxQAAAAmRigAAAAMjFAAQAAZGKAAgAAyMQABQAAkIkBCgAAIFOffxfeUDBp0qQw/+IX4117jz76aNUfO7Vj6vHHHw/z3/3ud2F+7ty57Jr6U2rHzac+9akwT/3bLFq0KMyHDYv/L5HakfOb3/ymMDt69Gh4bGdnZ5hjYEjt81q1Kl52vnjx4jC/5pr49x2ndvnMmTMnzHfv3l2YpfbMpfYopT7HU8ePHDkyzBcsWBDms2bNCvPJkyeH+Y033hjmZ86cCfM///nPYZ7qEak9UhcvXgzzsvf/gYorUAAAAJkYoAAAADIxQAEAAGRigAIAAMjEAAUAAJCJAQoAACATAxQAAEAm9kD1kNo1ktoTVYvPfOYzYf7BBx+E+enTp8N806ZNYZ7aIZPa09La2lpT/vWvfz3M77vvvjCfPn16mLe3t4d5ak/NoUOHwvzll18uzFLnFkPDe++9F+Zf+MIXwjy1C+iqq64K82nTpoX50qVLC7N33303PDa1B+nkyZNhPmLEiDBP7cmLapfSj//UHqbhw+MvlRs3bgzz1157LcxT5yfVn1L1pfoXu+iqwxUoAACATAxQAAAAmRigAAAAMjFAAQAAZGKAAgAAyMQABQAAkIk1Bj24e5g38sfRW1pawvz+++8P89SKhRUrVoT52rVrw3zChAlhfu+994b5kiVLwnz27NlhPnr06DBPnb+Ojo4w37p1a5g/9dRTYb5u3brC7OLFi+GxgJReBZDKDx8+HOZ//OMfs2vqq9Sak5TUj+EfPHgwzN9+++0wT5271I/5p3r/hQsXwrzWNQGpr01oDq5AAQAAZGKAAgAAyMQABQAAkIkBCgAAIBMDFAAAQCYGKAAAgEwMUAAAAJkstV/CzK6V9K+SJknqkrTc3X9kZt+W9LCky8tHvuXuLybeV6mXWaR2DS1cuDDMv/Od7xRm8+bNC48dMWJEmKek9pwcPXo0zE+dOhXmra2tYT527Nia8pEjR4Z5yrlz58L8pZdeCvMf//jHYb558+YwP378eGHGDhdtdPf5zfjAQ6l/DWa17plK4TGKQGH/6ssizUuS/sndN5nZGEkbzezVSvYDd/9+vaoEgDqjfwFoiOQA5e77Je2vvH7KzLZLmtLowgCgVvQvAI2S9RwoM5sm6VZJb1Ru+pqZvW1mK8zsyjrXBgB1Q/8CUE99HqDM7ApJKyV9w91PSnpK0nWS5qr7f3hPFBy3zMzeMrO36lAvAGSjfwGotz4NUGbWqu7m8zN3/5UkuftBd+909y5JP5HU6zOs3X25u89v1pNIAQxt9C8AjZAcoKz7xx+elrTd3Z/scfvkHne7T9K2+pcHANWjfwFolL78FN4dkv5R0lYzu/yz3N+S9ICZzZXkkvZIeqQhFQJA9ehfABoiuQeqrh+s5HtUUrtGxowZE+Y333xzYbZkyZLw2K985SthPmHChDBv9J6URjt48GCY79y5M8x///vfh3lqD9SGDRvCPLVnC6Gm7YGqp7L3LwANUdi/2EQOAACQiQEKAAAgEwMUAABAJgYoAACATAxQAAAAmRigAAAAMjFAAQAAZOrLIs0hI7UT6+TJk2G+fv36wuzDDz8Mjz1w4ECY33777WEe7aCSpClT4l9A397eHuadnZ1hfvTo0TDfsWNHmK9evTrMt2/fXtP737t3b5ifPXs2zAEA6IkrUAAAAJkYoAAAADIxQAEAAGRigAIAAMjEAAUAAJCJAQoAACATAxQAAEAm9kDVUbQr6d133w2PPXz4cJhv2LAhzFN7oKZOnRrmte6BOnLkSJin9jStW7eupvefqg8AgHriChQAAEAmBigAAIBMDFAAAACZGKAAAAAyMUABAABkYoACAADIxAAFAACQydy9/z6YWf99MABlsdHd5ze7iFrRv4AhqbB/cQUKAAAgEwMUAABAJgYoAACATAxQAAAAmRigAAAAMjFAAQAAZGKAAgAAyJQcoMxshJltMLMtZvaOmf1z5fbpZvaGmf3JzJ4zs7bGlwsAeehhABqhL1egOiR9zt1vkTRX0j1mdpukf5H0A3efJemYpIcaVyYAVI0eBqDukgOUdztdebO18sclfU7S85Xbn5W0pCEVAkAN6GEAGqFPz4EysxYz2yzpkKRXJe2SdNzdL1XuslfSlMaUCAC1oYcBqLc+DVDu3unucyVNlbRQ0id7u1tvx5rZMjN7y8zeqr5MAKhetT2M/gWgSNZP4bn7cUmvS7pN0jgzG16JpkraV3DMcnefPxh+mSiAgS23h9G/ABTpy0/hTTCzcZXXR0q6S9J2SWskLa3c7UFJqxpVJABUix4GoBGGp++iyZKeNbMWdQ9cv3T3/2tmf5D0CzP7b5L+TdLTDawTAKpFDwNQd+be61OXGvPBzPrvgwEoi42D4Vtg9C9gSCrsX2wiBwAAyMQABQAAkIkBCgAAIBMDFAAAQCYGKAAAgEwMUAAAAJkYoAAAADL1ZZFmPX0o6b0eb19dua2sylxfmWuTyl1fmWuTBl99/65RhfQz+ld9lbm+Mtcmlbu+Mtcm1bF/9esizb/54GZvlXnBXpnrK3NtUrnrK3NtEvUNFGU/D9RXvTLXJpW7vjLXJtW3Pr6FBwAAkIkBCgAAIFOzB6jlTf74KWWur8y1SeWur8y1SdQ3UJT9PFBf9cpcm1Tu+spcm1TH+pr6HCgAAICBqNlXoAAAAAacpgxQZnaPmf3RzHaa2WPNqCFiZnvMbKuZbTazt0pQzwozO2Rm23rcNt7MXjWzP1VeXlmy+r5tZn+pnMPNZvb5JtV2rZmtMbPtZvaOmf3Hyu1NP39BbWU5dyPMbIOZbanU98+V26eb2RuVc/ecmbU1o75moodl1UL/qr620vavRH1lOX+N7WHu3q9/JLVI2iVphqQ2SVskzenvOhI17pF0dbPr6FHP30maJ2lbj9u+K+mxyuuPSfqXktX3bUn/qQTnbrKkeZXXx0jaIWlOGc5fUFtZzp1JuqLyequkNyTdJumXkr5Uuf1/SPoPza61n88LPSyvFvpX9bWVtn8l6ivL+WtoD2vGFaiFkna6+253vyDpF5IWN6GOAcPd10o6+pGbF0t6tvL6s5KW9GtRPRTUVwruvt/dN1VePyVpu6QpKsH5C2orBe92uvJma+WPS/qcpOcrtzf1c69J6GEZ6F/VK3P/StRXCo3uYc0YoKZI+qDH23tVohNe4ZJeMbONZras2cUUmOju+6XuT2JJ1zS5nt58zczerlwib9ol+svMbJqkW9X9v5BSnb+P1CaV5NyZWYuZbZZ0SNKr6r7yctzdL1XuUsbHb6PRw2pXqsdfgVI8Bi8rc/+ShmYPa8YAZb3cVrYfBbzD3edJ+gdJj5rZ3zW7oAHoKUnXSZorab+kJ5pZjJldIWmlpG+4+8lm1vJRvdRWmnPn7p3uPlfSVHVfeflkb3fr36qajh42+JXmMSiVu39JQ7eHNWOA2ivp2h5vT5W0rwl1FHL3fZWXhyT9Wt0nvWwOmtlkSaq8PNTkev6Kux+sfOJ2SfqJmngOzaxV3Q/un7n7ryo3l+L89VZbmc7dZe5+XNLr6n7+wDgzu/x7NEv3+O0H9LDaleLxV6RMj8Ey96+i+sp0/i5rRA9rxgD1pqRZlWfBt0n6kqTVTaijV2Y22szGXH5d0t9L2hYf1RSrJT1Yef1BSauaWMvfuPzgrrhPTTqHZmaSnpa03d2f7BE1/fwV1VaiczfBzMZVXh8p6S51P8dhjaSllbuV7nOvH9DDatf0x1+kRI/B0vYviR7WrGfGf17dz9bfJek/N6OGoLYZ6v6pmi2S3ilDfZJ+ru7LoBfV/b/fhyRdJek1SX+qvBxfsvr+l6Stkt5W94N9cpNq+7S6L8++LWlz5c/ny3D+gtrKcu5ulvRvlTq2SfovldtnSNogaaek/yOpvVmfe836Qw/Lqof+VX1tpe1fifrKcv4a2sPYRA4AAJCJTeQAAACZGKAAAAAyMUABAABkYoACAADIxAAFAACQiQEKAAAgEwMUAABAJgYoAACATP8fHhGZWZGPVAcAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 720x720 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def test(i):\n",
    "    _, l_dist, l_sample = generator_model(example_data[i].unsqueeze_(0).to(device))\n",
    "    noised_sample = translator(l_sample)\n",
    "    print(noised_sample)\n",
    "    print(l_sample)\n",
    "    noised_sample = 2 * ((l_sample - l_sample.min())/(l_sample.max() - l_sample.min())) + 1e-15 * ((noised_sample - noised_sample.min())/(noised_sample.max() - noised_sample.min()))\n",
    "    final = generator_model.decoder(noised_sample)\n",
    "    pred = torch.argmax(classifier(F.upsample(final, (28,28), mode='bilinear', align_corners=True)))\n",
    "    print(\"Prediction: \", pred.item())\n",
    "    plt.figure(figsize=(10,10))\n",
    "    plt.subplot(1,2,1)\n",
    "    plt.imshow(example_data[i][0].detach().cpu().numpy(), cmap='gray', interpolation='none')\n",
    "    plt.subplot(1,2,2)\n",
    "    plt.imshow(final[0][0].detach().cpu().numpy(), cmap='gray', interpolation='none')\n",
    "\n",
    "\n",
    "test(22)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
