{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from torchvision.utils import save_image\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from timeit import default_timer\n",
    "from collections import defaultdict\n",
    "from tqdm import trange\n",
    "\n",
    "class Trainer():\n",
    "    def __init__(self, model, optimizer, loss_f, device, save_dir=\"results\", is_progress_bar=True):\n",
    "            self.device = device\n",
    "            self.model = model.to(device)\n",
    "            self.loss_f = loss_f\n",
    "            self.optimizer = optimizer\n",
    "            self.save_dir = save_dir\n",
    "            self.is_progress_bar = is_progress_bar\n",
    "            \n",
    "    def __call__(self, data_loader, epochs=10, checkpoint_every=10):\n",
    "        start = default_timer()\n",
    "        self.model.train()\n",
    "        for epoch in range(epochs):\n",
    "            storer = defaultdict(list)\n",
    "            mean_epoch_loss = self._train_epoch(data_loader, storer, epoch)\n",
    "            mean_epoch_loss = self._test_epoch(data_loader, storer, epoch)\n",
    "            with torch.no_grad():\n",
    "                sample = torch.randn(64, self.model.latent_dim).to(device)\n",
    "                sample = self.model.decoder(sample).cpu()  # make sure on cpu\n",
    "                save_image(sample.view(64, 1, 32, 32),\n",
    "                           './results/samples/' + str(epoch) + '.png')\n",
    "            \n",
    "    def _train_epoch(self, data_loader, storer, epoch):\n",
    "        epoch_loss = 0.\n",
    "        kwargs = dict(desc=\"Epoch {}\".format(epoch + 1), leave=False,\n",
    "                      disable=not self.is_progress_bar)\n",
    "        with trange(len(data_loader), **kwargs) as t:\n",
    "            for _, (data, _) in enumerate(data_loader):\n",
    "                iter_loss = self._train_iteration(data, storer)\n",
    "                epoch_loss += iter_loss\n",
    "                t.set_postfix(loss=iter_loss)\n",
    "                t.update()\n",
    "        mean_epoch_loss = epoch_loss / len(data_loader)\n",
    "        return mean_epoch_loss\n",
    "    \n",
    "    def _train_iteration(self, data, storer):\n",
    "        batch_size, channel, height, width = data.size()\n",
    "        data = data.to(self.device)\n",
    "        recon_batch, latent_dist, latent_sample = self.model(data)\n",
    "        loss = self.loss_f(data, recon_batch, latent_dist, self.model.training, \n",
    "                           storer, latent_sample=latent_sample)\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "            \n",
    "        return loss.item()\n",
    "    \n",
    "    def _test_epoch(self, data_loader, storer, epoch):\n",
    "        epoch_loss = 0.\n",
    "        kwargs = dict(desc=\"Epoch {}\".format(epoch + 1), leave=False,\n",
    "                      disable=not self.is_progress_bar)\n",
    "        with trange(len(data_loader), **kwargs) as t:\n",
    "            for _, (data, _) in enumerate(data_loader):\n",
    "                iter_loss = self._train_iteration(data, storer)\n",
    "                epoch_loss += iter_loss\n",
    "                t.set_postfix(loss=iter_loss)\n",
    "                t.update()\n",
    "        mean_epoch_loss = epoch_loss / len(data_loader)\n",
    "        return mean_epoch_loss\n",
    "    \n",
    "    def _test_iteration(self, data, storer):\n",
    "        batch_size, channel, height, width = data.size()\n",
    "        data = data.to(self.device)\n",
    "        recon_batch, latent_dist, latent_sample = self.model(data)\n",
    "        loss = self.loss_f(data, recon_batch, latent_dist, self.model.training, \n",
    "                               storer, latent_sample=latent_sample)\n",
    "            \n",
    "        return loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOSSES = [\"betaH\", \"betaB\"]\n",
    "RECON_DIST = [\"bernoulli\", \"laplace\", \"gaussian\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "mnist_dataset = datasets.MNIST('/home/data/bvaa', \n",
    "                   train=True, \n",
    "                   download=True, \n",
    "                   transform=transforms.Compose([\n",
    "                       transforms.Resize(32),\n",
    "                       transforms.ToTensor()\n",
    "                   ]))\n",
    "\n",
    "mnist_dataset_test = datasets.MNIST('/home/data/bvaa', train=False, download=True, transform=transforms.Compose([\n",
    "                       transforms.Resize(32),\n",
    "                       transforms.ToTensor()\n",
    "                   ]))\n",
    "\n",
    "train_loader = DataLoader(mnist_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "test_loader = DataLoader(mnist_dataset_test, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = enumerate(test_loader)\n",
    "batch_idx, (example_data, example_targets) = next(examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vae import VAE\n",
    "from encoder import Encoder\n",
    "from decoder import Decoder\n",
    "from losses import get_loss_fn\n",
    "from torch import optim\n",
    "\n",
    "latent_dim = 12\n",
    "img_size = [1,32,32]\n",
    "\n",
    "lr = 5e-4\n",
    "\n",
    "betaB_args = {\"rec_dist\": \"bernoulli\",\n",
    "              \"reg_anneal\": 10000, \n",
    "              \"betaH_B\": 4,\n",
    "              \"betaB_initC\": 0,\n",
    "              \"betaB_finC\": 25,\n",
    "              \"betaB_G\": 100\n",
    "             }\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "loss_f = get_loss_fn(\"betaB\", n_data=len(train_loader.dataset), device=device, **betaB_args)\n",
    "\n",
    "encoder = Encoder(img_size, latent_dim)\n",
    "decoder = Decoder(img_size, latent_dim)\n",
    "\n",
    "generator_model = VAE(img_size, latent_dim, encoder, decoder).to(device)\n",
    "optimizer = optim.Adam(generator_model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer = Trainer(generator_model, optimizer, loss_f, device, logger=logger,\\\n",
    "#                   save_dir=exp_dir, is_progress_bar=False)\n",
    "# epochs = 100\n",
    "# checkpoint_every = 10\n",
    "# trainer(train_loader, epochs=epochs, checkpoint_every=checkpoint_every)\n",
    "# torch.save(trainer.model.state_dict(), 'test.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generator_model.load_state_dict(torch.load('models/test.pt'))\n",
    "# generator_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Classifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Classifier, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, 3, 1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, 1)\n",
    "        self.dropout1 = nn.Dropout2d(0.25)\n",
    "        self.dropout2 = nn.Dropout2d(0.5)\n",
    "        self.fc1 = nn.Linear(9216, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = self.dropout1(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.fc2(x)\n",
    "        output = F.softmax(x, dim=1)\n",
    "        return output\n",
    "\n",
    "classifier = Classifier()\n",
    "classifier.load_state_dict(torch.load('models/mnist_cnn_non_log.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AddNoise():\n",
    "    def __init__(self, shape, device, noise=None, percent_noise=0.1):\n",
    "        self.device = device\n",
    "        self.percent_noise = percent_noise\n",
    "        if noise is None:\n",
    "            self.noise = torch.randn(shape)\n",
    "        else:\n",
    "            self.noise = noise\n",
    "\n",
    "    def noisy(self, data):\n",
    "        x = self.noise.to(self.device) * self.percent_noise + data * (1 - self.percent_noise)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Noise for Testing: \n",
      " tensor([[ 0.7344,  0.0049,  0.1108, -0.9742,  0.7806,  0.4000, -0.6167, -1.0133,\n",
      "          0.4483, -0.2564, -0.7429,  0.7341]])\n"
     ]
    }
   ],
   "source": [
    "sample_noise = torch.randn(torch.Size([1, 12]))\n",
    "backup_sample_noise = sample_noise\n",
    "print(\"Sample Noise for Testing: \\n\", sample_noise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Decoder(\n",
       "  (lin1): Linear(in_features=12, out_features=256, bias=True)\n",
       "  (lin2): Linear(in_features=256, out_features=256, bias=True)\n",
       "  (lin3): Linear(in_features=256, out_features=512, bias=True)\n",
       "  (convT1): ConvTranspose2d(32, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "  (convT2): ConvTranspose2d(32, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "  (convT3): ConvTranspose2d(32, 1, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder = generator_model.decoder\n",
    "decoder.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Classifier(\n",
       "  (conv1): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (dropout1): Dropout2d(p=0.25, inplace=False)\n",
       "  (dropout2): Dropout2d(p=0.5, inplace=False)\n",
       "  (fc1): Linear(in_features=9216, out_features=128, bias=True)\n",
       "  (fc2): Linear(in_features=128, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import exp\n",
    "\n",
    "def gaussian(window_size, sigma):\n",
    "    gauss = torch.Tensor([exp(-(x - window_size//2)**2/float(2*sigma**2)) for x in range(window_size)])\n",
    "    return gauss/gauss.sum()\n",
    "\n",
    "def create_window(window_size, channel):\n",
    "    _1D_window = gaussian(window_size, 1.5).unsqueeze(1)\n",
    "    _2D_window = _1D_window.mm(_1D_window.t()).float().unsqueeze(0).unsqueeze(0)\n",
    "    window = torch.Tensor(_2D_window.expand(channel, 1, window_size, window_size).contiguous())\n",
    "    return window\n",
    "\n",
    "def _ssim(img1, img2, window, window_size, channel, size_average = True):\n",
    "    mu1 = F.conv2d(img1, window, padding = window_size//2, groups = channel)\n",
    "    mu2 = F.conv2d(img2, window, padding = window_size//2, groups = channel)\n",
    "\n",
    "    mu1_sq = mu1.pow(2)\n",
    "    mu2_sq = mu2.pow(2)\n",
    "    mu1_mu2 = mu1*mu2\n",
    "\n",
    "    sigma1_sq = F.conv2d(img1*img1, window, padding = window_size//2, groups = channel) - mu1_sq\n",
    "    sigma2_sq = F.conv2d(img2*img2, window, padding = window_size//2, groups = channel) - mu2_sq\n",
    "    sigma12 = F.conv2d(img1*img2, window, padding = window_size//2, groups = channel) - mu1_mu2\n",
    "\n",
    "    C1 = 0.01**2\n",
    "    C2 = 0.03**2\n",
    "\n",
    "    ssim_map = ((2*mu1_mu2 + C1)*(2*sigma12 + C2))/((mu1_sq + mu2_sq + C1)*(sigma1_sq + sigma2_sq + C2))\n",
    "\n",
    "    if size_average:\n",
    "        return ssim_map.mean()\n",
    "    else:\n",
    "        return ssim_map.mean(1).mean(1).mean(1)\n",
    "\n",
    "class SSIM(nn.Module):\n",
    "    def __init__(self, window_size = 11, size_average = True):\n",
    "        super(SSIM, self).__init__()\n",
    "        self.window_size = window_size\n",
    "        self.size_average = size_average\n",
    "        self.channel = 1\n",
    "        self.window = create_window(window_size, self.channel)\n",
    "\n",
    "    def forward(self, img1, img2):\n",
    "        (_, channel, _, _) = img1.size()\n",
    "\n",
    "        if channel == self.channel and self.window.data.type() == img1.data.type():\n",
    "            window = self.window\n",
    "        else:\n",
    "            window = create_window(self.window_size, channel)\n",
    "            \n",
    "            if img1.is_cuda:\n",
    "                window = window.cuda(img1.get_device())\n",
    "            window = window.type_as(img1)\n",
    "            \n",
    "            self.window = window\n",
    "            self.channel = channel\n",
    "\n",
    "\n",
    "        return _ssim(img1, img2, window, self.window_size, channel, self.size_average)\n",
    "\n",
    "def ssim(img1, img2, window_size = 11, size_average = True):\n",
    "    (_, channel, _, _) = img1.size()\n",
    "    window = create_window(window_size, channel)\n",
    "    \n",
    "    if img1.is_cuda:\n",
    "        window = window.cuda(img1.get_device())\n",
    "    window = window.type_as(img1)\n",
    "    \n",
    "    return _ssim(img1, img2, window, window_size, channel, size_average)\n",
    "\n",
    "ssim_loss = SSIM(window_size = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constrained Translator\n",
    "class Translator(nn.Module):\n",
    "    def __init__(self, hidden_layers=[5, 10, 5], latent_dim=latent_dim):\n",
    "        super(Translator, self).__init__()\n",
    "        self.hidden_layers = hidden_layers\n",
    "        self.hidden_layers.insert(0, latent_dim)\n",
    "        self.hidden_layers.append(latent_dim)\n",
    "        self.layers = []\n",
    "        \n",
    "        for i in range(len(self.hidden_layers)-1):\n",
    "            self.layers.append(nn.Linear(self.hidden_layers[i], self.hidden_layers[i+1]))\n",
    "        \n",
    "        self.layers = nn.ModuleList(self.layers)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        l_sample = x\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        noised_sample = x\n",
    "        noised_sample = 2 * ((l_sample - l_sample.min())/(l_sample.max() - l_sample.min())) + 1e-3 * ((noised_sample - noised_sample.min())/(noised_sample.max() - noised_sample.min()))\n",
    "        return noised_sample\n",
    "\n",
    "translator = Translator().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from skimage.metrics import structural_similarity as ssim\n",
    "\n",
    "def create_logits(target_label, pred, confidence=0.5, device=device):\n",
    "    logits = torch.zeros(pred.shape, dtype=torch.float64)\n",
    "    pred_labels = [int(element.item()) for element in torch.argmax(pred, dim=1)]\n",
    "#     print(\"length\", len(pred_labels))\n",
    "#     print(\"pred_lables:\",pred_labels)\n",
    "#     print(logits.shape)\n",
    "    logits[range(logits.shape[0]), pred_labels] = torch.DoubleTensor([1-confidence]*pred.shape[0])\n",
    "    logits[range(logits.shape[0]), [target_label]*pred.shape[0]] += torch.DoubleTensor([confidence]*pred.shape[0])\n",
    "    return logits.to(device)\n",
    "\n",
    "def structural(org_image, noised_image):\n",
    "    batch_size, channels, width, height = org_image.shape\n",
    "    loss1 = 0\n",
    "    for b_ in range(batch_size):\n",
    "        ch_loss = 0\n",
    "        for ch_ in range(channels):\n",
    "            ch_loss += 1-ssim(org_image[b_][ch_].detach().cpu().numpy(), noised_image[b_][ch_].detach().cpu().numpy())\n",
    "        loss1 += ch_loss/channels\n",
    "    return loss1\n",
    "            \n",
    "class T_Loss(nn.Module):\n",
    "    def __init__(self, decoder=decoder, classifier=classifier):\n",
    "        super(T_Loss, self).__init__()\n",
    "        self.decoder = decoder\n",
    "        self.classifier = classifier\n",
    "    def forward(self, x, org_x, target_label):\n",
    "        \n",
    "        org_image = self.decoder(org_x)\n",
    "        noised_image = self.decoder(x)\n",
    "        batch_size, channels, width, height = org_image.shape\n",
    "        loss1 = ssim_loss(org_image, noised_image)\n",
    "#         print(type(loss1))\n",
    "#         print(loss1)\n",
    "#         loss1 = torch.Tensor(loss1 / batch_size).to(device)\n",
    "#         print(\"loss1:\", loss1)\n",
    "        preds = self.classifier(F.upsample(noised_image, (28,28), mode='bilinear', align_corners=True))\n",
    "#         print(\"preds:\",preds)\n",
    "        target = create_logits(target_label, preds)\n",
    "#         print(target)\n",
    "        loss2 = nn.BCELoss(reduction='sum')(preds, target.float())\n",
    "#         print(type(loss2))\n",
    "#         print(\"loss1:\",100*(1-loss1))\n",
    "#         print(\"loss2:\",loss2)\n",
    "        loss = 100*(1-loss1) + loss2\n",
    "        \n",
    "        \n",
    "        out_labels = preds.argmax(dim=1, keepdim=True)\n",
    "#         print(out_labels)\n",
    "#         print(torch.empty(out_labels.shape).fill_(target_label))\n",
    "#         print(preds)\n",
    "#         correct = out_labels.eq(torch.Tensor([target_label]*out_labels.shape[0]).to(device)).sum()\n",
    "        correct = out_labels.eq(torch.empty(out_labels.shape).fill_(target_label).to(device)).sum()\n",
    "#         print(out_labels.shape)\n",
    "#         print(correct)\n",
    "#         print(torch.Tensor([target_label]*out_labels.shape[0]))\n",
    "#         print(out_labels)\n",
    "        return loss, correct.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "T_Loss(\n",
       "  (decoder): Decoder(\n",
       "    (lin1): Linear(in_features=12, out_features=256, bias=True)\n",
       "    (lin2): Linear(in_features=256, out_features=256, bias=True)\n",
       "    (lin3): Linear(in_features=256, out_features=512, bias=True)\n",
       "    (convT1): ConvTranspose2d(32, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (convT2): ConvTranspose2d(32, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (convT3): ConvTranspose2d(32, 1, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "  )\n",
       "  (classifier): Classifier(\n",
       "    (conv1): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (dropout1): Dropout2d(p=0.25, inplace=False)\n",
       "    (dropout2): Dropout2d(p=0.5, inplace=False)\n",
       "    (fc1): Linear(in_features=9216, out_features=128, bias=True)\n",
       "    (fc2): Linear(in_features=128, out_features=10, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tloss = T_Loss().to(device)\n",
    "tloss.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "938"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05de9c4fb6e845528c1e59ea063cf912",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=10.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/bvaa/lib/python3.6/site-packages/torch/nn/functional.py:2416: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.\n",
      "  warnings.warn(\"nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0\tLoss: 144.599295\tCorrect: 41605\n",
      "Train Epoch: 1\tLoss: 144.584532\tCorrect: 41570\n",
      "Train Epoch: 2\tLoss: 143.940578\tCorrect: 41744\n",
      "Train Epoch: 3\tLoss: 143.776217\tCorrect: 41891\n",
      "Train Epoch: 4\tLoss: 145.291839\tCorrect: 41155\n",
      "Train Epoch: 5\tLoss: 143.919593\tCorrect: 41980\n",
      "Train Epoch: 6\tLoss: 144.050595\tCorrect: 41762\n",
      "Train Epoch: 7\tLoss: 143.569306\tCorrect: 42109\n",
      "Train Epoch: 8\tLoss: 145.972787\tCorrect: 40866\n",
      "Train Epoch: 9\tLoss: 145.148130\tCorrect: 41166\n",
      "\n"
     ]
    }
   ],
   "source": [
    "attack_log_interval = 1\n",
    "alt_target = 2\n",
    "translator.train()\n",
    "optimizer = optim.Adam(translator.parameters(), lr=1e-3)\n",
    "for epoch in tqdm(range(10)):\n",
    "    epoch_loss = 0\n",
    "    epoch_correct = 0\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data = torch.FloatTensor(data).to(device)\n",
    "        \n",
    "        _, l_dist, l_sample = generator_model(data)\n",
    "        \n",
    "        noised_sample = translator(l_sample)\n",
    "        loss, correct = tloss(noised_sample, l_sample, alt_target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "#         print(correct)\n",
    "        epoch_loss += loss.item()\n",
    "        epoch_correct += correct\n",
    "        \n",
    "    if (epoch+1) % attack_log_interval == 0:\n",
    "        print('Train Epoch: {}\\tLoss: {:.6f}\\tCorrect: {}'.format(\n",
    "            epoch, epoch_loss/batch_idx, epoch_correct))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = enumerate(test_loader)\n",
    "batch_idx, (example_data, example_targets) = next(examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(i):\n",
    "    _, l_dist, l_sample = generator_model(example_data[i].unsqueeze_(0).to(device))\n",
    "    noised_sample = translator(l_sample)\n",
    "    print(noised_sample)\n",
    "    print(l_sample)\n",
    "    noised_sample = 2 * ((l_sample - l_sample.min())/(l_sample.max() - l_sample.min())) + 1e-2 * ((noised_sample - noised_sample.min())/(noised_sample.max() - noised_sample.min()))\n",
    "    final = generator_model.decoder(noised_sample)\n",
    "    pred = torch.argmax(classifier(F.upsample(final, (28,28), mode='bilinear', align_corners=True)))\n",
    "    print(\"Prediction: \", pred.item())\n",
    "    plt.figure(figsize=(10,10))\n",
    "    plt.subplot(1,2,1)\n",
    "    plt.imshow(example_data[i][0].detach().cpu().numpy(), cmap='gray', interpolation='none')\n",
    "    plt.subplot(1,2,2)\n",
    "    plt.imshow(final[0][0].detach().cpu().numpy(), cmap='gray', interpolation='none')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2.0010e+00, 4.1188e-01, 9.9816e-04, 1.0778e+00, 9.5563e-01, 1.2962e+00,\n",
      "         1.0209e+00, 6.7881e-01, 8.3330e-01, 1.1575e+00, 2.8102e-01, 1.2111e+00]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor([[ 0.6506, -0.9151, -1.3199, -0.2590, -0.3784, -0.0428, -0.3141, -0.6520,\n",
      "         -0.4999, -0.1795, -1.0440, -0.1276]], device='cuda:0',\n",
      "       grad_fn=<AddBackward0>)\n",
      "Prediction:  0\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "test(11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2.8493e-01, 2.0010e+00, 9.8961e-01, 1.0745e+00, 1.0328e+00, 1.4169e+00,\n",
      "         9.9004e-01, 1.5516e+00, 1.1337e+00, 9.1041e-01, 9.9875e-04, 1.7488e-01]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor([[-1.3855,  1.3464, -0.2637, -0.1286, -0.1934,  0.4182, -0.2616,  0.6310,\n",
      "         -0.0344, -0.3883, -1.8375, -1.5606]], device='cuda:0',\n",
      "       grad_fn=<AddBackward0>)\n",
      "Prediction:  2\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "test(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(translator.state_dict(), 'translator/approach5-for-2.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
