{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from torchvision.utils import save_image\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from timeit import default_timer\n",
    "from collections import defaultdict\n",
    "from tqdm import trange\n",
    "\n",
    "class Trainer():\n",
    "    def __init__(self, model, optimizer, loss_f, device, save_dir=\"results\", is_progress_bar=True):\n",
    "            self.device = device\n",
    "            self.model = model.to(device)\n",
    "            self.loss_f = loss_f\n",
    "            self.optimizer = optimizer\n",
    "            self.save_dir = save_dir\n",
    "            self.is_progress_bar = is_progress_bar\n",
    "            \n",
    "    def __call__(self, data_loader, epochs=10, checkpoint_every=10):\n",
    "        start = default_timer()\n",
    "        self.model.train()\n",
    "        for epoch in range(epochs):\n",
    "            storer = defaultdict(list)\n",
    "            mean_epoch_loss = self._train_epoch(data_loader, storer, epoch)\n",
    "            mean_epoch_loss = self._test_epoch(data_loader, storer, epoch)\n",
    "            with torch.no_grad():\n",
    "                sample = torch.randn(64, self.model.latent_dim).to(device)\n",
    "                sample = self.model.decoder(sample).cpu()  # make sure on cpu\n",
    "                save_image(sample.view(64, 1, 32, 32),\n",
    "                           './results/samples/' + str(epoch) + '.png')\n",
    "            \n",
    "    def _train_epoch(self, data_loader, storer, epoch):\n",
    "        epoch_loss = 0.\n",
    "        kwargs = dict(desc=\"Epoch {}\".format(epoch + 1), leave=False,\n",
    "                      disable=not self.is_progress_bar)\n",
    "        with trange(len(data_loader), **kwargs) as t:\n",
    "            for _, (data, _) in enumerate(data_loader):\n",
    "                iter_loss = self._train_iteration(data, storer)\n",
    "                epoch_loss += iter_loss\n",
    "                t.set_postfix(loss=iter_loss)\n",
    "                t.update()\n",
    "        mean_epoch_loss = epoch_loss / len(data_loader)\n",
    "        return mean_epoch_loss\n",
    "    \n",
    "    def _train_iteration(self, data, storer):\n",
    "        batch_size, channel, height, width = data.size()\n",
    "        data = data.to(self.device)\n",
    "        recon_batch, latent_dist, latent_sample = self.model(data)\n",
    "        loss = self.loss_f(data, recon_batch, latent_dist, self.model.training, \n",
    "                           storer, latent_sample=latent_sample)\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "            \n",
    "        return loss.item()\n",
    "    \n",
    "    def _test_epoch(self, data_loader, storer, epoch):\n",
    "        epoch_loss = 0.\n",
    "        kwargs = dict(desc=\"Epoch {}\".format(epoch + 1), leave=False,\n",
    "                      disable=not self.is_progress_bar)\n",
    "        with trange(len(data_loader), **kwargs) as t:\n",
    "            for _, (data, _) in enumerate(data_loader):\n",
    "                iter_loss = self._train_iteration(data, storer)\n",
    "                epoch_loss += iter_loss\n",
    "                t.set_postfix(loss=iter_loss)\n",
    "                t.update()\n",
    "        mean_epoch_loss = epoch_loss / len(data_loader)\n",
    "        return mean_epoch_loss\n",
    "    \n",
    "    def _test_iteration(self, data, storer):\n",
    "        batch_size, channel, height, width = data.size()\n",
    "        data = data.to(self.device)\n",
    "        recon_batch, latent_dist, latent_sample = self.model(data)\n",
    "        loss = self.loss_f(data, recon_batch, latent_dist, self.model.training, \n",
    "                               storer, latent_sample=latent_sample)\n",
    "            \n",
    "        return loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOSSES = [\"betaH\", \"betaB\"]\n",
    "RECON_DIST = [\"bernoulli\", \"laplace\", \"gaussian\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "mnist_dataset = datasets.MNIST('/home/data/bvaa', \n",
    "                   train=True, \n",
    "                   download=True, \n",
    "                   transform=transforms.Compose([\n",
    "                       transforms.Resize(32),\n",
    "                       transforms.ToTensor()\n",
    "                   ]))\n",
    "\n",
    "mnist_dataset_test = datasets.MNIST('/home/data/bvaa', train=False, download=True, transform=transforms.Compose([\n",
    "                       transforms.Resize(32),\n",
    "                       transforms.ToTensor()\n",
    "                   ]))\n",
    "\n",
    "train_loader = DataLoader(mnist_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "test_loader = DataLoader(mnist_dataset_test, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = enumerate(test_loader)\n",
    "batch_idx, (example_data, example_targets) = next(examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vae import VAE\n",
    "from encoder import Encoder\n",
    "from decoder import Decoder\n",
    "from losses import get_loss_fn\n",
    "from torch import optim\n",
    "\n",
    "latent_dim = 12\n",
    "img_size = [1,32,32]\n",
    "\n",
    "lr = 5e-4\n",
    "\n",
    "betaB_args = {\"rec_dist\": \"bernoulli\",\n",
    "              \"reg_anneal\": 10000, \n",
    "              \"betaH_B\": 4,\n",
    "              \"betaB_initC\": 0,\n",
    "              \"betaB_finC\": 25,\n",
    "              \"betaB_G\": 100\n",
    "             }\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "loss_f = get_loss_fn(\"betaB\", n_data=len(train_loader.dataset), device=device, **betaB_args)\n",
    "\n",
    "encoder = Encoder(img_size, latent_dim)\n",
    "decoder = Decoder(img_size, latent_dim)\n",
    "\n",
    "generator_model = VAE(img_size, latent_dim, encoder, decoder).to(device)\n",
    "optimizer = optim.Adam(generator_model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer = Trainer(generator_model, optimizer, loss_f, device, logger=logger,\\\n",
    "#                   save_dir=exp_dir, is_progress_bar=False)\n",
    "# epochs = 100\n",
    "# checkpoint_every = 10\n",
    "# trainer(train_loader, epochs=epochs, checkpoint_every=checkpoint_every)\n",
    "# torch.save(trainer.model.state_dict(), 'test.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generator_model.load_state_dict(torch.load('models/test.pt'))\n",
    "# generator_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Classifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Classifier, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, 3, 1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, 1)\n",
    "        self.dropout1 = nn.Dropout2d(0.25)\n",
    "        self.dropout2 = nn.Dropout2d(0.5)\n",
    "        self.fc1 = nn.Linear(9216, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = self.dropout1(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.fc2(x)\n",
    "        output = F.softmax(x, dim=1)\n",
    "        return output\n",
    "\n",
    "classifier = Classifier()\n",
    "classifier.load_state_dict(torch.load('models/mnist_cnn_non_log.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AddNoise():\n",
    "    def __init__(self, shape, device, noise=None, percent_noise=0.1):\n",
    "        self.device = device\n",
    "        self.percent_noise = percent_noise\n",
    "        if noise is None:\n",
    "            self.noise = torch.randn(shape)\n",
    "        else:\n",
    "            self.noise = noise\n",
    "\n",
    "    def noisy(self, data):\n",
    "        x = self.noise.to(self.device) * self.percent_noise + data * (1 - self.percent_noise)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Noise for Testing: \n",
      " tensor([[ 0.7344,  0.0049,  0.1108, -0.9742,  0.7806,  0.4000, -0.6167, -1.0133,\n",
      "          0.4483, -0.2564, -0.7429,  0.7341]])\n"
     ]
    }
   ],
   "source": [
    "sample_noise = torch.randn(torch.Size([1, 12]))\n",
    "backup_sample_noise = sample_noise\n",
    "print(\"Sample Noise for Testing: \\n\", sample_noise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Decoder(\n",
       "  (lin1): Linear(in_features=12, out_features=256, bias=True)\n",
       "  (lin2): Linear(in_features=256, out_features=256, bias=True)\n",
       "  (lin3): Linear(in_features=256, out_features=512, bias=True)\n",
       "  (convT1): ConvTranspose2d(32, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "  (convT2): ConvTranspose2d(32, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "  (convT3): ConvTranspose2d(32, 1, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder = generator_model.decoder\n",
    "decoder.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Classifier(\n",
       "  (conv1): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (dropout1): Dropout2d(p=0.25, inplace=False)\n",
       "  (dropout2): Dropout2d(p=0.5, inplace=False)\n",
       "  (fc1): Linear(in_features=9216, out_features=128, bias=True)\n",
       "  (fc2): Linear(in_features=128, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constrained Translator\n",
    "class Translator(nn.Module):\n",
    "    def __init__(self, hidden_layers=[10], latent_dim=latent_dim):\n",
    "        super(Translator, self).__init__()\n",
    "        self.hidden_layers = hidden_layers\n",
    "        self.hidden_layers.insert(0, latent_dim)\n",
    "        self.hidden_layers.append(latent_dim)\n",
    "        self.layers = []\n",
    "        \n",
    "        for i in range(len(self.hidden_layers)-1):\n",
    "            self.layers.append(nn.Linear(self.hidden_layers[i], self.hidden_layers[i+1]))\n",
    "        \n",
    "        self.layers = nn.ModuleList(self.layers)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "translator = Translator().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.0, 0.4, 1.0, 0.0, 0.4, 0.0, 0.0, 0.4, 0.4, 0.0]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[0.0000, 0.4000, 1.0000, 0.0000, 0.4000, 0.0000, 0.0000, 0.4000, 0.4000,\n",
    "         0.0000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.metrics import structural_similarity as ssim\n",
    "\n",
    "def create_logits(target_label, pred, confidence=0.4, device=device):\n",
    "    logits = torch.zeros(pred.shape, dtype=torch.float64)\n",
    "    pred_labels = [int(element.item()) for element in torch.argmax(pred, dim=1)]\n",
    "#     print(\"length\", len(pred_labels))\n",
    "#     print(\"pred_lables:\",pred_labels)\n",
    "#     print(logits.shape)\n",
    "    logits[range(logits.shape[0]), pred_labels] = torch.DoubleTensor([1-confidence]*pred.shape[0])\n",
    "    logits[range(logits.shape[0]), [target_label]*pred.shape[0]] += torch.DoubleTensor([confidence]*pred.shape[0])\n",
    "    return logits.to(device)\n",
    "\n",
    "class T_Loss(nn.Module):\n",
    "    def __init__(self, decoder=decoder, classifier=classifier):\n",
    "        super(T_Loss, self).__init__()\n",
    "        self.decoder = decoder\n",
    "        self.classifier = classifier\n",
    "    def forward(self, x, org_x, target_label):\n",
    "        \n",
    "        org_image = self.decoder(org_x)\n",
    "        noised_image = self.decoder(x)\n",
    "        \n",
    "        loss1 = nn.MSELoss()(noised_image, org_image)\n",
    "        loss1 = ssim(noised_image, org_image)\n",
    "        preds = self.classifier(F.upsample(noised_image, (28,28), mode='bilinear', align_corners=True))\n",
    "#         print(\"preds:\",preds)\n",
    "        target = create_logits(target_label, preds)\n",
    "#         print(target)\n",
    "        loss2 = nn.BCELoss(reduction='sum')(preds, target.float())\n",
    "        \n",
    "        loss = torch.sqrt(loss1 + loss2)\n",
    "        \n",
    "        \n",
    "        out_labels = preds.argmax(dim=1, keepdim=True)\n",
    "#         print(out_labels)\n",
    "#         print(torch.empty(out_labels.shape).fill_(target_label))\n",
    "#         print(preds)\n",
    "#         correct = out_labels.eq(torch.Tensor([target_label]*out_labels.shape[0]).to(device)).sum()\n",
    "        correct = out_labels.eq(torch.empty(out_labels.shape).fill_(target_label).to(device)).sum()\n",
    "#         print(out_labels.shape)\n",
    "#         print(correct)\n",
    "#         print(torch.Tensor([target_label]*out_labels.shape[0]))\n",
    "#         print(out_labels)\n",
    "        return loss, correct.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "T_Loss(\n",
       "  (decoder): Decoder(\n",
       "    (lin1): Linear(in_features=12, out_features=256, bias=True)\n",
       "    (lin2): Linear(in_features=256, out_features=256, bias=True)\n",
       "    (lin3): Linear(in_features=256, out_features=512, bias=True)\n",
       "    (convT1): ConvTranspose2d(32, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (convT2): ConvTranspose2d(32, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (convT3): ConvTranspose2d(32, 1, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "  )\n",
       "  (classifier): Classifier(\n",
       "    (conv1): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (dropout1): Dropout2d(p=0.25, inplace=False)\n",
       "    (dropout2): Dropout2d(p=0.5, inplace=False)\n",
       "    (fc1): Linear(in_features=9216, out_features=128, bias=True)\n",
       "    (fc2): Linear(in_features=128, out_features=10, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tloss = T_Loss().to(device)\n",
    "tloss.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "938"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9e5767118c84bf78ea02abd2f6fb007",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/bvaa/lib/python3.6/site-packages/torch/nn/functional.py:2416: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.\n",
      "  warnings.warn(\"nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0\tLoss: 4159.838549\tCorrect: 48669\n",
      "\n"
     ]
    }
   ],
   "source": [
    "attack_log_interval = 1\n",
    "alt_target = 2\n",
    "translator.train()\n",
    "optimizer = optim.Adam(translator.parameters(), lr=1e-3)\n",
    "for epoch in tqdm(range(1)):\n",
    "    epoch_loss = 0\n",
    "    epoch_correct = 0\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data = torch.FloatTensor(data).to(device)\n",
    "        \n",
    "        _, l_dist, l_sample = generator_model(data)\n",
    "        \n",
    "        noised_sample = translator(l_sample)\n",
    "        \n",
    "        loss, correct = tloss(noised_sample, l_sample, alt_target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "#         print(correct)\n",
    "        epoch_loss += loss.item()\n",
    "        epoch_correct += correct\n",
    "        \n",
    "    if (epoch+1) % attack_log_interval == 0:\n",
    "        print('Train Epoch: {}\\tLoss: {:.6f}\\tCorrect: {}'.format(\n",
    "            epoch, epoch_loss, epoch_correct))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = enumerate(test_loader)\n",
    "batch_idx, (example_data, example_targets) = next(examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(5)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_targets[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f1d7836c588>"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(example_data[i][0].detach().cpu().numpy(), cmap='gray', interpolation='none')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, l_dist, l_sample = generator_model(example_data[i].unsqueeze_(0).to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "noised_sample = translator(l_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.7019,  1.4366,  0.4129, -0.4780,  0.3363, -0.0516, -0.2690, -0.4795,\n",
       "         -0.2782,  0.1440,  0.6383,  1.5425]], device='cuda:0',\n",
       "       grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ -8.6915,  -9.6417, -14.7350,   8.0693,   4.6963,  -8.9189,  -8.1040,\n",
       "           5.4865,   5.3854,   4.2186, -10.0787,  -5.7107]], device='cuda:0',\n",
       "       grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "noised_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "final = generator_model.decoder(noised_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 32, 32])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 1.9376e-22,\n",
       "           1.6062e-25, 1.8362e-23],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 6.7117e-32,\n",
       "           7.7072e-37, 2.6800e-35],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 4.1685e-36,\n",
       "           0.0000e+00, 0.0000e+00],\n",
       "          ...,\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
       "           0.0000e+00, 1.3659e-38],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
       "           0.0000e+00, 1.3002e-35],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
       "           0.0000e+00, 1.5645e-26]]]], device='cuda:0',\n",
       "       grad_fn=<SigmoidBackward>)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f1d5e3b0b00>"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(final[0][0].detach().cpu().numpy(), cmap='gray', interpolation='none')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1, device='cuda:0')"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.argmax(classifier(F.upsample(final, (28,28), mode='bilinear', align_corners=True)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# recon_img = generator_model.decoder(0.005*sample_noise.cuda())\n",
    "# pred = classifier(F.upsample(recon_img, (28,28), mode='bilinear', align_corners=True).cpu())\n",
    "# print(torch.argmax(pred))\n",
    "# plt.imshow(recon_img[0][0].detach().cpu().numpy(), cmap='gray', interpolation='none')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def evaluate(i, percentage, sample_noise=sample_noise):\n",
    "#     recon_img,l_dist,l_sample = generator_model(torch.FloatTensor(example_data[i]).unsqueeze(0).to(device))\n",
    "#     noise = AddNoise(l_sample.shape, device, sample_noise, percent_noise=percentage)\n",
    "#     l_sample_noised = noise.noisy(l_sample)\n",
    "#     recon_noised = generator_model.decoder(l_sample_noised)\n",
    "#     plt.figure(figsize=(10,10))\n",
    "#     plt.subplot(1,3,1)\n",
    "#     plt.imshow(example_data[i][0].detach().cpu().numpy(), cmap='gray', interpolation='none')\n",
    "#     plt.title(\"{} -> {}\".format(example_targets[i].item(), torch.argmax(classifier(F.upsample(example_data[i].unsqueeze(0), (28,28), mode='bilinear').to(device))).item()))\n",
    "#     plt.subplot(1,3,2)\n",
    "#     plt.imshow(recon_img[0][0].detach().cpu().numpy(), cmap='gray', interpolation='none')\n",
    "#     plt.title(\"{} -> {}\".format(example_targets[i].item(), torch.argmax(classifier(F.upsample(recon_img, (28,28), mode='bilinear').to(device))).item()))\n",
    "#     plt.subplot(1,3,3)\n",
    "#     plt.imshow(recon_noised[0][0].detach().cpu().numpy(), cmap='gray', interpolation='none')\n",
    "#     plt.title(\"{} -> {}\".format(example_targets[i].item(), torch.argmax(classifier(F.upsample(recon_noised, (28,28), mode='bilinear').to(device))).item()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
