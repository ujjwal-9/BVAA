{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from torchvision.utils import save_image\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from timeit import default_timer\n",
    "from collections import defaultdict\n",
    "from tqdm import trange\n",
    "\n",
    "class Trainer():\n",
    "    def __init__(self, model, optimizer, loss_f, device, save_dir=\"results\", is_progress_bar=True):\n",
    "            self.device = device\n",
    "            self.model = model.to(device)\n",
    "            self.loss_f = loss_f\n",
    "            self.optimizer = optimizer\n",
    "            self.save_dir = save_dir\n",
    "            self.is_progress_bar = is_progress_bar\n",
    "            \n",
    "    def __call__(self, data_loader, epochs=10, checkpoint_every=10):\n",
    "        start = default_timer()\n",
    "        self.model.train()\n",
    "        for epoch in range(epochs):\n",
    "            storer = defaultdict(list)\n",
    "            mean_epoch_loss = self._train_epoch(data_loader, storer, epoch)\n",
    "            mean_epoch_loss = self._test_epoch(data_loader, storer, epoch)\n",
    "            with torch.no_grad():\n",
    "                sample = torch.randn(64, self.model.latent_dim).to(device)\n",
    "                sample = self.model.decoder(sample).cpu()  # make sure on cpu\n",
    "                save_image(sample.view(64, 1, 32, 32),\n",
    "                           './results/samples/' + str(epoch) + '.png')\n",
    "            \n",
    "    def _train_epoch(self, data_loader, storer, epoch):\n",
    "        epoch_loss = 0.\n",
    "        kwargs = dict(desc=\"Epoch {}\".format(epoch + 1), leave=False,\n",
    "                      disable=not self.is_progress_bar)\n",
    "        with trange(len(data_loader), **kwargs) as t:\n",
    "            for _, (data, _) in enumerate(data_loader):\n",
    "                iter_loss = self._train_iteration(data, storer)\n",
    "                epoch_loss += iter_loss\n",
    "                t.set_postfix(loss=iter_loss)\n",
    "                t.update()\n",
    "        mean_epoch_loss = epoch_loss / len(data_loader)\n",
    "        return mean_epoch_loss\n",
    "    \n",
    "    def _train_iteration(self, data, storer):\n",
    "        batch_size, channel, height, width = data.size()\n",
    "        data = data.to(self.device)\n",
    "        recon_batch, latent_dist, latent_sample = self.model(data)\n",
    "        loss = self.loss_f(data, recon_batch, latent_dist, self.model.training, \n",
    "                           storer, latent_sample=latent_sample)\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "            \n",
    "        return loss.item()\n",
    "    \n",
    "    def _test_epoch(self, data_loader, storer, epoch):\n",
    "        epoch_loss = 0.\n",
    "        kwargs = dict(desc=\"Epoch {}\".format(epoch + 1), leave=False,\n",
    "                      disable=not self.is_progress_bar)\n",
    "        with trange(len(data_loader), **kwargs) as t:\n",
    "            for _, (data, _) in enumerate(data_loader):\n",
    "                iter_loss = self._train_iteration(data, storer)\n",
    "                epoch_loss += iter_loss\n",
    "                t.set_postfix(loss=iter_loss)\n",
    "                t.update()\n",
    "        mean_epoch_loss = epoch_loss / len(data_loader)\n",
    "        return mean_epoch_loss\n",
    "    \n",
    "    def _test_iteration(self, data, storer):\n",
    "        batch_size, channel, height, width = data.size()\n",
    "        data = data.to(self.device)\n",
    "        recon_batch, latent_dist, latent_sample = self.model(data)\n",
    "        loss = self.loss_f(data, recon_batch, latent_dist, self.model.training, \n",
    "                               storer, latent_sample=latent_sample)\n",
    "            \n",
    "        return loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOSSES = [\"betaH\", \"betaB\"]\n",
    "RECON_DIST = [\"bernoulli\", \"laplace\", \"gaussian\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "mnist_dataset = datasets.MNIST('/home/data/bvaa', \n",
    "                   train=True, \n",
    "                   download=True, \n",
    "                   transform=transforms.Compose([\n",
    "                       transforms.Resize(32),\n",
    "                       transforms.ToTensor()\n",
    "                   ]))\n",
    "\n",
    "mnist_dataset_test = datasets.MNIST('/home/data/bvaa', train=False, download=True, transform=transforms.Compose([\n",
    "                       transforms.Resize(32),\n",
    "                       transforms.ToTensor()\n",
    "                   ]))\n",
    "\n",
    "train_loader = DataLoader(mnist_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "test_loader = DataLoader(mnist_dataset_test, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = enumerate(test_loader)\n",
    "batch_idx, (example_data, example_targets) = next(examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vae import VAE\n",
    "from encoder import Encoder\n",
    "from decoder import Decoder\n",
    "from losses import get_loss_fn\n",
    "from torch import optim\n",
    "\n",
    "latent_dim = 12\n",
    "img_size = [1,32,32]\n",
    "\n",
    "lr = 5e-4\n",
    "\n",
    "betaB_args = {\"rec_dist\": \"bernoulli\",\n",
    "              \"reg_anneal\": 10000, \n",
    "              \"betaH_B\": 4,\n",
    "              \"betaB_initC\": 0,\n",
    "              \"betaB_finC\": 25,\n",
    "              \"betaB_G\": 100\n",
    "             }\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "loss_f = get_loss_fn(\"betaB\", n_data=len(train_loader.dataset), device=device, **betaB_args)\n",
    "\n",
    "encoder = Encoder(img_size, latent_dim)\n",
    "decoder = Decoder(img_size, latent_dim)\n",
    "\n",
    "generator_model = VAE(img_size, latent_dim, encoder, decoder).to(device)\n",
    "optimizer = optim.Adam(generator_model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer = Trainer(generator_model, optimizer, loss_f, device, logger=logger,\\\n",
    "#                   save_dir=exp_dir, is_progress_bar=False)\n",
    "# epochs = 100\n",
    "# checkpoint_every = 10\n",
    "# trainer(train_loader, epochs=epochs, checkpoint_every=checkpoint_every)\n",
    "# torch.save(trainer.model.state_dict(), 'test.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generator_model.load_state_dict(torch.load('models/test.pt'))\n",
    "# generator_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Classifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Classifier, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, 3, 1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, 1)\n",
    "        self.dropout1 = nn.Dropout2d(0.25)\n",
    "        self.dropout2 = nn.Dropout2d(0.5)\n",
    "        self.fc1 = nn.Linear(9216, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = self.dropout1(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.fc2(x)\n",
    "        output = F.softmax(x, dim=1)\n",
    "        return output\n",
    "\n",
    "classifier = Classifier()\n",
    "classifier.load_state_dict(torch.load('models/mnist_cnn_non_log.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AddNoise():\n",
    "    def __init__(self, shape, device, noise=None, percent_noise=0.1):\n",
    "        self.device = device\n",
    "        self.percent_noise = percent_noise\n",
    "        if noise is None:\n",
    "            self.noise = torch.randn(shape)\n",
    "        else:\n",
    "            self.noise = noise\n",
    "\n",
    "    def noisy(self, data):\n",
    "        x = self.noise.to(self.device) * self.percent_noise + data * (1 - self.percent_noise)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Noise for Testing: \n",
      " tensor([[ 0.7344,  0.0049,  0.1108, -0.9742,  0.7806,  0.4000, -0.6167, -1.0133,\n",
      "          0.4483, -0.2564, -0.7429,  0.7341]])\n"
     ]
    }
   ],
   "source": [
    "sample_noise = torch.randn(torch.Size([1, 12]))\n",
    "backup_sample_noise = sample_noise\n",
    "print(\"Sample Noise for Testing: \\n\", sample_noise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Decoder(\n",
       "  (lin1): Linear(in_features=12, out_features=256, bias=True)\n",
       "  (lin2): Linear(in_features=256, out_features=256, bias=True)\n",
       "  (lin3): Linear(in_features=256, out_features=512, bias=True)\n",
       "  (convT1): ConvTranspose2d(32, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "  (convT2): ConvTranspose2d(32, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "  (convT3): ConvTranspose2d(32, 1, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder = generator_model.decoder\n",
    "decoder.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Classifier(\n",
       "  (conv1): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (dropout1): Dropout2d(p=0.25, inplace=False)\n",
       "  (dropout2): Dropout2d(p=0.5, inplace=False)\n",
       "  (fc1): Linear(in_features=9216, out_features=128, bias=True)\n",
       "  (fc2): Linear(in_features=128, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constrained Translator\n",
    "class Translator(nn.Module):\n",
    "    def __init__(self, hidden_layers=[512, 1024, 1024, 1024, 2048], latent_dim=latent_dim):\n",
    "        super(Translator, self).__init__()\n",
    "        self.hidden_layers = hidden_layers\n",
    "        self.hidden_layers.insert(0, latent_dim)\n",
    "        self.hidden_layers.append(latent_dim)\n",
    "        self.layers = []\n",
    "        \n",
    "        for i in range(len(self.hidden_layers)-1):\n",
    "            self.layers.append(nn.Linear(self.hidden_layers[i], self.hidden_layers[i+1]))\n",
    "        \n",
    "        self.layers = nn.ModuleList(self.layers)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "translator = Translator().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_logits(target_label, pred, confidence=1, device=device):\n",
    "    logits = torch.zeros(pred.shape, dtype=torch.float64)\n",
    "    pred_labels = torch.argmax(pred, dim=1)\n",
    "#     print(logits.shape)\n",
    "    logits[:, pred_labels] = torch.DoubleTensor([1-confidence]*pred.shape[0])\n",
    "    logits[:, [target_label]*pred.shape[0]] += torch.DoubleTensor([confidence]*pred.shape[0])\n",
    "    return logits.to(device)\n",
    "\n",
    "class T_Loss(nn.Module):\n",
    "    def __init__(self, decoder=decoder, classifier=classifier):\n",
    "        super(T_Loss, self).__init__()\n",
    "        self.decoder = decoder\n",
    "        self.classifier = classifier\n",
    "    def forward(self, x, org_x, target_label):\n",
    "        org_image = self.decoder(org_x)\n",
    "        noised_image = self.decoder(x)\n",
    "        loss1 = nn.MSELoss()(noised_image, org_image)\n",
    "        preds = self.classifier(F.upsample(noised_image, (28,28), mode='bilinear', align_corners=True))\n",
    "        target = create_logits(target_label, preds)\n",
    "        loss2 = nn.BCELoss()(preds, target.float())\n",
    "        loss = loss1 + 2*loss2\n",
    "        out_labels = preds.argmax(dim=1, keepdim=True)\n",
    "        correct = out_labels.eq(torch.Tensor([target_label]*out_labels.shape[0]).to(device)).sum()\n",
    "        return loss, correct.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "T_Loss(\n",
       "  (decoder): Decoder(\n",
       "    (lin1): Linear(in_features=12, out_features=256, bias=True)\n",
       "    (lin2): Linear(in_features=256, out_features=256, bias=True)\n",
       "    (lin3): Linear(in_features=256, out_features=512, bias=True)\n",
       "    (convT1): ConvTranspose2d(32, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (convT2): ConvTranspose2d(32, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (convT3): ConvTranspose2d(32, 1, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "  )\n",
       "  (classifier): Classifier(\n",
       "    (conv1): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (dropout1): Dropout2d(p=0.25, inplace=False)\n",
       "    (dropout2): Dropout2d(p=0.5, inplace=False)\n",
       "    (fc1): Linear(in_features=9216, out_features=128, bias=True)\n",
       "    (fc2): Linear(in_features=128, out_features=10, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tloss = T_Loss().to(device)\n",
    "tloss.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71bdc9e7c8444b2182c95a6493380362",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/bvaa/lib/python3.6/site-packages/torch/nn/functional.py:2416: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.\n",
      "  warnings.warn(\"nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0\tLoss: 2468.950634\tCorrect: 640\n",
      "Train Epoch: 1\tLoss: 2493.821359\tCorrect: 0\n",
      "Train Epoch: 2\tLoss: 2490.018757\tCorrect: 0\n",
      "Train Epoch: 3\tLoss: 2485.292402\tCorrect: 0\n",
      "Train Epoch: 4\tLoss: 2482.318564\tCorrect: 0\n",
      "Train Epoch: 5\tLoss: 2480.832618\tCorrect: 0\n",
      "Train Epoch: 6\tLoss: 2478.619244\tCorrect: 0\n",
      "Train Epoch: 7\tLoss: 2478.376268\tCorrect: 0\n",
      "Train Epoch: 8\tLoss: 2477.618975\tCorrect: 0\n",
      "Train Epoch: 9\tLoss: 2477.050852\tCorrect: 0\n",
      "Train Epoch: 10\tLoss: 2476.511465\tCorrect: 0\n",
      "Train Epoch: 11\tLoss: 2476.289140\tCorrect: 0\n",
      "Train Epoch: 12\tLoss: 2476.012638\tCorrect: 0\n",
      "Train Epoch: 13\tLoss: 2475.941919\tCorrect: 0\n",
      "Train Epoch: 14\tLoss: 2476.047133\tCorrect: 0\n",
      "Train Epoch: 15\tLoss: 2476.053952\tCorrect: 0\n",
      "Train Epoch: 16\tLoss: 2475.867257\tCorrect: 0\n",
      "Train Epoch: 17\tLoss: 2475.773671\tCorrect: 0\n",
      "Train Epoch: 18\tLoss: 2475.719392\tCorrect: 0\n",
      "Train Epoch: 19\tLoss: 2475.959057\tCorrect: 0\n",
      "Train Epoch: 20\tLoss: 2475.523338\tCorrect: 0\n",
      "Train Epoch: 21\tLoss: 2475.539330\tCorrect: 0\n",
      "Train Epoch: 22\tLoss: 2475.765887\tCorrect: 0\n",
      "Train Epoch: 23\tLoss: 2475.392677\tCorrect: 0\n",
      "Train Epoch: 24\tLoss: 2475.671383\tCorrect: 0\n",
      "Train Epoch: 25\tLoss: 2475.758234\tCorrect: 0\n",
      "Train Epoch: 26\tLoss: 2475.411647\tCorrect: 0\n",
      "Train Epoch: 27\tLoss: 2475.970108\tCorrect: 0\n",
      "Train Epoch: 28\tLoss: 2475.711456\tCorrect: 0\n",
      "Train Epoch: 29\tLoss: 2475.400136\tCorrect: 0\n",
      "Train Epoch: 30\tLoss: 2475.323169\tCorrect: 0\n",
      "Train Epoch: 31\tLoss: 2474.983944\tCorrect: 0\n",
      "Train Epoch: 32\tLoss: 2475.863691\tCorrect: 0\n",
      "Train Epoch: 33\tLoss: 2475.449394\tCorrect: 0\n",
      "Train Epoch: 34\tLoss: 2475.703513\tCorrect: 0\n",
      "Train Epoch: 35\tLoss: 2475.476171\tCorrect: 0\n",
      "Train Epoch: 36\tLoss: 2475.086303\tCorrect: 0\n",
      "Train Epoch: 37\tLoss: 2475.879265\tCorrect: 0\n",
      "Train Epoch: 38\tLoss: 2475.105629\tCorrect: 0\n",
      "Train Epoch: 39\tLoss: 2495.411451\tCorrect: 0\n",
      "Train Epoch: 40\tLoss: 2594.370010\tCorrect: 0\n",
      "Train Epoch: 41\tLoss: 2603.615171\tCorrect: 0\n",
      "Train Epoch: 42\tLoss: 2606.783653\tCorrect: 0\n",
      "Train Epoch: 43\tLoss: 2602.854349\tCorrect: 0\n",
      "Train Epoch: 44\tLoss: 2574.462206\tCorrect: 0\n",
      "Train Epoch: 45\tLoss: 2559.999723\tCorrect: 0\n",
      "Train Epoch: 46\tLoss: 2551.157395\tCorrect: 0\n",
      "Train Epoch: 47\tLoss: 2535.521730\tCorrect: 0\n",
      "Train Epoch: 48\tLoss: 2513.336589\tCorrect: 0\n",
      "Train Epoch: 49\tLoss: 2469.252421\tCorrect: 0\n",
      "Train Epoch: 50\tLoss: 2392.513463\tCorrect: 0\n",
      "Train Epoch: 51\tLoss: 2278.076520\tCorrect: 0\n",
      "Train Epoch: 52\tLoss: 2162.736531\tCorrect: 0\n",
      "Train Epoch: 53\tLoss: 2094.537032\tCorrect: 0\n",
      "Train Epoch: 54\tLoss: 2061.324272\tCorrect: 0\n",
      "Train Epoch: 55\tLoss: 2072.524193\tCorrect: 0\n",
      "Train Epoch: 56\tLoss: 2107.415976\tCorrect: 0\n",
      "Train Epoch: 57\tLoss: 2180.786084\tCorrect: 0\n",
      "Train Epoch: 58\tLoss: 2273.197979\tCorrect: 0\n",
      "Train Epoch: 59\tLoss: 2353.398191\tCorrect: 0\n",
      "Train Epoch: 60\tLoss: 2414.309116\tCorrect: 0\n",
      "Train Epoch: 61\tLoss: 2431.460265\tCorrect: 0\n",
      "Train Epoch: 62\tLoss: 2444.416309\tCorrect: 0\n",
      "Train Epoch: 63\tLoss: 2454.288561\tCorrect: 0\n",
      "Train Epoch: 64\tLoss: 2465.906060\tCorrect: 0\n",
      "Train Epoch: 65\tLoss: 2471.349697\tCorrect: 0\n",
      "Train Epoch: 66\tLoss: 2479.424467\tCorrect: 0\n",
      "Train Epoch: 67\tLoss: 2480.270591\tCorrect: 0\n",
      "Train Epoch: 68\tLoss: 2486.288054\tCorrect: 0\n",
      "Train Epoch: 69\tLoss: 2494.155722\tCorrect: 0\n",
      "Train Epoch: 70\tLoss: 2506.914067\tCorrect: 0\n",
      "Train Epoch: 71\tLoss: 2521.086941\tCorrect: 0\n",
      "Train Epoch: 72\tLoss: 2530.267158\tCorrect: 0\n",
      "Train Epoch: 73\tLoss: 2544.647983\tCorrect: 0\n",
      "Train Epoch: 74\tLoss: 2567.440380\tCorrect: 0\n",
      "Train Epoch: 75\tLoss: 2583.830733\tCorrect: 0\n",
      "Train Epoch: 76\tLoss: 2609.392767\tCorrect: 0\n",
      "Train Epoch: 77\tLoss: 2630.118989\tCorrect: 0\n",
      "Train Epoch: 78\tLoss: 2642.098432\tCorrect: 0\n",
      "Train Epoch: 79\tLoss: 2652.491976\tCorrect: 0\n",
      "Train Epoch: 80\tLoss: 2672.821336\tCorrect: 0\n",
      "Train Epoch: 81\tLoss: 2685.335094\tCorrect: 0\n",
      "Train Epoch: 82\tLoss: 2701.336536\tCorrect: 0\n",
      "Train Epoch: 83\tLoss: 2714.323979\tCorrect: 0\n",
      "Train Epoch: 84\tLoss: 2722.529705\tCorrect: 0\n",
      "Train Epoch: 85\tLoss: 2731.702574\tCorrect: 0\n",
      "Train Epoch: 86\tLoss: 2740.753883\tCorrect: 0\n",
      "Train Epoch: 87\tLoss: 2738.476819\tCorrect: 0\n",
      "Train Epoch: 88\tLoss: 2738.619679\tCorrect: 0\n",
      "Train Epoch: 89\tLoss: 2731.138717\tCorrect: 0\n",
      "Train Epoch: 90\tLoss: 2726.609157\tCorrect: 0\n",
      "Train Epoch: 91\tLoss: 2706.975121\tCorrect: 0\n",
      "Train Epoch: 92\tLoss: 2666.110173\tCorrect: 0\n",
      "Train Epoch: 93\tLoss: 2626.679051\tCorrect: 0\n",
      "Train Epoch: 94\tLoss: 2585.226666\tCorrect: 0\n",
      "Train Epoch: 95\tLoss: 2553.480807\tCorrect: 0\n",
      "Train Epoch: 96\tLoss: 2530.684185\tCorrect: 0\n",
      "Train Epoch: 97\tLoss: 2519.993888\tCorrect: 0\n",
      "Train Epoch: 98\tLoss: 2533.492584\tCorrect: 0\n",
      "Train Epoch: 99\tLoss: 2523.727777\tCorrect: 0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "attack_log_interval = 1\n",
    "alt_target = 2\n",
    "translator.train()\n",
    "optimizer = optim.Adam(translator.parameters(), lr=1e-3)\n",
    "for epoch in tqdm(range(100)):\n",
    "    epoch_loss = 0\n",
    "    epoch_correct = 0\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data = torch.FloatTensor(data).to(device)\n",
    "        \n",
    "        _, l_dist, l_sample = generator_model(data)\n",
    "        \n",
    "        noised_sample = translator(l_sample)\n",
    "        \n",
    "        loss, correct = tloss(noised_sample, l_sample, alt_target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        epoch_correct += correct\n",
    "        \n",
    "    if (epoch+1) % attack_log_interval == 0:\n",
    "        print('Train Epoch: {}\\tLoss: {:.6f}\\tCorrect: {}'.format(\n",
    "            epoch, epoch_loss, epoch_correct))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# recon_img = generator_model.decoder(0.005*sample_noise.cuda())\n",
    "# pred = classifier(F.upsample(recon_img, (28,28), mode='bilinear', align_corners=True).cpu())\n",
    "# print(torch.argmax(pred))\n",
    "# plt.imshow(recon_img[0][0].detach().cpu().numpy(), cmap='gray', interpolation='none')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def evaluate(i, percentage, sample_noise=sample_noise):\n",
    "#     recon_img,l_dist,l_sample = generator_model(torch.FloatTensor(example_data[i]).unsqueeze(0).to(device))\n",
    "#     noise = AddNoise(l_sample.shape, device, sample_noise, percent_noise=percentage)\n",
    "#     l_sample_noised = noise.noisy(l_sample)\n",
    "#     recon_noised = generator_model.decoder(l_sample_noised)\n",
    "#     plt.figure(figsize=(10,10))\n",
    "#     plt.subplot(1,3,1)\n",
    "#     plt.imshow(example_data[i][0].detach().cpu().numpy(), cmap='gray', interpolation='none')\n",
    "#     plt.title(\"{} -> {}\".format(example_targets[i].item(), torch.argmax(classifier(F.upsample(example_data[i].unsqueeze(0), (28,28), mode='bilinear').to(device))).item()))\n",
    "#     plt.subplot(1,3,2)\n",
    "#     plt.imshow(recon_img[0][0].detach().cpu().numpy(), cmap='gray', interpolation='none')\n",
    "#     plt.title(\"{} -> {}\".format(example_targets[i].item(), torch.argmax(classifier(F.upsample(recon_img, (28,28), mode='bilinear').to(device))).item()))\n",
    "#     plt.subplot(1,3,3)\n",
    "#     plt.imshow(recon_noised[0][0].detach().cpu().numpy(), cmap='gray', interpolation='none')\n",
    "#     plt.title(\"{} -> {}\".format(example_targets[i].item(), torch.argmax(classifier(F.upsample(recon_noised, (28,28), mode='bilinear').to(device))).item()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
