{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from torchvision.utils import save_image\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "TRAIN_LOSSES_LOGFILE = \"train_losses.log\"\n",
    "\n",
    "class LossesLogger(object):\n",
    "    def __init__(self, file_path_name):\n",
    "        if os.path.isfile(file_path_name):\n",
    "            os.remove(file_path_name)\n",
    "        \n",
    "        self.logger = logging.getLogger(\"losses_logger\")\n",
    "        self.logger.setLevel(1)\n",
    "        file_handler = logging.FileHandler(file_path_name)\n",
    "        file_handler.setLevel(1)\n",
    "        self.logger.addHandler(file_handler)\n",
    "        \n",
    "        header = \",\".join([\"Epoch\", \"Loss\", \"Value\"])\n",
    "        self.logger.debug(header)\n",
    "    \n",
    "    def log(self, epoch, losses_storer):\n",
    "        for k, v in losses_storer.item():\n",
    "            log_string = \",\".join(str(item) for item in [epoch, k, sum(v)/len(l)])\n",
    "            self.logger.debug(log_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from timeit import default_timer\n",
    "from collections import defaultdict\n",
    "from tqdm import trange\n",
    "\n",
    "class Trainer():\n",
    "    def __init__(self, model, optimizer, loss_f, device, logger=logging.getLogger(__name__),\n",
    "                 save_dir=\"results\", is_progress_bar=True):\n",
    "            self.device = device\n",
    "            self.model = model.to(device)\n",
    "            self.loss_f = loss_f\n",
    "            self.optimizer = optimizer\n",
    "            self.save_dir = save_dir\n",
    "            self.is_progress_bar = is_progress_bar\n",
    "            self.logger = logger\n",
    "            self.losses_logger = LossesLogger(os.path.join(self.save_dir, TRAIN_LOSSES_LOGFILE))\n",
    "            self.logger.info(\"Training Device: {}\".format(self.device))\n",
    "            \n",
    "    def __call__(self, data_loader, epochs=10, checkpoint_every=10):\n",
    "        start = default_timer()\n",
    "        self.model.train()\n",
    "        for epoch in range(epochs):\n",
    "            storer = defaultdict(list)\n",
    "            mean_epoch_loss = self._train_epoch(data_loader, storer, epoch)\n",
    "            mean_epoch_loss = self._test_epoch(data_loader, storer, epoch)\n",
    "            with torch.no_grad():\n",
    "                sample = torch.randn(64, self.model.latent_dim).to(device)\n",
    "                sample = model.decoder(sample).cpu()  # make sure on cpu\n",
    "                save_image(sample.view(64, 1, 32, 32),\n",
    "                           './results/samples/' + str(epoch) + '.png')\n",
    "            \n",
    "    def _train_epoch(self, data_loader, storer, epoch):\n",
    "        epoch_loss = 0.\n",
    "        kwargs = dict(desc=\"Epoch {}\".format(epoch + 1), leave=False,\n",
    "                      disable=not self.is_progress_bar)\n",
    "        with trange(len(data_loader), **kwargs) as t:\n",
    "            for _, (data, _) in enumerate(data_loader):\n",
    "                iter_loss = self._train_iteration(data, storer)\n",
    "                epoch_loss += iter_loss\n",
    "                t.set_postfix(loss=iter_loss)\n",
    "                t.update()\n",
    "        mean_epoch_loss = epoch_loss / len(data_loader)\n",
    "        return mean_epoch_loss\n",
    "    \n",
    "    def _train_iteration(self, data, storer):\n",
    "        batch_size, channel, height, width = data.size()\n",
    "        data = data.to(self.device)\n",
    "        recon_batch, latent_dist, latent_sample = self.model(data)\n",
    "        loss = self.loss_f(data, recon_batch, latent_dist, self.model.training, \n",
    "                           storer, latent_sample=latent_sample)\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "            \n",
    "        return loss.item()\n",
    "    \n",
    "    def _test_epoch(self, data_loader, storer, epoch):\n",
    "        epoch_loss = 0.\n",
    "        kwargs = dict(desc=\"Epoch {}\".format(epoch + 1), leave=False,\n",
    "                      disable=not self.is_progress_bar)\n",
    "        with trange(len(data_loader), **kwargs) as t:\n",
    "            for _, (data, _) in enumerate(data_loader):\n",
    "                iter_loss = self._train_iteration(data, storer)\n",
    "                epoch_loss += iter_loss\n",
    "                t.set_postfix(loss=iter_loss)\n",
    "                t.update()\n",
    "        mean_epoch_loss = epoch_loss / len(data_loader)\n",
    "        return mean_epoch_loss\n",
    "    \n",
    "    def _test_iteration(self, data, storer):\n",
    "        batch_size, channel, height, width = data.size()\n",
    "        data = data.to(self.device)\n",
    "        recon_batch, latent_dist, latent_sample = self.model(data)\n",
    "        loss = self.loss_f(data, recon_batch, latent_dist, self.model.training, \n",
    "                               storer, latent_sample=latent_sample)\n",
    "            \n",
    "        return loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOSSES = [\"betaH\", \"betaB\"]\n",
    "RECON_DIST = [\"bernoulli\", \"laplace\", \"gaussian\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "mnist_dataset = datasets.MNIST('../../data', \n",
    "                   train=True, \n",
    "                   download=True, \n",
    "                   transform=transforms.Compose([\n",
    "                       transforms.Resize(32),\n",
    "                       transforms.ToTensor()\n",
    "                   ]))\n",
    "\n",
    "mnist_dataset_test = datasets.MNIST('../../data', train=False, download=True, transform=transforms.Compose([\n",
    "                       transforms.Resize(32),\n",
    "                       transforms.ToTensor()\n",
    "                   ]))\n",
    "\n",
    "train_loader = DataLoader(mnist_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "test_loader = DataLoader(mnist_dataset_test, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "RES_DIR = \"./results\"\n",
    "formatter = logging.Formatter('%(asctime)s %(levelname)s - %(funcName)s: %(message)s',\n",
    "                                  \"%H:%M:%S\")\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(1)\n",
    "stream = logging.StreamHandler()\n",
    "stream.setLevel(1)\n",
    "stream.setFormatter(formatter)\n",
    "logger.addHandler(stream)\n",
    "\n",
    "exp_dir = os.path.join(RES_DIR, \"first\")\n",
    "# logger.info(\"Root directory for saving and loading experiments: {}\".format(exp_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vae import VAE\n",
    "from losses import get_loss_fn\n",
    "from torch import optim\n",
    "\n",
    "latent_dim = 12\n",
    "img_size = [1,32,32]\n",
    "\n",
    "lr = 5e-4\n",
    "\n",
    "betaB_args = {\"rec_dist\": \"bernoulli\",\n",
    "              \"reg_anneal\": 10000, \n",
    "              \"betaH_B\": 4,\n",
    "              \"betaB_initC\": 0,\n",
    "              \"betaB_finC\": 25,\n",
    "              \"betaB_G\": 100\n",
    "             }\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "loss_f = get_loss_fn(\"betaB\", n_data=len(train_loader.dataset), device=device, **betaB_args)\n",
    "\n",
    "generator_model = VAE(img_size, latent_dim).to(device)\n",
    "optimizer = optim.Adam(generator_model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "15:17:23 INFO - __init__: Training Device: cuda\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(generator_model, optimizer, loss_f, device, logger=logger,\n",
    "                 save_dir=exp_dir, is_progress_bar=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# epochs = 100\n",
    "# checkpoint_every = 10\n",
    "# trainer(train_loader, epochs=epochs, checkpoint_every=checkpoint_every)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    def __init__(self, generator, device, image_size=32, mode='train', model_path='./model/Siamese',\n",
    "                 generate_path='./Generated', num_epochs=100, distance_weight=1.0, \n",
    "                 dataset='MNIST', tensorboard=True, batch_size=64, batch_size_test=1000):\n",
    "        self.mode = mode\n",
    "        self.image_size = image_size\n",
    "        self.model_path = model_path\n",
    "        self.generate_path = generate_path\n",
    "        self.dataset = dataset\n",
    "        self.num_epochs = num_epochs\n",
    "        self.distance_weight = distance_weight\n",
    "        self.tensorboard = tensorboard\n",
    "        self.generator = generator\n",
    "        self.batch_size = batch_size\n",
    "        self.batch_size_test = batch_size_test\n",
    "        self.device = device\n",
    "\n",
    "config = Config(generator=generator_model, device=device, num_epochs=400, tensorboard=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from discriminator import SiameseDiscriminator\n",
    "from siameseDataset import SiameseMNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_siamese_dataset = SiameseMNIST(mnist_dataset)\n",
    "mnist_siamese_dataset_test = SiameseMNIST(mnist_dataset_test)\n",
    "\n",
    "siamese_data_loader = DataLoader(dataset=mnist_siamese_dataset, batch_size=config.batch_size, shuffle=True)\n",
    "siamese_data_loader_test = DataLoader(dataset=mnist_siamese_dataset_test, batch_size=config.batch_size_test, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/400 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We are training\n",
      "\n",
      "0 2020-01-22 15:17:24.493797\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 1/400 [00:34<3:49:41, 34.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 2020-01-22 15:17:59.034694\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 2/400 [01:08<3:47:48, 34.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 2020-01-22 15:18:32.922900\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  1%|          | 3/400 [01:42<3:47:06, 34.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 2020-01-22 15:19:07.199240\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  1%|          | 4/400 [02:17<3:47:03, 34.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 2020-01-22 15:19:41.787470\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  1%|▏         | 5/400 [02:51<3:45:21, 34.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 2020-01-22 15:20:15.616456\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  2%|▏         | 6/400 [03:24<3:43:57, 34.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6 2020-01-22 15:20:49.426523\n"
     ]
    }
   ],
   "source": [
    "from generator import SiameseGanSolver\n",
    "\n",
    "start_time = datetime.now()\n",
    "\n",
    "solver = SiameseGanSolver(config, siamese_data_loader)\n",
    "solver.train()\n",
    "\n",
    "end_time = datetime.now()\n",
    "print(send_mail(start_time, end_time))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
