{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from datetime import datetime\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "\n",
    "from torchvision import datasets, transforms\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "from torch import nn\n",
    "from torchvision.utils import save_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "\n",
    "mnist_dataset = datasets.MNIST('../../data', \n",
    "                   train=True, \n",
    "                   download=True, \n",
    "                   transform=transforms.Compose([\n",
    "                       transforms.Resize(32),\n",
    "                       transforms.ToTensor()\n",
    "                   ]))\n",
    "\n",
    "mnist_dataset_test = datasets.MNIST('../../data', train=False, download=True, transform=transforms.Compose([\n",
    "                       transforms.Resize(32),\n",
    "                       transforms.ToTensor()\n",
    "                   ]))\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(mnist_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(mnist_dataset_test, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Encoder(nn.Module):\n",
    "#     def __init__(self, x_size, h_size, z_size):\n",
    "#         super(Encoder, self).__init__()\n",
    "#         self.fc1 = nn.Linear(x_size, h_size)\n",
    "#         self.mu_gen = nn.Linear(h_size, z_size)\n",
    "#         # make the output to be the logarithm \n",
    "#         # i.e will have to take the exponent\n",
    "#         # which forces variance to be positive\n",
    "#         # not that this is the diagonal of the covariance\n",
    "#         self.log_var_gen = nn.Linear(h_size, z_size)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = F.relu(self.fc1(x))\n",
    "#         mu = self.mu_gen(x)\n",
    "#         log_var = self.log_var_gen(x)\n",
    "#         return mu, log_var\n",
    "    \n",
    "class Encoder(nn.Module):\n",
    "    r\"\"\"\n",
    "    References:\n",
    "            [1] Burgess, Christopher P., et al. \"Understanding disentangling in\n",
    "            $\\beta$-VAE.\" arXiv preprint arXiv:1804.03599 (2018).\n",
    "    \"\"\"\n",
    "    def __init__(self, img_size, latent_dim=10):\n",
    "\n",
    "        super(Encoder, self).__init__()\n",
    "\n",
    "        hidden_channels = 32\n",
    "        kernel_size = 4\n",
    "        hidden_dim = 256\n",
    "\n",
    "        self.latent_dim = latent_dim\n",
    "        self.img_size = img_size\n",
    "\n",
    "        self.reshape = (hidden_channels, kernel_size, kernel_size)\n",
    "\n",
    "        n_channels = self.img_size[0]\n",
    "\n",
    "        cnn_kwargs = dict(stride=2, padding=1)\n",
    "\n",
    "        self.conv1 = nn.Conv2d(n_channels, hidden_channels, kernel_size, **cnn_kwargs)\n",
    "        self.conv2 = nn.Conv2d(hidden_channels, hidden_channels, kernel_size, **cnn_kwargs)\n",
    "        self.conv3 = nn.Conv2d(hidden_channels, hidden_channels, kernel_size, **cnn_kwargs)\n",
    "\n",
    "        if self.img_size[1] == self.img_size[2] == 64:\n",
    "            self.conv_64 = nn.Conv2d(hidden_channels, hidden_channels, kernel_size, **cnn_kwargs)\n",
    "\n",
    "        self.lin1 = nn.Linear(np.product(self.reshape), hidden_dim)\n",
    "        self.lin2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "\n",
    "        self.mu_logvar_gen = nn.Linear(hidden_dim, self.latent_dim * 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "\n",
    "        x = torch.relu(self.conv1(x))\n",
    "        x = torch.relu(self.conv2(x))\n",
    "        x = torch.relu(self.conv3(x))\n",
    "\n",
    "        if self.img_size[1] == self.img_size[2] == 64:\n",
    "            x = torch.relu(self.conv_64(x))\n",
    "\n",
    "        x = x.view((batch_size, -1))\n",
    "        x = torch.relu(self.lin1(x))\n",
    "        x = torch.relu(self.lin2(x))\n",
    "\n",
    "        mu_logvar = self.mu_logvar_gen(x)\n",
    "\n",
    "        mu, logvar = mu_logvar.view(-1, self.latent_dim, 2).unbind(-1)\n",
    "\n",
    "        return mu, logvar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Decoder(nn.Module):\n",
    "#     def __init__(self, x_size, h_size, z_size):\n",
    "#         super(Decoder, self).__init__()\n",
    "#         self.fc1 = nn.Linear(z_size, h_size)\n",
    "#         self.fc3 = nn.Linear(h_size, x_size)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = F.relu(self.fc1(x))\n",
    "#         x = F.relu(self.fc3(x))\n",
    "#         # black and white MNIST => sigmoid for each pixel\n",
    "#         x = torch.sigmoid(x) \n",
    "#         return x\n",
    "    \n",
    "class Decoder(nn.Module):\n",
    "    r\"\"\"\n",
    "    References:\n",
    "            [1] Burgess, Christopher P., et al. \"Understanding disentangling in\n",
    "            $\\beta$-VAE.\" arXiv preprint arXiv:1804.03599 (2018).\n",
    "    \"\"\"\n",
    "    def __init__(self, img_size, latent_dim):\n",
    "\n",
    "        super(Decoder, self).__init__()\n",
    "\n",
    "        hidden_channels = 32\n",
    "        kernel_size = 4\n",
    "        hidden_dim = 256\n",
    "\n",
    "        self.img_size = img_size\n",
    "        self.latent_dim = latent_dim\n",
    "\n",
    "        self.reshape = (hidden_channels, kernel_size, kernel_size)\n",
    "\n",
    "        n_channels = self.img_size[0]\n",
    "\n",
    "        self.lin1 = nn.Linear(latent_dim, hidden_dim)\n",
    "        self.lin2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.lin3 = nn.Linear(hidden_dim, np.product(self.reshape))\n",
    "\n",
    "        cnn_kwargs = dict(stride=2, padding=1)\n",
    "\n",
    "        if self.img_size[1] == self.img_size[2] == 64:\n",
    "            self.convT_64 = nn.ConvTranspose2d(hidden_channels, hidden_channels, kernel_size, **cnn_kwargs)\n",
    "\n",
    "        self.convT1 = nn.ConvTranspose2d(hidden_channels, hidden_channels, kernel_size, **cnn_kwargs)\n",
    "        self.convT2 = nn.ConvTranspose2d(hidden_channels, hidden_channels, kernel_size, **cnn_kwargs)\n",
    "        self.convT3 = nn.ConvTranspose2d(hidden_channels, n_channels, kernel_size, **cnn_kwargs)\n",
    "\n",
    "    def forward(self, z):\n",
    "        batch_size = z.size(0)\n",
    "\n",
    "        x = torch.relu(self.lin1(z))\n",
    "        x = torch.relu(self.lin2(x))\n",
    "        x = torch.relu(self.lin3(x))\n",
    "\n",
    "        x = x.view(batch_size, *self.reshape)\n",
    "\n",
    "        if self.img_size[1] == self.img_size[2] == 64:\n",
    "            x = torch.relu(self.convT_64(x))\n",
    "\n",
    "        x = torch.relu(self.convT1(x))\n",
    "        x = torch.relu(self.convT2(x))\n",
    "\n",
    "        x = torch.sigmoid(self.convT3(x))\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class VAE(nn.Module):\n",
    "#     def __init__(self, x_size, h_size, z_size):\n",
    "#         super(VAE, self).__init__()\n",
    "#         self.x_size = x_size\n",
    "#         self.z_size = z_size\n",
    "#         self.encoder = Encoder(x_size, h_size, z_size)\n",
    "#         self.decoder = Decoder(x_size, h_size, z_size)\n",
    "\n",
    "#     def reparameterize(self, mu, log_var):\n",
    "#         std = torch.exp(0.5 * log_var) # square root in exponent => std\n",
    "#         eps = torch.randn_like(std)\n",
    "#         z = std * eps + mu\n",
    "#         return z\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         # make image linear (i.e vector form)\n",
    "#         x = x.view(-1, self.x_size)\n",
    "#         mu, log_var = self.encoder(x)\n",
    "#         z = self.reparameterize(mu, log_var)\n",
    "#         x_hat = self.decoder(z)\n",
    "#         return x_hat, mu, log_var\n",
    "\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self, img_size, latent_dim):\n",
    "        super(VAE, self).__init__()\n",
    "        if list(img_size[1:]) not in [[32,32], [64,64]]:\n",
    "            raise RuntimeError(\"{} sized images not supported. Only (None, 32, 32) and (None, 64, 64) supported. Build your own architecture or reshape images!\".format(img_size))\n",
    "\n",
    "        self.latent_dim = latent_dim\n",
    "        self.img_size = img_size\n",
    "        self.num_pixels = self.img_size[1] * self.img_size[2]\n",
    "        self.encoder = Encoder(img_size, self.latent_dim)\n",
    "        self.decoder = Decoder(img_size, self.latent_dim)\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reparameterize(self, mean, logvar):\n",
    "        if self.training:\n",
    "            std = torch.exp(0.5 * logvar)\n",
    "            eps = torch.randn_like(std)\n",
    "            return mean + std * eps\n",
    "\n",
    "        else:\n",
    "            return mean\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        latent_distribution = self.encoder(x)\n",
    "        latent_sample = self.reparameterize(*latent_distribution)\n",
    "        reconstruct = self.decoder(latent_sample)\n",
    "        return reconstruct, latent_distribution, latent_sample\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        self.apply(weights_init)\n",
    "\n",
    "    def sample_latent(self, x):\n",
    "        latent_distribution = self.encoder(x)\n",
    "        latent_sample = self.reparameterize(*latent_distribution)\n",
    "        return latent_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = enumerate(test_loader)\n",
    "batch_idx, (example_data, example_targets) = next(examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 6 Axes>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 6 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig = plt.figure()\n",
    "for i in range(6):\n",
    "    plt.subplot(2,3,i+1)\n",
    "    plt.tight_layout()\n",
    "    plt.imshow(example_data[i][0], cmap='gray', interpolation='none')\n",
    "    plt.title(\"Ground Truth: {}\".format(example_targets[i]))\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 32, 32]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(example_data[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_activation_name(activation):\n",
    "    \"\"\"Given a string or a `torch.nn.modules.activation` return the name of the activation.\"\"\"\n",
    "    if isinstance(activation, str):\n",
    "        return activation\n",
    "\n",
    "    mapper = {nn.LeakyReLU: \"leaky_relu\", nn.ReLU: \"relu\", nn.Tanh: \"tanh\",\n",
    "              nn.Sigmoid: \"sigmoid\", nn.Softmax: \"sigmoid\"}\n",
    "    for k, v in mapper.items():\n",
    "        if isinstance(activation, k):\n",
    "            return k\n",
    "\n",
    "    raise ValueError(\"Unkown given activation type : {}\".format(activation))\n",
    "\n",
    "\n",
    "def get_gain(activation):\n",
    "    \"\"\"Given an object of `torch.nn.modules.activation` or an activation name\n",
    "    return the correct gain.\"\"\"\n",
    "    if activation is None:\n",
    "        return 1\n",
    "\n",
    "    activation_name = get_activation_name(activation)\n",
    "\n",
    "    param = None if activation_name != \"leaky_relu\" else activation.negative_slope\n",
    "    gain = nn.init.calculate_gain(activation_name, param)\n",
    "\n",
    "    return gain\n",
    "def linear_init(layer, activation=\"relu\"):\n",
    "    \"\"\"Initialize a linear layer.\n",
    "    Args:\n",
    "        layer (nn.Linear): parameters to initialize.\n",
    "        activation (`torch.nn.modules.activation` or str, optional) activation that\n",
    "            will be used on the `layer`.\n",
    "    \"\"\"\n",
    "    x = layer.weight\n",
    "\n",
    "    if activation is None:\n",
    "        return nn.init.xavier_uniform_(x)\n",
    "\n",
    "    activation_name = get_activation_name(activation)\n",
    "\n",
    "    if activation_name == \"leaky_relu\":\n",
    "        a = 0 if isinstance(activation, str) else activation.negative_slope\n",
    "        return nn.init.kaiming_uniform_(x, a=a, nonlinearity='leaky_relu')\n",
    "    elif activation_name == \"relu\":\n",
    "        return nn.init.kaiming_uniform_(x, nonlinearity='relu')\n",
    "    elif activation_name in [\"sigmoid\", \"tanh\"]:\n",
    "        return nn.init.xavier_uniform_(x, gain=get_gain(activation))\n",
    "def weights_init(module):\n",
    "    if isinstance(module, nn.modules.conv._ConvNd):\n",
    "        # TO-DO: check litterature\n",
    "        linear_init(module)\n",
    "    elif isinstance(module, nn.Linear):\n",
    "        linear_init(module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_size = list(example_data[0].shape) # mnist image\n",
    "# h_size = 256\n",
    "z_size = 12\n",
    "model = VAE(img_size, z_size).to(device) # migrates to CUDA if you can"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VAE(\n",
       "  (encoder): Encoder(\n",
       "    (conv1): Conv2d(1, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (conv2): Conv2d(32, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (conv3): Conv2d(32, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (lin1): Linear(in_features=512, out_features=256, bias=True)\n",
       "    (lin2): Linear(in_features=256, out_features=256, bias=True)\n",
       "    (mu_logvar_gen): Linear(in_features=256, out_features=24, bias=True)\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (lin1): Linear(in_features=12, out_features=256, bias=True)\n",
       "    (lin2): Linear(in_features=256, out_features=256, bias=True)\n",
       "    (lin3): Linear(in_features=256, out_features=512, bias=True)\n",
       "    (convT1): ConvTranspose2d(32, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (convT2): ConvTranspose2d(32, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (convT3): ConvTranspose2d(32, 1, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_annealing(init, fin, step, annealing_steps):\n",
    "    \"\"\"Linear annealing of a parameter.\"\"\"\n",
    "    if annealing_steps == 0:\n",
    "        return fin\n",
    "    assert fin > init\n",
    "    delta = fin - init\n",
    "    annealed = min(init + delta * step / annealing_steps, fin)\n",
    "    return annealed\n",
    "\n",
    "def _kl_normal_loss(mean, logvar):\n",
    "    latent_dim = mean.size(1)\n",
    "    latent_kl = 0.5 * (-1 - logvar + mean.pow(2) + logvar.exp()).mean(dim=0)\n",
    "    total_kl = latent_kl.sum()\n",
    "\n",
    "    return total_kl\n",
    "\n",
    "def _reconstruction_loss(data, recon_data, distribution=\"bernoulli\"):\n",
    "    batch_size, n_channels, height, width = recon_data.size()\n",
    "    is_colored = n_channels == 3\n",
    "\n",
    "    if distribution == \"bernoulli\":\n",
    "        loss = F.binary_cross_entropy(recon_data, data, reduction=\"sum\")\n",
    "\n",
    "    elif distribution == \"gaussian\":\n",
    "        loss = F.mse_loss(recon_data * 255, data * 255, reduction=\"sum\") / 255\n",
    "\n",
    "    elif distribution == \"laplace\":\n",
    "        loss = F.l1_loss(recon_data, data, reduction=\"sum\")\n",
    "        loss = loss * 3\n",
    "        loss = loss * (loss != 0)\n",
    "\n",
    "    loss = loss / batch_size\n",
    "\n",
    "    return loss\n",
    "\n",
    "def loss_function(x_hat, x, mu, log_var, is_train, n_train_steps, steps_anneal=0, beta=1, C_init=0., C_fin=20., gamma=100.):\n",
    "    \"\"\"Compute the ELBO loss\"\"\"\n",
    "    x_size = x_hat.size(-1)\n",
    "    # black or white image => use sigmoid for each pixel\n",
    "#     rec_loss = F.binary_cross_entropy(x_hat, x.view(-1, x_size), reduction='sum')\n",
    "    rec_loss = _reconstruction_loss(x, x_hat, distribution=\"bernoulli\")\n",
    "    # closed form solution for gaussian prior and posterior\n",
    "#     kl_div = -0.5 * torch.sum(1 + log_var - mu.pow(2) - log_var.exp())\n",
    "    kl_div = _kl_normal_loss(mu, log_var)\n",
    "    \n",
    "    C = (linear_annealing(C_init, C_fin, n_train_steps, steps_anneal) if is_train else C_fin)\n",
    "    vae_loss = rec_loss + gamma * (kl_div - C).abs()\n",
    "#     vae_loss = rec_loss + beta * kl_div\n",
    "    return vae_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    def __init__(self, model, optimizer=optim.Adam, loss_function=loss_function):\n",
    "        self.model = model \n",
    "        self.optimizer = optimizer(self.model.parameters())\n",
    "        self.loss_function = loss_function\n",
    "        self.epoch = 0\n",
    "        \n",
    "    def __call__(self, train, test, n_epochs=100):\n",
    "        self.epoch = 0\n",
    "        for _ in range(n_epochs):\n",
    "            self._train_epoch(train)\n",
    "            self._test_epoch(test)\n",
    "            with torch.no_grad():\n",
    "                sample = torch.randn(64, self.model.latent_dim).to(device)\n",
    "                sample = model.decoder(sample).cpu()  # make sure on cpu\n",
    "                save_image(sample.view(64, 1, 32, 32),\n",
    "                           '../results/sample_' + str(self.epoch) + '.png')\n",
    "        \n",
    "    def _train_epoch(self, train):\n",
    "        self.epoch += 1\n",
    "        model.train() # make sure train mode (e.g. dropout)\n",
    "        train_loss = 0\n",
    "        for i, (x, _) in enumerate(train):\n",
    "            x = x.to(device) # data on GPU \n",
    "            self.optimizer.zero_grad() # reset all previous gradients\n",
    "            x_hat, latent_distribution, latent_sample = model(x)\n",
    "            loss = self.loss_function(x_hat, x, *latent_distribution, True, i)\n",
    "            loss.backward() # backpropagate (i.e store gradients)\n",
    "            train_loss += loss.item() # compute loss (.item because only the value)\n",
    "            self.optimizer.step() # take optimizing step (~gradient descent)\n",
    "\n",
    "        print('Epoch: {} Train loss: {:.4f}'.format(\n",
    "              self.epoch, train_loss / len(train.dataset)))\n",
    "        \n",
    "    def _test_epoch(self, test):\n",
    "        model.eval() # make sure evaluate mode (e.g. dropout)\n",
    "        test_loss = 0\n",
    "        with torch.no_grad():  # stop gradients computation\n",
    "            for i, (x, _) in enumerate(test):\n",
    "                x = x.to(device)\n",
    "                x_hat, latent_distribution, latent_sample = model(x)\n",
    "                test_loss += loss_function(x_hat, x, *latent_distribution, False, i).item()\n",
    "\n",
    "        print('Test loss: {:.4f}'.format(test_loss/len(test.dataset)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer = Trainer(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# trainer(train_loader, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(model.state_dict(), './model/betaVAE.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('./model/betaVAE.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim.lr_scheduler import StepLR\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, 3, 1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, 1)\n",
    "        self.dropout1 = nn.Dropout2d(0.25)\n",
    "        self.dropout2 = nn.Dropout2d(0.5)\n",
    "        self.fc1 = nn.Linear(12544, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = self.dropout1(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.fc2(x)\n",
    "        output = F.log_softmax(x, dim=1)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, device, train_loader, optimizer, epoch):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "#         data_hat,_,_ = vae_model(data)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.nll_loss(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, vae_model, device, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            data_hat,_,_ = vae_model(data)\n",
    "            output = model(data_hat)\n",
    "            test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n",
    "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_batch_size = 1000\n",
    "batch_size = 64\n",
    "epochs = 2\n",
    "lr = 0.1\n",
    "gamma = 0.7\n",
    "seed = 1\n",
    "no_cuda = False\n",
    "log_interval = 1000\n",
    "save_model = True\n",
    "use_cuda = True\n",
    "torch.manual_seed(seed)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else 'cpu')\n",
    "kwargs = {'num_workers': 1, 'pin_memory': True} if use_cuda else {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_classification = Net().to(device)\n",
    "optimizer = optim.Adadelta(model_classification.parameters(), lr=lr)\n",
    "\n",
    "scheduler = StepLR(optimizer, step_size=1, gamma=gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for epoch in range(1, epochs + 1):\n",
    "#     train(model_classification, device, train_loader, optimizer, epoch)\n",
    "#     test(model_classification, model, device, test_loader)\n",
    "#     scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SiameseDiscriminator(nn.Module):\n",
    "    def __init__(self, image_size):\n",
    "        super(SiameseDiscriminator, self).__init__()\n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.Conv2d(1, 4, kernel_size=3),\n",
    "            nn.LeakyReLU(0.1, inplace=True),\n",
    "            nn.BatchNorm2d(4),\n",
    "            nn.Dropout2d(p=.2),\n",
    "\n",
    "            nn.Conv2d(4, 8, kernel_size=3),\n",
    "            nn.LeakyReLU(0.1, inplace=True),\n",
    "            nn.BatchNorm2d(8),\n",
    "            nn.Dropout2d(p=.2),\n",
    "\n",
    "            nn.Conv2d(8, 8, kernel_size=3),\n",
    "            nn.LeakyReLU(0.1, inplace=True),\n",
    "            nn.BatchNorm2d(8),\n",
    "            nn.Dropout2d(p=.2)\n",
    "        )\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(8 * 26 * 26, 500),\n",
    "            nn.LeakyReLU(0.1, inplace=True),\n",
    "\n",
    "            nn.Linear(500, 500),\n",
    "            nn.LeakyReLU(0.1, inplace=True),\n",
    "\n",
    "            nn.Linear(500, 15)\n",
    "        )\n",
    "    \n",
    "    def forward_once(self, x):\n",
    "        \"\"\"Define the computation performed at every call by one side of siamese network.\"\"\"\n",
    "#         x = x_.unsqueeze(0)\n",
    "#         print(x.shape)\n",
    "        output = self.cnn(x)\n",
    "        output = output.view(output.size()[0], -1)\n",
    "        output = self.fc(output)\n",
    "        return output\n",
    "    \n",
    "    def forward(self, input1, input2):\n",
    "        \"\"\"Define the computation performed at every call.\"\"\"\n",
    "        output1 = self.forward_once(input1)\n",
    "        output2 = self.forward_once(input2)\n",
    "        return output1, output2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SiameseLoss(nn.Module):\n",
    "    def __init__(self, margin):\n",
    "        super(Loss, self).__init__()\n",
    "        self.margin = margin\n",
    "    \n",
    "    def forward(self, output1, output2, label):\n",
    "        \"\"\"Define the computation performed at every call.\"\"\"\n",
    "        euclidean_distance = F.pairwise_distance(output1, output2)\n",
    "        distance_from_margin = torch.clamp(torch.pow(euclidean_distance, 2) - self.margin, max=50.0)\n",
    "        exp_distance_from_margin = torch.exp(distance_from_margin)\n",
    "        distance_based_loss = (1.0 + math.exp(-self.margin)) / (1.0 + exp_distance_from_margin)\n",
    "        similar_loss = -0.5 * (1 - label) * torch.log(distance_based_loss)\n",
    "        dissimilar_loss = -0.5 * label * torch.log(1.0 - distance_based_loss)\n",
    "        return torch.mean(similar_loss + dissimilar_loss)\n",
    "    \n",
    "    def predict(self, output1, output2, threshold_factor=0.5):\n",
    "        \"\"\"Predict a dissimilarity label given two embeddings.\n",
    "        Return `1` if dissimilar.\n",
    "        \"\"\"\n",
    "        return F.pairwise_distance(output1, output2) > self.margin * threshold_factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DistanceBasedLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Distance based loss function.\n",
    "    For reference see:\n",
    "    Hadsell et al., CVPR'06\n",
    "    Chopra et al., CVPR'05\n",
    "    Vo and Hays, ECCV'16\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, margin):\n",
    "        \"\"\"Set parameters of distance-based loss function.\"\"\"\n",
    "        super(DistanceBasedLoss, self).__init__()\n",
    "        self.margin = margin\n",
    "\n",
    "    def forward(self, output1, output2, label):\n",
    "        \"\"\"Define the computation performed at every call.\"\"\"\n",
    "        euclidean_distance = F.pairwise_distance(output1, output2)\n",
    "        distance_from_margin = torch.clamp(torch.pow(euclidean_distance, 2) - self.margin, max=50.0)\n",
    "        exp_distance_from_margin = torch.exp(distance_from_margin)\n",
    "        distance_based_loss = (1.0 + math.exp(-self.margin)) / (1.0 + exp_distance_from_margin)\n",
    "        similar_loss = -0.5 * (1 - label) * torch.log(distance_based_loss)\n",
    "        dissimilar_loss = -0.5 * label * torch.log(1.0 - distance_based_loss)\n",
    "        return torch.mean(similar_loss + dissimilar_loss)\n",
    "\n",
    "    def predict(self, output1, output2, threshold_factor=0.5):\n",
    "        \"\"\"Predict a dissimilarity label given two embeddings.\n",
    "        Return `1` if dissimilar.\n",
    "        \"\"\"\n",
    "        return F.pairwise_distance(output1, output2) > self.margin * threshold_factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContrastiveLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Contrastive loss function.\n",
    "    Based on: http://yann.lecun.com/exdb/publis/pdf/hadsell-chopra-lecun-06.pdf\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, margin):\n",
    "        \"\"\"Set parameters of contrastive loss function.\"\"\"\n",
    "        super(ContrastiveLoss, self).__init__()\n",
    "        self.margin = margin\n",
    "\n",
    "    def forward(self, output1, output2, label):\n",
    "        \"\"\"Define the computation performed at every call.\"\"\"\n",
    "        euclidean_distance = F.pairwise_distance(output1, output2)\n",
    "        clamped = torch.clamp(self.margin - euclidean_distance, min=0.0)\n",
    "        similar_loss = (1 - label) * 0.5 * torch.pow(euclidean_distance, 2)\n",
    "        dissimilar_loss = label * 0.5 * torch.pow(clamped, 2)\n",
    "        contrastive_loss = similar_loss + dissimilar_loss\n",
    "\n",
    "        return torch.mean(contrastive_loss)\n",
    "\n",
    "    def predict(self, output1, output2, threshold_factor=0.5):\n",
    "        \"\"\"Predict a dissimilarity label given two embeddings.\n",
    "        Return `1` if dissimilar.\n",
    "        \"\"\"\n",
    "        return F.pairwise_distance(output1, output2) > self.margin * threshold_factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import exp\n",
    "\n",
    "\n",
    "def gaussian(window_size, sigma):\n",
    "    \"\"\"Compute gaussian window, that is a tensor with values of the bell curve.\"\"\"\n",
    "    gauss = torch.Tensor(\n",
    "        [exp(-(x - window_size//2)**2/float(2*sigma**2)) for x in range(window_size)])\n",
    "    return gauss/gauss.sum()\n",
    "\n",
    "\n",
    "def create_window(window_size, channel):\n",
    "    \"\"\"Generate a two dimensional window with desired number of channels.\"\"\"\n",
    "    _1D_window = gaussian(window_size, 1.5).unsqueeze(1)\n",
    "    _2D_window = _1D_window.mm(_1D_window.t()).float().unsqueeze(0).unsqueeze(0)\n",
    "    window = torch.Tensor(_2D_window.expand(channel, 1, window_size, window_size).contiguous())\n",
    "    return window\n",
    "\n",
    "\n",
    "def _ssim(img1, img2, window, window_size, channel, size_average=True):\n",
    "    \"\"\"Compute the structural similarity index between two images.\"\"\"\n",
    "    mu1 = F.conv2d(img1, window, padding=window_size//2, groups=channel)\n",
    "    mu2 = F.conv2d(img2, window, padding=window_size//2, groups=channel)\n",
    "\n",
    "    mu1_sq = mu1.pow(2)\n",
    "    mu2_sq = mu2.pow(2)\n",
    "    mu1_mu2 = mu1*mu2\n",
    "\n",
    "    sigma1_sq = F.conv2d(img1*img1, window, padding=window_size//2, groups=channel) - mu1_sq\n",
    "    sigma2_sq = F.conv2d(img2*img2, window, padding=window_size//2, groups=channel) - mu2_sq\n",
    "    sigma12 = F.conv2d(img1*img2, window, padding=window_size//2, groups=channel) - mu1_mu2\n",
    "\n",
    "    C1 = 0.01**2\n",
    "    C2 = 0.03**2\n",
    "\n",
    "    ssim_map = ((2*mu1_mu2 + C1)*(2*sigma12 + C2))/((mu1_sq + mu2_sq + C1) *\n",
    "                                                    (sigma1_sq + sigma2_sq + C2))\n",
    "\n",
    "    if size_average:\n",
    "        return ssim_map.mean()\n",
    "    else:\n",
    "        return ssim_map.mean(1).mean(1).mean(1)\n",
    "\n",
    "\n",
    "class SSIM(nn.Module):\n",
    "    \"\"\"Wrapper class used to compute the structural similarity index.\"\"\"\n",
    "\n",
    "    def __init__(self, window_size=11, size_average=True):\n",
    "        super(SSIM, self).__init__()\n",
    "        self.window_size = window_size\n",
    "        self.size_average = size_average\n",
    "        self.channel = 1\n",
    "        self.window = create_window(window_size, self.channel)\n",
    "\n",
    "    def forward(self, img1, img2):\n",
    "        \"\"\"Execute the computation of the structural similarity index.\"\"\"\n",
    "        (_, channel, _, _) = img1.size()\n",
    "\n",
    "        if channel == self.channel and self.window.data.type() == img1.data.type():\n",
    "            window = self.window\n",
    "        else:\n",
    "            window = create_window(self.window_size, channel)\n",
    "\n",
    "            if img1.is_cuda:\n",
    "                window = window.cuda(img1.get_device())\n",
    "            window = window.type_as(img1)\n",
    "\n",
    "            self.window = window\n",
    "            self.channel = channel\n",
    "\n",
    "        return _ssim(img1, img2, window, self.window_size, channel, self.size_average)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data.sampler import BatchSampler\n",
    "\n",
    "\n",
    "class SiameseMNIST(Dataset):\n",
    "    \"\"\"\n",
    "    Train: For each sample creates randomly a positive or a negative pair\n",
    "    Test: Creates fixed pairs for testing\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, mnist_dataset):\n",
    "        self.mnist_dataset = mnist_dataset\n",
    "\n",
    "        self.train = self.mnist_dataset.train\n",
    "        self.transform = self.mnist_dataset.transform\n",
    "\n",
    "        if self.train:\n",
    "            self.train_labels = self.mnist_dataset.targets\n",
    "            self.train_data = self.mnist_dataset.data\n",
    "            self.labels_set = set(self.train_labels.numpy())\n",
    "            self.label_to_indices = {label: np.where(self.train_labels.numpy() == label)[0]\n",
    "                                     for label in self.labels_set}\n",
    "        else:\n",
    "            # generate fixed pairs for testing\n",
    "            self.test_labels = self.mnist_dataset.targets\n",
    "            self.test_data = self.mnist_dataset.data\n",
    "            self.labels_set = set(self.test_labels.numpy())\n",
    "            self.label_to_indices = {label: np.where(self.test_labels.numpy() == label)[0]\n",
    "                                     for label in self.labels_set}\n",
    "\n",
    "            random_state = np.random.RandomState(29)\n",
    "\n",
    "            positive_pairs = [[i,\n",
    "                               random_state.choice(self.label_to_indices[self.test_labels[i].item()]),\n",
    "                               1]\n",
    "                              for i in range(0, len(self.test_data), 2)]\n",
    "\n",
    "            negative_pairs = [[i,\n",
    "                               random_state.choice(self.label_to_indices[\n",
    "                                                       np.random.choice(\n",
    "                                                           list(self.labels_set - set([self.test_labels[i].item()]))\n",
    "                                                       )\n",
    "                                                   ]),\n",
    "                               0]\n",
    "                              for i in range(1, len(self.test_data), 2)]\n",
    "            self.test_pairs = positive_pairs + negative_pairs\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        if self.train:\n",
    "            target = np.random.randint(0, 2)\n",
    "            img1, label1 = self.train_data[index], self.train_labels[index].item()\n",
    "            if target == 1:\n",
    "                siamese_index = index\n",
    "                while siamese_index == index:\n",
    "                    siamese_index = np.random.choice(self.label_to_indices[label1])\n",
    "            else:\n",
    "                siamese_label = np.random.choice(list(self.labels_set - set([label1])))\n",
    "                siamese_index = np.random.choice(self.label_to_indices[siamese_label])\n",
    "            img2 = self.train_data[siamese_index]\n",
    "        else:\n",
    "            img1 = self.test_data[self.test_pairs[index][0]]\n",
    "            img2 = self.test_data[self.test_pairs[index][1]]\n",
    "            target = self.test_pairs[index][2]\n",
    "\n",
    "        img1 = Image.fromarray(img1.numpy(), mode='L')\n",
    "        img2 = Image.fromarray(img2.numpy(), mode='L')\n",
    "        if self.transform is not None:\n",
    "            img1 = self.transform(img1)\n",
    "            img2 = self.transform(img2)\n",
    "        return torch.FloatTensor([target]), img1, img2\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.mnist_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_siamese_dataset = SiameseMNIST(mnist_dataset)\n",
    "mnist_siamese_dataset_test = SiameseMNIST(mnist_dataset_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    def __init__(self, image_size=32, mode='train', model_path='./model/', \n",
    "                 generate_path='./model/', num_epochs=100, distance_weight=1.0, dataset='MNIST', \n",
    "                 tensorboard=True, generator=model, batch_size=64, batch_size_test=1000):\n",
    "        self.mode = mode\n",
    "        self.image_size = image_size\n",
    "        self.model_path = model_path\n",
    "        self.generate_path = generate_path\n",
    "        self.dataset = dataset\n",
    "        self.num_epochs = num_epochs\n",
    "        self.distance_weight = distance_weight\n",
    "        self.tensorboard = tensorboard\n",
    "        self.generator = generator\n",
    "        self.batch_size = batch_size\n",
    "        self.batch_size_test = batch_size_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "class SiameseGanSolver(object):\n",
    "    \"\"\"Solving GAN-like neural network with siamese discriminator.\"\"\"\n",
    "\n",
    "    def __init__(self, config, data_loader):\n",
    "        \"\"\"Set parameters of neural network and its training.\"\"\"\n",
    "        self.ssim_loss = SSIM()\n",
    "        self.generator = config.generator\n",
    "        self.discriminator = None\n",
    "        self.distance_based_loss = None\n",
    "\n",
    "        self.g_optimizer = None\n",
    "        self.d_optimizer = None\n",
    "\n",
    "        self.g_conv_dim = 128\n",
    "\n",
    "        self.beta1 = 0.9\n",
    "        self.beta2 = 0.999\n",
    "        self.learning_rate = 0.0001\n",
    "        self.image_size = config.image_size\n",
    "        self.num_epochs = config.num_epochs\n",
    "        self.distance_weight = config.distance_weight\n",
    "\n",
    "        self.data_loader = data_loader\n",
    "#         print(self.data_loader.dataset)\n",
    "        self.generate_path = config.generate_path\n",
    "        self.model_path = config.model_path\n",
    "        self.tensorboard = config.tensorboard\n",
    "\n",
    "        if self.tensorboard:\n",
    "            self.tb_writer = tensorboardX.SummaryWriter(\n",
    "                filename_suffix='_%s_%s' % (config.distance_weight, config.dataset))\n",
    "            self.tb_graph_added = False\n",
    "\n",
    "        self.build_model()\n",
    "\n",
    "    def build_model(self):\n",
    "        \"\"\"Build generator and discriminator.\"\"\"\n",
    "#         self.generator = Generator(self.g_conv_dim, noise=self.noise, residual=self.residual)\n",
    "        self.discriminator = SiameseDiscriminator(self.image_size)\n",
    "        self.distance_based_loss = DistanceBasedLoss(2.0)\n",
    "\n",
    "        self.g_optimizer = torch.optim.Adam(\n",
    "            self.generator.parameters(), self.learning_rate, [self.beta1, self.beta2])\n",
    "        self.d_optimizer = torch.optim.Adam(\n",
    "            self.discriminator.parameters(), self.learning_rate, [self.beta1, self.beta2])\n",
    "\n",
    "        if torch.cuda.is_available():\n",
    "            self.generator.cuda()\n",
    "            self.discriminator.cuda()\n",
    "            self.distance_based_loss.cuda()\n",
    "\n",
    "    def train(self):\n",
    "        \"\"\"Train generator and discriminator in minimax game.\"\"\"\n",
    "        # Prepare tensorboard writer\n",
    "        if self.tensorboard:\n",
    "            step = 0\n",
    "        \n",
    "        print(\"We are training\\n\")\n",
    "\n",
    "        for epoch in tqdm(range(self.num_epochs)):\n",
    "            print(str(epoch) + \" \" + str(datetime.now()))\n",
    "#             i = 0\n",
    "            for label, images0, images1 in self.data_loader:\n",
    "#                 i += 1\n",
    "#                 print(i)\n",
    "                images0 = to_variable(images0)\n",
    "                images1 = to_variable(images1)\n",
    "#                 print(\"label:\", label)\n",
    "                label = to_variable(label)\n",
    "#                 print(\"We extracted samples\")\n",
    "                # Train discriminator to recognize identity of real images\n",
    "                output0, output1 = self.discriminator(images0, images1)\n",
    "                d_real_loss = self.distance_based_loss(output0, output1, label)\n",
    "#                 print(\"We calculated loss\")\n",
    "                # Backpropagation\n",
    "                self.distance_based_loss.zero_grad()\n",
    "                self.discriminator.zero_grad()\n",
    "                d_real_loss.backward()\n",
    "                self.d_optimizer.step()\n",
    "#                 print(\"We did backprop\")\n",
    "                # Train discriminator to recognize identity of fake(privatized) images\n",
    "                \n",
    "                privatized_imgs, _, _ = self.generator(images0)\n",
    "#                 print(privatized_imgs)\n",
    "                output0, output1 = self.discriminator(images0, privatized_imgs)\n",
    "\n",
    "                # Discriminator wants to minimize Euclidean distance between\n",
    "                # original & privatized versions, hence label = 0\n",
    "                d_fake_loss = self.distance_based_loss(output0, output1, 0)\n",
    "                distance = 1.0 - self.ssim_loss(privatized_imgs, images0)\n",
    "                d_fake_loss += self.distance_weight * distance\n",
    "#                 print(\"We calculated loss\")\n",
    "                # Backpropagation\n",
    "                self.distance_based_loss.zero_grad()\n",
    "                self.discriminator.zero_grad()\n",
    "                self.generator.zero_grad()\n",
    "                d_fake_loss.backward()\n",
    "                self.d_optimizer.step()\n",
    "\n",
    "                # Train generator to fool discriminator\n",
    "                # Generator wants to push the distance between original & privatized\n",
    "                # right to the margin, hence label = 1\n",
    "                privatized_imgs, _, _ = self.generator(images0)\n",
    "                output0, output1 = self.discriminator(images0, privatized_imgs)\n",
    "                g_loss = self.distance_based_loss(output0, output1, 1)\n",
    "                distance = 1.0 - self.ssim_loss(privatized_imgs, images0)\n",
    "                g_loss += self.distance_weight * distance\n",
    "#                 print(\"We calculated loss\")\n",
    "                # Backpropagation\n",
    "                self.distance_based_loss.zero_grad()\n",
    "                self.discriminator.zero_grad()\n",
    "                self.generator.zero_grad()\n",
    "                g_loss.backward()\n",
    "                self.g_optimizer.step()\n",
    "\n",
    "                # Write losses to tensorboard\n",
    "                if self.tensorboard:\n",
    "                    self.tb_writer.add_scalar('phase0/discriminator_real_loss',\n",
    "                                              d_real_loss.item(), step)\n",
    "                    self.tb_writer.add_scalar('phase0/discriminator_fake_loss',\n",
    "                                              d_fake_loss.item(), step)\n",
    "                    self.tb_writer.add_scalar('phase0/generator_loss',\n",
    "                                              g_loss.item(), step)\n",
    "                    self.tb_writer.add_scalar('phase0/distance_loss',\n",
    "                                              distance.item(), step)\n",
    "\n",
    "                    step += 1\n",
    "\n",
    "            # Monitor training after each epoch\n",
    "            if self.tensorboard:\n",
    "                self._monitor_phase_0(self.tb_writer, step)\n",
    "\n",
    "            # At the end save generator and discriminator to files\n",
    "            if (epoch + 1) % 10 == 0:\n",
    "                g_path = os.path.join(self.model_path, 'generator-%d.pt' % (epoch+1))\n",
    "                torch.save(self.generator.state_dict(), g_path)\n",
    "                d_path = os.path.join(self.model_path, 'discriminator-%d.pt' % (epoch+1))\n",
    "                torch.save(self.discriminator.state_dict(), d_path)\n",
    "\n",
    "        if self.tensorboard:\n",
    "            self.tb_writer.close()\n",
    "\n",
    "    def _monitor_phase_0(self, writer, step, n_images=10):\n",
    "        \"\"\"Monitor discriminator's accuracy, generate preview images of generator.\"\"\"\n",
    "        # Measure accuracy of identity verification by discriminator\n",
    "        correct_pairs = 0\n",
    "        total_pairs = 0\n",
    "\n",
    "        for label, images0, images1 in self.data_loader:\n",
    "            images0 = to_variable(images0)\n",
    "            images1 = to_variable(images1)\n",
    "            label = to_variable(label)\n",
    "\n",
    "            # Predict label = 1 if outputs are dissimilar (distance > margin)\n",
    "            privatized_images0, _, _ = self.generator(images0)\n",
    "            output0, output1 = self.discriminator(privatized_images0, images1)\n",
    "            predictions = self.distance_based_loss.predict(output0, output1)\n",
    "            predictions = predictions.type(label.data.type())\n",
    "\n",
    "            correct_pairs += (predictions == label).sum().item()\n",
    "            total_pairs += len(predictions == label)\n",
    "\n",
    "            if total_pairs > 1000:\n",
    "                break\n",
    "\n",
    "        # Write accuracy to tensorboard\n",
    "        accuracy = correct_pairs / total_pairs\n",
    "        writer.add_scalar('phase0/discriminator_accuracy', accuracy, step)\n",
    "\n",
    "        # Generate previews of privatized images\n",
    "        reals, fakes = [], []\n",
    "        for _, image, _ in self.data_loader.dataset:\n",
    "#             print(\"i: \", image.shape)\n",
    "            g_image, _, _ = self.generator(to_variable(image).unsqueeze(0))\n",
    "            g_image = g_image.squeeze(0)\n",
    "#             print(\"g: \", g_image.shape)\n",
    "            reals.append(denorm(to_variable(image).data[0]))\n",
    "            fakes.append(denorm(to_variable(g_image).data[0]))\n",
    "            if len(reals) == n_images:\n",
    "                break\n",
    "\n",
    "        # Write images to tensorboard\n",
    "        real_previews = torchvision.utils.make_grid(reals, nrow=n_images)\n",
    "        fake_previews = torchvision.utils.make_grid(fakes, nrow=n_images)\n",
    "#         print(real_previews.shape)\n",
    "#         print(fake_previews.shape)\n",
    "#         img = torchvision.utils.make_grid([real_previews, fake_previews], nrow=1)\n",
    "        img = torchvision.utils.make_grid(torch.stack(\n",
    "            [*real_previews.unsqueeze_(1).unbind(0), *fake_previews.unsqueeze_(1).unbind(0)]), nrow=1)\n",
    "        writer.add_image('Previews', img, step)\n",
    "\n",
    "    def generate(self):\n",
    "        \"\"\"Generate privatized images.\"\"\"\n",
    "        # Load trained parameters (generator)\n",
    "        g_path = os.path.join(self.model_path, 'generator-%d.pkl' % self.num_epochs)\n",
    "        self.generator.load_state_dict(torch.load(g_path))\n",
    "        self.generator.eval()\n",
    "\n",
    "        # Generate the images\n",
    "        for relative_path, image in self.data_loader:\n",
    "            fake_image, _, _ = self.generator(to_variable(image))\n",
    "            fake_path = os.path.join(self.generate_path, relative_path[0])\n",
    "            if not os.path.exists(os.path.dirname(fake_path)):\n",
    "                os.makedirs(os.path.dirname(fake_path))\n",
    "            torchvision.utils.save_image(fake_image.data, fake_path, nrow=1)\n",
    "\n",
    "    def check_discriminator_accuracy(self):\n",
    "        \"\"\"Measure discriminator's accuracy.\"\"\"\n",
    "        # Measure accuracy of identity verification by discriminator\n",
    "        correct_pairs = 0\n",
    "        total_pairs = 0\n",
    "\n",
    "        g_path = os.path.join(self.model_path, 'generator-%d.pkl' % self.num_epochs)\n",
    "        self.generator.load_state_dict(torch.load(g_path))\n",
    "        self.generator.eval()\n",
    "\n",
    "        d_path = os.path.join(self.model_path, 'discriminator-%d.pkl' % self.num_epochs)\n",
    "        self.discriminator.load_state_dict(torch.load(d_path))\n",
    "        self.discriminator.eval()\n",
    "\n",
    "        for label, images0, images1 in self.data_loader:\n",
    "            images0 = to_variable(images0)\n",
    "            images1 = to_variable(images1)\n",
    "            label = to_variable(label)\n",
    "\n",
    "            # Predict label = 1 if outputs are dissimilar (distance > margin)\n",
    "            privatized_images0, _, _ = self.generator(images0)\n",
    "            output0, output1 = self.discriminator(privatized_images0, images1)\n",
    "            predictions = self.distance_based_loss.predict(output0, output1)\n",
    "            predictions = predictions.type(label.data.type())\n",
    "\n",
    "            correct_pairs += (predictions == label).sum().item()\n",
    "            total_pairs += len(predictions)\n",
    "\n",
    "        accuracy = correct_pairs / total_pairs\n",
    "        print('distance weight = %f' % self.distance_weight)\n",
    "        print('accuracy = %f' % accuracy)\n",
    "        \n",
    "def to_variable(tensor):\n",
    "    \"\"\"Convert tensor to variable.\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        tensor = tensor.cuda()\n",
    "    return tensor\n",
    "\n",
    "def denorm(image):\n",
    "    \"\"\"Convert image range (-1, 1) to (0, 1).\"\"\"\n",
    "    out = (image + 1) / 2\n",
    "    return out.clamp(0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, time\n",
    "import tensorboardX, math, os\n",
    "from torch.utils.data import DataLoader\n",
    "config = Config(num_epochs=200, tensorboard=True)\n",
    "siamese_data_loader = DataLoader(dataset=mnist_siamese_dataset, batch_size=config.batch_size, shuffle=True)\n",
    "siamese_data_loader_test = DataLoader(dataset=mnist_siamese_dataset_test, batch_size=config.batch_size_test, shuffle=True)\n",
    "solver = SiameseGanSolver(config, siamese_data_loader)\n",
    "date1 = datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We are training\n",
      "\n",
      "0 2020-01-08 01:22:07.404951\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100 [00:05<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<timed eval>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-33-d8c944187a66>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    103\u001b[0m                 \u001b[0;31m# right to the margin, hence label = 1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m                 \u001b[0mprivatized_imgs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 105\u001b[0;31m                 \u001b[0moutput0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiscriminator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprivatized_imgs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    106\u001b[0m                 \u001b[0mg_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistance_based_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m                 \u001b[0mdistance\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1.0\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mssim_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprivatized_imgs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimages0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/miniconda3/envs/bvaa/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-25-90240f249361>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input1, input2)\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0;34m\"\"\"Define the computation performed at every call.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m         \u001b[0moutput1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward_once\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m         \u001b[0moutput2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward_once\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0moutput1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-25-90240f249361>\u001b[0m in \u001b[0;36mforward_once\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;31m#         x = x_.unsqueeze(0)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;31m#         print(x.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/miniconda3/envs/bvaa/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/miniconda3/envs/bvaa/lib/python3.6/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/miniconda3/envs/bvaa/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/miniconda3/envs/bvaa/lib/python3.6/site-packages/torch/nn/modules/batchnorm.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     79\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunning_mean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunning_var\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrack_running_stats\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m             exponential_average_factor, self.eps)\n\u001b[0m\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/miniconda3/envs/bvaa/lib/python3.6/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mbatch_norm\u001b[0;34m(input, running_mean, running_var, weight, bias, training, momentum, eps)\u001b[0m\n\u001b[1;32m   1668\u001b[0m     return torch.batch_norm(\n\u001b[1;32m   1669\u001b[0m         \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrunning_mean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrunning_var\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1670\u001b[0;31m         \u001b[0mtraining\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmomentum\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackends\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcudnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menabled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1671\u001b[0m     )\n\u001b[1;32m   1672\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "solver.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'SiameseGanSolver' object has no attribute 'state_dict'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-37-df293912f047>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msolver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'./model/solver.pt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'SiameseGanSolver' object has no attribute 'state_dict'"
     ]
    }
   ],
   "source": [
    "torch.save(solver.state_dict(), './model/solver.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
