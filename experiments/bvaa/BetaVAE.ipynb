{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "\n",
    "from torchvision import datasets, transforms\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "from torch import nn\n",
    "from torchvision.utils import save_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('../../data', \n",
    "                   train=True, \n",
    "                   download=True, \n",
    "                   transform=transforms.Compose([\n",
    "                       transforms.Resize(32),\n",
    "                       transforms.ToTensor()\n",
    "                   ])),batch_size=batch_size, shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('../../data', train=False, download=True, transform=transforms.Compose([\n",
    "                       transforms.Resize(32),\n",
    "                       transforms.ToTensor()\n",
    "                   ])),batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Encoder(nn.Module):\n",
    "#     def __init__(self, x_size, h_size, z_size):\n",
    "#         super(Encoder, self).__init__()\n",
    "#         self.fc1 = nn.Linear(x_size, h_size)\n",
    "#         self.mu_gen = nn.Linear(h_size, z_size)\n",
    "#         # make the output to be the logarithm \n",
    "#         # i.e will have to take the exponent\n",
    "#         # which forces variance to be positive\n",
    "#         # not that this is the diagonal of the covariance\n",
    "#         self.log_var_gen = nn.Linear(h_size, z_size)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = F.relu(self.fc1(x))\n",
    "#         mu = self.mu_gen(x)\n",
    "#         log_var = self.log_var_gen(x)\n",
    "#         return mu, log_var\n",
    "    \n",
    "class Encoder(nn.Module):\n",
    "    r\"\"\"\n",
    "    References:\n",
    "            [1] Burgess, Christopher P., et al. \"Understanding disentangling in\n",
    "            $\\beta$-VAE.\" arXiv preprint arXiv:1804.03599 (2018).\n",
    "    \"\"\"\n",
    "    def __init__(self, img_size, latent_dim=10):\n",
    "\n",
    "        super(Encoder, self).__init__()\n",
    "\n",
    "        hidden_channels = 32\n",
    "        kernel_size = 4\n",
    "        hidden_dim = 256\n",
    "\n",
    "        self.latent_dim = latent_dim\n",
    "        self.img_size = img_size\n",
    "\n",
    "        self.reshape = (hidden_channels, kernel_size, kernel_size)\n",
    "\n",
    "        n_channels = self.img_size[0]\n",
    "\n",
    "        cnn_kwargs = dict(stride=2, padding=1)\n",
    "\n",
    "        self.conv1 = nn.Conv2d(n_channels, hidden_channels, kernel_size, **cnn_kwargs)\n",
    "        self.conv2 = nn.Conv2d(hidden_channels, hidden_channels, kernel_size, **cnn_kwargs)\n",
    "        self.conv3 = nn.Conv2d(hidden_channels, hidden_channels, kernel_size, **cnn_kwargs)\n",
    "\n",
    "        if self.img_size[1] == self.img_size[2] == 64:\n",
    "            self.conv_64 = nn.Conv2d(hidden_channels, hidden_channels, kernel_size, **cnn_kwargs)\n",
    "\n",
    "        self.lin1 = nn.Linear(np.product(self.reshape), hidden_dim)\n",
    "        self.lin2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "\n",
    "        self.mu_logvar_gen = nn.Linear(hidden_dim, self.latent_dim * 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "\n",
    "        x = torch.relu(self.conv1(x))\n",
    "        x = torch.relu(self.conv2(x))\n",
    "        x = torch.relu(self.conv3(x))\n",
    "\n",
    "        if self.img_size[1] == self.img_size[2] == 64:\n",
    "            x = torch.relu(self.conv_64(x))\n",
    "\n",
    "        x = x.view((batch_size, -1))\n",
    "        x = torch.relu(self.lin1(x))\n",
    "        x = torch.relu(self.lin2(x))\n",
    "\n",
    "        mu_logvar = self.mu_logvar_gen(x)\n",
    "\n",
    "        mu, logvar = mu_logvar.view(-1, self.latent_dim, 2).unbind(-1)\n",
    "\n",
    "        return mu, logvar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Decoder(nn.Module):\n",
    "#     def __init__(self, x_size, h_size, z_size):\n",
    "#         super(Decoder, self).__init__()\n",
    "#         self.fc1 = nn.Linear(z_size, h_size)\n",
    "#         self.fc3 = nn.Linear(h_size, x_size)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = F.relu(self.fc1(x))\n",
    "#         x = F.relu(self.fc3(x))\n",
    "#         # black and white MNIST => sigmoid for each pixel\n",
    "#         x = torch.sigmoid(x) \n",
    "#         return x\n",
    "    \n",
    "class Decoder(nn.Module):\n",
    "    r\"\"\"\n",
    "    References:\n",
    "            [1] Burgess, Christopher P., et al. \"Understanding disentangling in\n",
    "            $\\beta$-VAE.\" arXiv preprint arXiv:1804.03599 (2018).\n",
    "    \"\"\"\n",
    "    def __init__(self, img_size, latent_dim):\n",
    "\n",
    "        super(Decoder, self).__init__()\n",
    "\n",
    "        hidden_channels = 32\n",
    "        kernel_size = 4\n",
    "        hidden_dim = 256\n",
    "\n",
    "        self.img_size = img_size\n",
    "        self.latent_dim = latent_dim\n",
    "\n",
    "        self.reshape = (hidden_channels, kernel_size, kernel_size)\n",
    "\n",
    "        n_channels = self.img_size[0]\n",
    "\n",
    "        self.lin1 = nn.Linear(latent_dim, hidden_dim)\n",
    "        self.lin2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.lin3 = nn.Linear(hidden_dim, np.product(self.reshape))\n",
    "\n",
    "        cnn_kwargs = dict(stride=2, padding=1)\n",
    "\n",
    "        if self.img_size[1] == self.img_size[2] == 64:\n",
    "            self.convT_64 = nn.ConvTranspose2d(hidden_channels, hidden_channels, kernel_size, **cnn_kwargs)\n",
    "\n",
    "        self.convT1 = nn.ConvTranspose2d(hidden_channels, hidden_channels, kernel_size, **cnn_kwargs)\n",
    "        self.convT2 = nn.ConvTranspose2d(hidden_channels, hidden_channels, kernel_size, **cnn_kwargs)\n",
    "        self.convT3 = nn.ConvTranspose2d(hidden_channels, n_channels, kernel_size, **cnn_kwargs)\n",
    "\n",
    "    def forward(self, z):\n",
    "        batch_size = z.size(0)\n",
    "\n",
    "        x = torch.relu(self.lin1(z))\n",
    "        x = torch.relu(self.lin2(x))\n",
    "        x = torch.relu(self.lin3(x))\n",
    "\n",
    "        x = x.view(batch_size, *self.reshape)\n",
    "\n",
    "        if self.img_size[1] == self.img_size[2] == 64:\n",
    "            x = torch.relu(self.convT_64(x))\n",
    "\n",
    "        x = torch.relu(self.convT1(x))\n",
    "        x = torch.relu(self.convT2(x))\n",
    "\n",
    "        x = torch.sigmoid(self.convT3(x))\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class VAE(nn.Module):\n",
    "#     def __init__(self, x_size, h_size, z_size):\n",
    "#         super(VAE, self).__init__()\n",
    "#         self.x_size = x_size\n",
    "#         self.z_size = z_size\n",
    "#         self.encoder = Encoder(x_size, h_size, z_size)\n",
    "#         self.decoder = Decoder(x_size, h_size, z_size)\n",
    "\n",
    "#     def reparameterize(self, mu, log_var):\n",
    "#         std = torch.exp(0.5 * log_var) # square root in exponent => std\n",
    "#         eps = torch.randn_like(std)\n",
    "#         z = std * eps + mu\n",
    "#         return z\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         # make image linear (i.e vector form)\n",
    "#         x = x.view(-1, self.x_size)\n",
    "#         mu, log_var = self.encoder(x)\n",
    "#         z = self.reparameterize(mu, log_var)\n",
    "#         x_hat = self.decoder(z)\n",
    "#         return x_hat, mu, log_var\n",
    "\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self, img_size, latent_dim):\n",
    "        super(VAE, self).__init__()\n",
    "        if list(img_size[1:]) not in [[32,32], [64,64]]:\n",
    "            raise RuntimeError(\"{} sized images not supported. Only (None, 32, 32) and (None, 64, 64) supported. Build your own architecture or reshape images!\".format(img_size))\n",
    "\n",
    "        self.latent_dim = latent_dim\n",
    "        self.img_size = img_size\n",
    "        self.num_pixels = self.img_size[1] * self.img_size[2]\n",
    "        self.encoder = Encoder(img_size, self.latent_dim)\n",
    "        self.decoder = Decoder(img_size, self.latent_dim)\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reparameterize(self, mean, logvar):\n",
    "        if self.training:\n",
    "            std = torch.exp(0.5 * logvar)\n",
    "            eps = torch.randn_like(std)\n",
    "            return mean + std * eps\n",
    "\n",
    "        else:\n",
    "            return mean\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        latent_distribution = self.encoder(x)\n",
    "        latent_sample = self.reparameterize(*latent_distribution)\n",
    "        reconstruct = self.decoder(latent_sample)\n",
    "        return reconstruct, latent_distribution, latent_sample\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        self.apply(weights_init)\n",
    "\n",
    "    def sample_latent(self, x):\n",
    "        latent_distribution = self.encoder(x)\n",
    "        latent_sample = self.reparameterize(*latent_distribution)\n",
    "        return latent_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = enumerate(test_loader)\n",
    "batch_idx, (example_data, example_targets) = next(examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZQAAAELCAYAAAD+9XA2AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8li6FKAAAgAElEQVR4nO3deZCU1b0+8OeoiAjIIsgimyjGCAgiKrjhghgUtUCCmgRFS8uUN1oVNd54NaVcb5JfaYzXWOFq1dWIgkEQAReQRSUIKKKyCbIIgopsg+yLbOf3R/d8ed73ds9095xeZng+VVQ9Q/fb/Q59mDPv9z2L895DRESkqo4q9gmIiEjNoA5FRESCUIciIiJBqEMREZEg1KGIiEgQ6lBERCSIGt2hOOdWO+d6F/H9v3POXVqs95fcqe1Iro7ktlOlDsU5d5Nzbo5zbpdzbmMy3+2cc6FOMB+cc5OcczuTf/Y75/bR18/l+JojnHOPBT7V8td+2TnnnXPt8vH6xaC2E3nN4G3HOXeSc+6fzrltzrktzrmXQ75+MantRF4zaNtxzv2Bzmmnc26Pc+6gc65RJsfn3KE45+4H8AyAJwE0B9AMwK8BXAjg2DTHHJ3r+4Xkve/rva/nva8HYCSAJ8q/9t7/Ov5859wxhT9Le+9LAbQr1vvng9pOQUwA8C2A1gBOAvB0kc4jKLWdvJ/j43RO9QA8BeA97/2WTF8g6z8AGgDYBeCGSp73EoD/ATAx+fzeyWNfBrAJwBoAjwA4Kvn8xwCMoOPbAfAAjkl+PR3A4wBmAdgBYAqAJvT8wcnX3AzgYQCrAfTO4Bz/K/Z3vZPH/geA9QD+AeAOANPpOcckz60dgLsB7AewD8BOAOOSz/kOwH0AFgHYBuCfAGpn8e9cC8ACAF3K3yuXz6uU/qjt5L/tALgawMryf5ua8kdtpzA/d+i9XPL7+mWmx+R6hdITQG0kfguqzC8A/BFAfQAzATyLxIfbHkAvALcAuC2L9/5F8vknIfEbyQMA4Jw7E4lGNBhASwAnAmiVxevGtQJQD0AbJD64tLz3wwC8BuBPPtGz96eHBwG4Eonv95zk+cE5d7RzbqtzrkcFL/0AgGkAFuf8XZQetR2Sp7bTA8AyACOcc5udc5845y6qwvdTKtR2SB5/7pS7DEBDAOMyPflcO5QmAMq89wfK/8I5Nzt5onucc5fQcyd472d57w8h0ZveBOAh7/0O7/1qJC6pBmfx3v/w3i/33u8BMBpA1+TfDwTwtvd+hvf+RwB/AHAox+8PAA4AeMx7vy/5Xrn6b+/9eu/9ZgBvl5+v9/6g976h9/7jVAc559oCuB2J355qErWdzOXUdpD4odQXwGQkykLPAHjTOde4CudSCtR2Mpdr22G3Ahjjvd+d6Zvm2qFsBtCEa3ze+wu89w2Tj/Hrfku5CRJlnDX0d2sAnJzFe6+nvBuJ3hxI/HZg7+W935U8l1xt8N7vq8Lx5dKdb2X+BuBR7/2OAOdQStR2Mpdr29kD4Cvv/XDv/X7v/UgAG5D4Db86U9vJXK5tBwDgnKsH4AYAw7M5LtcO5SMAPwK4PoPn8nLGZUj8ttCW/q4NgLXJvAvA8fRY8yzOaR0SNyABAM6545G4/MxVfBnmys4t9LLNVwD4q3NuPRI1UQCY65y7MfD7FJraTv7bzsIUr1kTlhVX28l/2yl3AxK/hMzM5qCcOhTv/VYAQwEMc84NdM7Vd84d5ZzrCqBuBccdROJy8Y/JY9oicfNoRPIp8wFc4pxr45xrAOChLE7rdQD9nHMXOeeOBfCfCDvPZgGAs5xznZ1zdQA8Gnt8AxL1ylDaI3GZ2hWJGiiQuNn6ZsD3KDi1nYK0nbEAmjnnfpmsmd+IRO3/o4DvUXBqOwVpO+VuBTDcJ+/OZyrnb9x7/wQSH8qDSHxTGwA8D+DfAcyu4NB7kOh1VyHR+70K4MXka05F4ibTQgCfIVH7y/R8FgP4t+TrrQOwBYd/s68y7/0SAH9CYsTHMgAzYk/5XwBdkmP+X6/s9ZL/0Xc651KWIbz3G5M10PVI/NsCwKYq1lVLgtpO3ttOGRK/xT+ExCifBwBc573/IffvojSo7eS37SSf0wbAJUiMisuKy7IDEhERSalGL70iIiKFow5FRESCUIciIiJBqEMREZEg1KGIiEgQWa1m6ZzTkLAS5L0v9WW71W5KU5n3vmmxT6IiajslK2Xb0RWKyJFrTeVPEUkpZdtRhyIiIkGoQxERkSDUoYiISBBF29pWpKqOPvrwzq6nnnpq5LF77rnHcp06dSyPHDnS8gcffJDHsxM58ugKRUREglCHIiIiQahDERGRIHQPRaqV2rVrW+7QoYPlu+66K/K8G288vLHl3r17Lc+YEd9OQkRC0RWKiIgEoQ5FRESCUMlLqpWGDRta5rIWZwBo1KiR5c8++8zyli1b8nh2Ikc2XaGIiEgQ6lBERCQIlbwCO+qow310s2bNIo+dcsopKY9ZtWqV5fXr1+fnxKqx448/3vKZZ55pmctcXOICgE2bNlmeMmWK5WXLluXjFEUEukIREZFA1KGIiEgQNaLk5dzhDQu5PAIATZse3lTM+8Obv61bt87y/v37Uz4nl/fnhQgHDBgQed7gwYMt79y50/LTTz9t+Z133sn6/Ws6/gx79epluX379pb5MwCiI7vefPNNyytWrMjHKYoIdIUiIiKBqEMREZEgakTJ65hjDn8b5513XuSxBx980PLu3bst/+53v7P8zTffWD5w4EDW78/rS5122mmWf//730eex6O+Jk6caHnbtm1Zv+eRhPc9OfbYYy3Hy1xs3759lvkzzaWkKdULt5datWpZ5vbC7ejQoUOR47m9cDvitsPvwbkifHy6163udIUiIiJBqEMREZEgakTJq3Hjxpa7desWeezyyy+3vGPHDsv16tWzzJMRM8Vlrk6dOlnmEVsnnXRS5Jg9e/ZYnjp1qmVNtqsY/ztedNFFGR0zc+ZMyxs3bgx+TlK6unTpYplHBbZo0cLyTTfdZPnbb7+NHP/+++9bHj9+vOUNGzZY5gm2/H5xXM7avHmz5QkTJlj+4Ycf0h5f3egKRUREglCHIiIiQahDERGRIKrtPRQeKsz3Ta6++urI83j71w8//NDy2rVrLfNM+Uw1b97cct++fVOeC58jAAwfPtzy9OnTLdekGmo+nHjiiZbPPvvsjI6ZN2+e5bKysuDnJKXrrLPOsnzvvfdarlu3rmW+79qkSZPI8Tz0/4YbbrDMw4l5RQ5+XSD9MGA+nn9OLF26NPI8nt7AeC+fhQsXWubFZYtNVygiIhKEOhQREQmiWpW8eHZr165dLV9//fWW48OGt27davmNN96wzIszZjpTtWXLlpb79Olj+brrrrPMZa5p06ZFjn/11Vct82XqwYMHM3r/IxX/m3J5oaLPjT/fXFY/kOpryZIllrnMzUOFearAcccdFzmev46Xw8pVtEpDJriUtn379shj6UrwvLDpCy+8YFklLxERqXHUoYiISBDVquR17rnnWubL16uuusoy70cCAF9++aXlWbNmWc5lZFfnzp0tDxo0yDJv7bt48WLLzz77bOT4L774wjLPmpeKcXmBc01aVE/C4f/zzz33nGX+v5nL6hj169e33K5dO8utW7fO+rX4eM5A+oUreaUPXlyylOgKRUREglCHIiIiQZR8yYsXXuQyF4+satSokWXe+hUARo0aZZknM2ZaLjn11FMt9+7d2zKPJuMJR6+88oplXmQOUJkrVzwx8fPPP7fMI/3ieOIaL+Spz6Dm49LQnDlzLM+fP99yLqO0eEHZqpa8fvKTn1jmnytAtF3zBMpNmzZZLtUFZXWFIiIiQahDERGRIIpW8uJLTp641rRp08jzhgwZYrlfv36WuaSxaNEiyyNGjIgcz+tn/fjjj5WeV3w7z4svvtjyFVdckfKceVQJl9ji5RWNSsrN999/b3ny5MmWec2m+KidSy+91DKXOnlyWDFGynD74hIKjxTkUT4AsHz5cstczuERQGpbqfGk4XRrZGVq165dlnlvFC6rZYpLWfEtiE8++WTLrVq1ssxbhfO25aVEVygiIhKEOhQREQlCHYqIiARRtHsoXEvmvUUGDhwYeR7PSOf7K3zfYty4cZbffvvtyPHZ1k0bNmwY+bpjx46W27Zta5n3MOGhyuvWrcvq/aRy/Bny/t9c0+b7EQBwxx13WObhlrxYKN9bySe+v8NtmPc7v/HGGy3zjGwAePHFFy3zsGk+f14MU0ofr9QR3zcpfh+3OtEVioiIBKEORUREgihayYuHzfXs2dPy0KFDI8/jUgbPDh0zZoxl3ueESyJxXHqoVauWZR7CfOGFF0aO4QUpeS+OuXPnWuaSm4S3fv16y++9955lbg/xrYF522AuLXHJiIcj53PYLS9Y+rOf/cwy72lRkcsvv9zy6NGjLf/973+3/NFHH1nW/jqlr0WLFpb5ZwwQHTYcH1Jc6nSFIiIiQahDERGRIIpW8mrQoIHla6+91nJ8ljCbMmWK5bfeesvy119/nfYYfr02bdpY5pnUPLKLt+YEgC5duljm/RS45Mb7nEh+8WoHK1eutMyz5oHoSJnzzjvPMpfGFi5caJkX+Ayte/fulgcMGFCl1+LVInjEGs+gX7BgQZXeQ/Kvffv2luOrg/DPrL179xbsnELQFYqIiAShDkVERIIoaMmL96XgxfCuvPJKyzz6CoiOwLr55pst9+nTx3JFiz7y8XwpyaPH+D25FBc/5w4dOljmiWibN2+2PGnSJMvxxQe1gF/V8aJ8f/3rXy3HJzbyyC4e8XXLLbdY5nbz8ssvR47nSZNVxSMa4+0rWzxi7IQTTrB83HHHVel1pbB40Vme2B3HpUyeoFuqdIUiIiJBqEMREZEgClryOnDggGVez5/LDbznCACcccYZlps0aZIys/hEIF7jKN0eAlwSia+rw2UqntjIk5HuvPNOy0uWLLH81VdfpX0tyQ2XqXh0XXxCLLcp3m6VM5e/4p/7yJEjLfO6bVUV37clW1zCTZclP7isyFuAc3kViJYi+WfeJ598Yvn000+3zCXRuNWrV1ueN29edidcBLpCERGRINShiIhIEOpQREQkiILeQ+FF63jfEF4kb9asWZFjOnfubJnvYaSrGcfvofBsYl44kof9XnbZZZbje1HwftF8znzfhRcZrOq+1VIxvg/Fs4h5TxogumAof9a8p02nTp0s33rrrZHj+R4dL7w4ffp0y3w/p6L7Y3ye27dvT/kc/r/BM/iB6H2fiurtkl885Pv888+3zPfigOg9FP5ceagw7xUf/5nD+N5efEpFKdIVioiIBKEORUREgija4pBcLuB9LTgDwMyZMy2n28OExUsPvEggD9Xjy0weyrl8+fLI8bz/BJdVGjdubLmsrMwyz+TWMOHiGTVqlGVeIaF///6WeYE+Lq0CQLNmzSzz4o48I5/3VuE9W4DoKglcEuWhzrw3ipQ+3raXy9+ffvpp5Hlcmk+nZcuW4U6shOgKRUREglCHIiIiQRSt5JUpXngxE/FSGI/q4ZE8vNXvnj17LI8dOzZy/MSJEy1zOUzlrNLGe9e88sorlrm8ySUn3nYViJYkuPzFpc5p06ZZXrFiReR4blM8UocXG2Vcmo1vZyylgVdMmDx5suX58+dHnpfJQp2//e1vLcdLn3w879PDZbZSpSsUEREJQh2KiIgEUfIlr2zF98XgSWE8GYknHM2ePdsyT4gD/u+oM6l+eGTV888/b3nVqlWWefQXAPTs2dMyjxLj7YQ55xMvMMilNJ60y38v+celqFy2j+bRoPF9k6rzop+6QhERkSDUoYiISBA1ouTFI3c6duwYeeyBBx6wzFv4fvnll5Yff/xxyzw6SGoe3qOGy0RcVgKiZa50I754NA6P0gKyL1VwCTa+pTVPnF2wYIHld99913J8Qq5UX7weIbcLzqVKVygiIhKEOhQREQmiRpS8eI2vU045JfIYb8/JpQQe7cNlkHi5QWounijGWwYDwIQJEyxzGZW3pB40aJDl+GTEipYkL8dtbeXKlZZfeumlyPN4pCGXvHj9sHjJTqovbhc8EpHL9KVKVygiIhKEOhQREQlCHYqIiARRbe+h8CJ7ffr0sXznnXdGnsf1yLlz51qeMmWK5W3btlnWoo9HDh6eWdFQXd4Gmu9hcBuqU6dO5Pj4MOJUuK3x+8cXROUthHmoc3UYRnok4nu6LVq0sMz337p27WqZtwwGgCVLllj++OOPLfPs+lKlKxQREQlCHYqIiARRbUtePJSzX79+lvlSEohuvzps2DDL3333nWUNuZQ4Loft3r07Zeb9MUTK8f43vMrCww8/bJl/fsXLpV9//bVlntLAWxCXKl2hiIhIEOpQREQkiGpb8urUqZPl7t27W+ZSBQB88sknlj/44APLPFpGI7tEJBReGJTLWVyO55UUduzYETl+4cKFlqvbop+6QhERkSDUoYiISBDVtuS1evVqy7yQXxxPZoxPGBMRKbY1a9ZEvuaJjRs3biz06VSJrlBERCQIdSgiIhJEtS15zZgxI2UWESkmXmNty5Ytlnn/Gi6/T548OXI8739T3egKRUREglCHIiIiQahDERGRIKrtPRQRkVLEe9vw/ZA33njD8q5duyyPGjUqcjwvCFnd6ApFRESCUIciIiJBuGwWRnTOaRXFEuS9d5U/q3jUbkrWZ9777pU/rXjUdkpWyrajKxQREQlCHYqIiASR7SivMgBrKn2WFFLbYp9ABtRuSpPajuQqZdvJ6h6KiIhIOip5iYhIEOpQREQkCHUoIiIShDoUEREJQh2KiIgEoQ5FRESCUIciIiJBqEMREZEg1KGIiEgQ6lBERCQIdSgiIhKEOhQREQlCHYqIiARRozsU59xq51zvIr7/d865S4v1/pI7tR3J1ZHcdqrUoTjnbnLOzXHO7XLObUzmu51zpb4l7STn3M7kn/3OuX309XM5vuYI59xjAc/xD3ROO51ze5xzB51zjUK9RzGp7UReM3Tb6e2cOxRrP78M9frFprYTec3Qbec659xs59xW59w659zzzrl6mR6fc4finLsfwDMAngTQHEAzAL8GcCGAY9Mcc3Su7xeS976v976e974egJEAnij/2nv/6/jznXPZbkQW4hwfp3OqB+ApAO9577cU+lxCU9spiG+4/XjvRxbpPIJS28m7+gCGAmgBoCOAUwD8v4yP9t5n/QdAAwC7ANxQyfNeAvA/ACYmn987eezLADYhsRPbIwCOSj7/MQAj6Ph2ADyAY5JfTwfwOIBZAHYAmAKgCT1/cPI1NwN4GMBqAL0zOMf/iv1d7+Sx/wFgPYB/ALgDwHR6zjHJc2sH4G4A+wHsA7ATwLjkc74DcB+ARQC2AfgngNo5/Hu75Pf1y1w+r1L6o7aT/7ZTfg7F/qzVdqpf20lxnoMAzMv0+bleofQEUBvAhAye+wsAf0Si55sJ4FkkPtz2AHoBuAXAbVm89y+Szz8Jid9IHgAA59yZSDSiwQBaAjgRQKssXjeuFYB6ANog8cGl5b0fBuA1AH/yid82+tPDgwBcicT3e07y/OCcOzp5Wdkjg3O5DEBDAOOy/i5Kj9oOyWPbaemc2+CcW+Wce8o5d3wVvp9SobZDCvBzBwAuAbA405PPtUNpAqDMe3+g/C+o7rbHOXcJPXeC936W9/4QEr3pTQAe8t7v8N6vRqKUMziL9/6H9365934PgNEAuib/fiCAt733M7z3PwL4A4BDOX5/AHAAwGPe+33J98rVf3vv13vvNwN4u/x8vfcHvfcNvfcfZ/AatwIY473fXYXzKBVqO5nLte0sBtAFibLFlQB6IFEiqu7UdjJX5Z87zrm+SHSkj2b6prl2KJsBNOEan/f+Au99w+Rj/LrfUm4CoBYSl4fl1gA4OYv3Xk95NxK9OZD47cDey3u/K3kuudrgvd9XhePLpTvfjCRviN0AYHiAcykFajuZy6nteO/Xee+/9N4f8t6vBPDvSPzgq+7UdjJX1Z87FyBRIhyQbEMZybVD+QjAjwCuz+C5nnIZEr8ttKW/awNgbTLvAsCX5s2zOKd1AFqXf5G8xD8xi+PjfOzrys4t/vxQbgCwAYnL9ppAbadwbYdfv6RHQGVIbacAbcc51x3AeAC3eu+nZ3NsTh2K934rEiMBhjnnBjrn6jvnjnLOdQVQt4LjDiJxufjH5DFtkbh5NCL5lPkALnHOtXHONQDwUBan9TqAfs65i5xzxwL4T4SdZ7MAwFnOuc7OuTr4v5eBG5CoV4Z2K4DhPnmHrLpT28l/23HOXeaca53MbQD8GZnddyhpajsFaTtdkBjMcLf3fmK2x+f8jXvvn0DiQ3kQiW9qA4Dnkbi8nl3Bofcg0euuQuK37lcBvJh8zalI3GRaCOAzJGp/mZ7PYgD/lny9dQC2IDHaIQjv/RIAf0JixMcyADNiT/lfAF2cc1ucc69X9nrJm2M7nXM9K3hOGyRuir2c84mXILWdvLed7gA+ds7tRuLf6XMAv831/EuJ2k7e284DSFxhvURzZBZker6uhvziKyIiRVajl14REZHCUYciIiJBqEMREZEg1KGIiEgQ6lBERCSIrFazdM5pSFgJ8t6X9KQ1tZuSVea9b1rsk6iI2k7JStl2dIUicuRaU/lTRFJK2XbUoYiISBDqUEREJAh1KCIiEoQ6FBERCUIdioiIBKEORUREglCHIiIiQahDERGRINShiIhIEOpQREQkiKzW8hIRkao744wzLHft2jXyWNOmh5fIOuqow7/zb9++3fKyZcssL1myxPK2bdsir1XoHXl1hSIiIkGoQxERkSBU8hIRqQLnortH1K5d23KrVq0sd+jQwXKfPn0sX3PNNZHj27VrZ/mYYw7/iN64caPl6dOnWx47dqzl8ePHR15r//79lZ1+ULpCERGRINShiIhIECp5ieQBl0E416lTxzKP5qlXr57lo48+OvJaO3bssLxhwwbLu3fvtlzo0TxHOv5M+bMDgI4dO1oeNGiQ5SFDhliuX7++ZR7JFcefK7eX6667zvKpp55qed68eZHj16w5vA9WIcpfukIREZEg1KGIiEgQ1bbkxWWBWrVqWY5fPqYrBRw6dMgyXwrGj+f3SfdaPBJj7969Kd9Dqpf4yJ1UuN1xGwCAY4891jKXubp06WL5rrvusnz55Zdb5nIIAEybNs3yk08+aXn27NmWd+3aVen5Sjg8kuv888+PPPbII49Yvvjiiy1zmzp48KDlAwcORI7ft29fyvfkNsWZJ0nee++9kWOeeOIJy2vXrrWcrxKprlBERCQIdSgiIhJEtS158WUeTxI6/fTTI8/jyUBcSli1apXld99913KzZs0ix3fq1Mnyzp07LXM569JLL7U8dOhQy+vXr4+8lkpgpSs+sopLGlxeYP3797fco0ePyGPcbtq0aWO5QYMGKd8jXjJjvXr1sswT5f785z9bHj16tOUff/wx7WtJGLz+1h133BF5rGfPnpUezz8bvvrqq8hjkyZNssylsQEDBljm9nbcccdZHjhwYOS13nrrLcs//PCDZR4hGJKuUEREJAh1KCIiEoQ6FBERCaJa3UPh+xuDBw+2fPPNN1uOz1pNNwTvgw8+sNy3b1/LzZs3jzzvxBNPtMzD/njYHdcmGzVqZJlnNUtp4HsQPKTziiuuiDzvlFNOscwzlFnDhg0tx9sd17X5HkxFs6LT4eHJZWVllnl/DM2ULyz+vPlnBBC9V7pixQrLPOT7u+++S5mB6M8N/nnEme+hcJtq0qRJ5LW6d+9ueenSpZZ1D0VEREqaOhQREQmiWpW8brzxRsu8h8DJJ59sOT78k2eh8mxiHorJw4l5KCcQHc7Jl5Y8NPPbb7+1zFtwqgxRPPxZdevWzfLPf/5zy71797bcunXryPEVtYlU4jPr0332vCrD5s2bLXMb4iHtQHS71zlz5lieP3++5fhsa8kv/rzeeeedyGPLly+3zPuWcOaSE6+uAUSHCnN7WbBggWWe9V7Rzz9ux/HH8kFXKCIiEoQ6FBERCaLkS148851LFLwHQLrRV0D08nHr1q2WeTQWH8+Xq0B0hE6LFi1Svg+PvOGsmfGFEx89xSXNX/3qV5Z5EUYe8VVROYA/ay5B8CireJlq3bp1lrlNfP/995a5bMIjezZt2hR5LT6GX2vPnj0pz1Hyj0dmvfnmm5HHeATY119/bZlX2sj08+IyPZe5uI1wySvuzDPPtMyjEvNFVygiIhKEOhQREQmi5Epe8UXyeDTXT3/6U8tcouDJQzzyBYiWFXhUTd26dVO+P2+ZCQAXXHCBZZ4AyaO8Fi1aZDk+YkPyh8tc7du3jzx2++23W77qqqss88Qv/qy++eabyPHcpnghPx6dw6Wo+AJ/fAyXJ3ixUi7BFmJ7VgmHS1HxcmdIPHqPf+akm7Adx/vvxCc95oOuUEREJAh1KCIiEkRJlLy4dNG4cePIY+kmMPIoi3Hjxll+6aWXIsfzpLBc8EggXp+JSxo84UgKh8ueZ599duQxXqeLL/V5ZNTixYst8544APDee+9ZXrlypWUuO/AabjyaUCQUHoGaLleE1xnLZIJuVekKRUREglCHIiIiQahDERGRIEriHgrXws8///zIY7wvBdcAeZG88ePHW67qPRNeTA2I7kHA+4HzcL587S0gmYvvo87DOvn+Bs9u5zY0YcKEyPE8jFjDe6VYeNY9/yxKN+0hjv8fFGIBUV2hiIhIEOpQREQkiJIoefFw3EcffTTyWMuWLS1zaWnJkiWWeQG2qurZs2fk6w4dOljmoXpcBuFzkcLhS3je0hkAZs6caZmHTvIQ4ptuusly586dI8dPmjTJMpdUedFHLrPFS2FaGLQ08P9ZXoXj+OOPT/kcIPq5ci7UZ8q3AHgL3yFDhljmRR8rMm/ePMu8Uki+6ApFRESCUIciIiJBlETJiy/x2rVrF3mM9yPhfQfef/99y/H9I0LifQs4894GvAClFE66zwMAnnnmGcu8DwQvFMmrMvAioABwzjnnWP7Nb35jmRcf5dn0b7zxRuT4+GKTUhwnnHCCZd4L529/+5tlLn8BwPPPP2/5hRdesMwrJuTTGUwUzhAAAAeBSURBVGecYXnQoEGW+/TpYznTWfNffPGFZV7ZIV90hSIiIkGoQxERkSCKVvLiCTu8XWt8ATO+nOMFIXnEQsjtT+P7apx00kmWec+L2bNnW9aInuKLtwH+rJ599lnLvIfJhRdeaLlNmzaR4/lzr1OnjmWe3NqtWzfL8cUpX3nlFcsfffSRZZ5oJvnH+4EMHTrUMk8SjG8ffdttt1nmkWHDhw+3zAuLVlX//v0jX/OW1dxG+VzSbUvNP5cAYOzYsZa57eeLrlBERCQIdSgiIhJEQUtetWrVssyjue6++27L8ZLX1KlTLU+ePNkyl79C4j1XAKBRo0aWeQLjp59+mpf3lzB4i1QemcVb8L7++uuWmzZtGjmet5vmchY/j8spvD00EB2dyG36X//6l+X4yDQJg3+2XHHFFZZPP/10y/EyF+PPeODAgZZ5XUEe1RffS4fLmnzMZZddZrljx46WefIiEJ1MzSMU070Hl1T/8pe/RJ7HP7Pi693lg65QREQkCHUoIiIShDoUEREJoqD3UHj45WmnnWaZZyXHh+BOnDjR8qJFiyyHHH7Js2nj91B4f5Rt27ZZ5kUCpbTxvYrly5enzHzPAwDmzp1rmRee5LbCC/Rdf/31keO5Ta9fv97y0qVLU56XhMOLgfJwcL6HW9EqC7xyB/884JUV0i04CkQ/45tvvtkyDzNv1aqV5fgeTDw8mPHPH75vMmzYMMu8KCoA7N2713LI6RXp6ApFRESCUIciIiJBFLTkxfue8NA8nuk5ZcqUyDEffvih5XwtbnbWWWdZ5mF+QPTyd8eOHZZVrqhZeJgxAKxduzZlZrNmzbLctm3byGO8wB/vtcKl3kItNnik4dU1+P8v49I6lySBaGmey1lcmjr33HMtc/kLiO7PxMOW44tQpsM/D/m1ZsyYYZmHKk+bNs1yvB0XoszFdIUiIiJBqEMREZEgClry4i1beaY7z1geM2ZM5BgeMZGvmZ48UzU+yosvIXnfle3bt+flXKT64IUi4yN9uGzCo4N4S2vJDy5Hl5WVWU63iGu8LJRJmYjLV506dYo8Fv+6HJfi+D14IVMAWL16teXp06dbHjduXMrnFGIGfKZ0hSIiIkGoQxERkSAKWvLiUVqTJk1KmYuBR2nEJ7jxBMoNGzZYVsnryMST46699lrL8QX+eALknj17LGt0YP7xlty8MCiXr3nEKS8aWQy8lTQATJgwwTJP5i7EfiZVpSsUEREJQh2KiIgEUbQtgEsJr/fDWxMD2rK1uuARNHFVndzFayu1bt3a8i233GI5PiGWRxRxCYb3p5D84P+zvP/N7t27LcdL2+nwyFTOvJ8Kl0Hjj3G7TDfKq2fPnpHjecIrT2zkc45PYCwVukIREZEg1KGIiEgQKnkhuh0nb/kLRC+TpbRwCSH+ufF6SDzKissWFeEyBpeznn76acs8gS2+dTUvjc9LjXMJQ/Lvyy+/tPzUU09ZHjJkiOW6deumPZ7Xz+IRY7wWYa9evSLHcLvg0WTp8M8fALjvvvss9+jRw/KoUaMsv/XWW5ZLqfylKxQREQlCHYqIiAShDkVERILQPRSk3zMBKPx+ApI5/ty6dOkSeezqq6+2zIuPfv7555b5Pkvz5s0jx/fr18/y7bffbpnr3Vx7jy88yPv48BbCvCWr5B/PLn/hhRcsjx8/3jIP843jlQ34fioPJR89enTkmIsvvtjygAEDLPO+ONx24j9/+P7MlVdemfI9ly1bZvmLL75Ie/6FpisUEREJQh2KiIgEcUSVvPiSkYf68UJ+8RnXvNeAyhWlhctMK1asiDzGJaz777/fMu/Dw+Jb+PKCgR06dLCcbob15MmTI1/z3hW830W6PTkkP7isyfsZca6q+NbkXGbjIeM8tJ1LWddcc03k+Hbt2lnevHmzZR6KXqrTGXSFIiIiQahDERGRII7Ykle3bt0s83aecbywX7pyiRQHl4/WrVsXeey1116zfNttt1nm0Vu8hS+XPYH0pS0e9TNz5kzLPIIIAObMmWO5VMsTEkZ8pjqXvNLtYcIrJnzyySeRx3g76S1btlhevHixZd6bqZToCkVERIJQhyIiIkEcUSUvnqTII7a4dBIfsbFgwQLLvE+BlJaDBw9Gvp46daplXuiRJ5pdcMEFluNlT95Hg/cw4Ulk06ZNsxwvW3CpQiRu6dKlKXN1pysUEREJQh2KiIgEcUSVvHgvDJ5wtGbNGsvx0gnvgZBuxIaUHp6QOnHiRMtr1661zKUsnsgIREf3vf/++5bnzp1rmUdvac03EV2hiIhIIOpQREQkCHUoIiISxBF1D4Xvj3z66aeWx4wZY7lhw4aRY3hI39atW/N4dpIvfD+Fh/fGh/qKSNXoCkVERIJQhyIiIkG4bIY7Ouc0NrIEee9d5c8qHrWbkvWZ9757sU+iImo7JStl29EVioiIBKEORUREgsh2lFcZgDWVPksKqW3lTyk6tZvSpLYjuUrZdrK6hyIiIpKOSl4iIhKEOhQREQlCHYqIiAShDkVERIJQhyIiIkGoQxERkSDUoYiISBDqUEREJAh1KCIiEsT/B299mhDq7iJiAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 6 Axes>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 6 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig = plt.figure()\n",
    "for i in range(6):\n",
    "    plt.subplot(2,3,i+1)\n",
    "    plt.tight_layout()\n",
    "    plt.imshow(example_data[i][0], cmap='gray', interpolation='none')\n",
    "    plt.title(\"Ground Truth: {}\".format(example_targets[i]))\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 32, 32]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(example_data[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_activation_name(activation):\n",
    "    \"\"\"Given a string or a `torch.nn.modules.activation` return the name of the activation.\"\"\"\n",
    "    if isinstance(activation, str):\n",
    "        return activation\n",
    "\n",
    "    mapper = {nn.LeakyReLU: \"leaky_relu\", nn.ReLU: \"relu\", nn.Tanh: \"tanh\",\n",
    "              nn.Sigmoid: \"sigmoid\", nn.Softmax: \"sigmoid\"}\n",
    "    for k, v in mapper.items():\n",
    "        if isinstance(activation, k):\n",
    "            return k\n",
    "\n",
    "    raise ValueError(\"Unkown given activation type : {}\".format(activation))\n",
    "\n",
    "\n",
    "def get_gain(activation):\n",
    "    \"\"\"Given an object of `torch.nn.modules.activation` or an activation name\n",
    "    return the correct gain.\"\"\"\n",
    "    if activation is None:\n",
    "        return 1\n",
    "\n",
    "    activation_name = get_activation_name(activation)\n",
    "\n",
    "    param = None if activation_name != \"leaky_relu\" else activation.negative_slope\n",
    "    gain = nn.init.calculate_gain(activation_name, param)\n",
    "\n",
    "    return gain\n",
    "def linear_init(layer, activation=\"relu\"):\n",
    "    \"\"\"Initialize a linear layer.\n",
    "    Args:\n",
    "        layer (nn.Linear): parameters to initialize.\n",
    "        activation (`torch.nn.modules.activation` or str, optional) activation that\n",
    "            will be used on the `layer`.\n",
    "    \"\"\"\n",
    "    x = layer.weight\n",
    "\n",
    "    if activation is None:\n",
    "        return nn.init.xavier_uniform_(x)\n",
    "\n",
    "    activation_name = get_activation_name(activation)\n",
    "\n",
    "    if activation_name == \"leaky_relu\":\n",
    "        a = 0 if isinstance(activation, str) else activation.negative_slope\n",
    "        return nn.init.kaiming_uniform_(x, a=a, nonlinearity='leaky_relu')\n",
    "    elif activation_name == \"relu\":\n",
    "        return nn.init.kaiming_uniform_(x, nonlinearity='relu')\n",
    "    elif activation_name in [\"sigmoid\", \"tanh\"]:\n",
    "        return nn.init.xavier_uniform_(x, gain=get_gain(activation))\n",
    "def weights_init(module):\n",
    "    if isinstance(module, nn.modules.conv._ConvNd):\n",
    "        # TO-DO: check litterature\n",
    "        linear_init(module)\n",
    "    elif isinstance(module, nn.Linear):\n",
    "        linear_init(module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_size = list(example_data[0].shape) # mnist image\n",
    "# h_size = 256\n",
    "z_size = 12\n",
    "model = VAE(img_size, z_size).to(device) # migrates to CUDA if you can"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VAE(\n",
       "  (encoder): Encoder(\n",
       "    (conv1): Conv2d(1, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (conv2): Conv2d(32, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (conv3): Conv2d(32, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (lin1): Linear(in_features=512, out_features=256, bias=True)\n",
       "    (lin2): Linear(in_features=256, out_features=256, bias=True)\n",
       "    (mu_logvar_gen): Linear(in_features=256, out_features=24, bias=True)\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (lin1): Linear(in_features=12, out_features=256, bias=True)\n",
       "    (lin2): Linear(in_features=256, out_features=256, bias=True)\n",
       "    (lin3): Linear(in_features=256, out_features=512, bias=True)\n",
       "    (convT1): ConvTranspose2d(32, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (convT2): ConvTranspose2d(32, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (convT3): ConvTranspose2d(32, 1, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_annealing(init, fin, step, annealing_steps):\n",
    "    \"\"\"Linear annealing of a parameter.\"\"\"\n",
    "    if annealing_steps == 0:\n",
    "        return fin\n",
    "    assert fin > init\n",
    "    delta = fin - init\n",
    "    annealed = min(init + delta * step / annealing_steps, fin)\n",
    "    return annealed\n",
    "\n",
    "def _kl_normal_loss(mean, logvar):\n",
    "    latent_dim = mean.size(1)\n",
    "    latent_kl = 0.5 * (-1 - logvar + mean.pow(2) + logvar.exp()).mean(dim=0)\n",
    "    total_kl = latent_kl.sum()\n",
    "\n",
    "    return total_kl\n",
    "\n",
    "def _reconstruction_loss(data, recon_data, distribution=\"bernoulli\"):\n",
    "    batch_size, n_channels, height, width = recon_data.size()\n",
    "    is_colored = n_channels == 3\n",
    "\n",
    "    if distribution == \"bernoulli\":\n",
    "        loss = F.binary_cross_entropy(recon_data, data, reduction=\"sum\")\n",
    "\n",
    "    elif distribution == \"gaussian\":\n",
    "        loss = F.mse_loss(recon_data * 255, data * 255, reduction=\"sum\") / 255\n",
    "\n",
    "    elif distribution == \"laplace\":\n",
    "        loss = F.l1_loss(recon_data, data, reduction=\"sum\")\n",
    "        loss = loss * 3\n",
    "        loss = loss * (loss != 0)\n",
    "\n",
    "    loss = loss / batch_size\n",
    "\n",
    "    return loss\n",
    "\n",
    "def loss_function(x_hat, x, mu, log_var, is_train, n_train_steps, steps_anneal=0, beta=1, C_init=0., C_fin=20., gamma=100.):\n",
    "    \"\"\"Compute the ELBO loss\"\"\"\n",
    "    x_size = x_hat.size(-1)\n",
    "    # black or white image => use sigmoid for each pixel\n",
    "#     rec_loss = F.binary_cross_entropy(x_hat, x.view(-1, x_size), reduction='sum')\n",
    "    rec_loss = _reconstruction_loss(x, x_hat, distribution=\"bernoulli\")\n",
    "    # closed form solution for gaussian prior and posterior\n",
    "#     kl_div = -0.5 * torch.sum(1 + log_var - mu.pow(2) - log_var.exp())\n",
    "    kl_div = _kl_normal_loss(mu, log_var)\n",
    "    \n",
    "    C = (linear_annealing(C_init, C_fin, n_train_steps, steps_anneal) if is_train else C_fin)\n",
    "    vae_loss = rec_loss + gamma * (kl_div - C).abs()\n",
    "#     vae_loss = rec_loss + beta * kl_div\n",
    "    return vae_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    def __init__(self, model, optimizer=optim.Adam, loss_function=loss_function):\n",
    "        self.model = model \n",
    "        self.optimizer = optimizer(self.model.parameters())\n",
    "        self.loss_function = loss_function\n",
    "        self.epoch = 0\n",
    "        \n",
    "    def __call__(self, train, test, n_epochs=100):\n",
    "        self.epoch = 0\n",
    "        for _ in range(n_epochs):\n",
    "            self._train_epoch(train)\n",
    "            self._test_epoch(test)\n",
    "            with torch.no_grad():\n",
    "                sample = torch.randn(64, self.model.latent_dim).to(device)\n",
    "                sample = model.decoder(sample).cpu()  # make sure on cpu\n",
    "                save_image(sample.view(64, 1, 32, 32),\n",
    "                           '../results/sample_' + str(self.epoch) + '.png')\n",
    "        \n",
    "    def _train_epoch(self, train):\n",
    "        self.epoch += 1\n",
    "        model.train() # make sure train mode (e.g. dropout)\n",
    "        train_loss = 0\n",
    "        for i, (x, _) in enumerate(train):\n",
    "            x = x.to(device) # data on GPU \n",
    "            self.optimizer.zero_grad() # reset all previous gradients\n",
    "            x_hat, latent_distribution, latent_sample = model(x)\n",
    "            loss = self.loss_function(x_hat, x, *latent_distribution, True, i)\n",
    "            loss.backward() # backpropagate (i.e store gradients)\n",
    "            train_loss += loss.item() # compute loss (.item because only the value)\n",
    "            self.optimizer.step() # take optimizing step (~gradient descent)\n",
    "\n",
    "        print('Epoch: {} Train loss: {:.4f}'.format(\n",
    "              self.epoch, train_loss / len(train.dataset)))\n",
    "        \n",
    "    def _test_epoch(self, test):\n",
    "        model.eval() # make sure evaluate mode (e.g. dropout)\n",
    "        test_loss = 0\n",
    "        with torch.no_grad():  # stop gradients computation\n",
    "            for i, (x, _) in enumerate(test):\n",
    "                x = x.to(device)\n",
    "                x_hat, latent_distribution, latent_sample = model(x)\n",
    "                test_loss += loss_function(x_hat, x, *latent_distribution, False, i).item()\n",
    "\n",
    "        print('Test loss: {:.4f}'.format(test_loss/len(test.dataset)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 Train loss: 7.0907\n",
      "Test loss: 3.5747\n",
      "Epoch: 2 Train loss: 4.1098\n",
      "Test loss: 3.0770\n",
      "Epoch: 3 Train loss: 3.6859\n",
      "Test loss: 3.9322\n",
      "Epoch: 4 Train loss: 3.4139\n",
      "Test loss: 2.6549\n",
      "Epoch: 5 Train loss: 3.1919\n",
      "Test loss: 2.6791\n",
      "Epoch: 6 Train loss: 2.9925\n",
      "Test loss: 2.6755\n",
      "Epoch: 7 Train loss: 2.8286\n",
      "Test loss: 3.6608\n",
      "Epoch: 8 Train loss: 2.7847\n",
      "Test loss: 3.0778\n",
      "Epoch: 9 Train loss: 2.7458\n",
      "Test loss: 2.6229\n",
      "Epoch: 10 Train loss: 2.6733\n",
      "Test loss: 2.3337\n",
      "Epoch: 11 Train loss: 2.6791\n",
      "Test loss: 2.3505\n",
      "Epoch: 12 Train loss: 2.5720\n",
      "Test loss: 2.3677\n",
      "Epoch: 13 Train loss: 2.5572\n",
      "Test loss: 2.7571\n",
      "Epoch: 14 Train loss: 2.5028\n",
      "Test loss: 2.9200\n",
      "Epoch: 15 Train loss: 2.4974\n",
      "Test loss: 2.3786\n",
      "Epoch: 16 Train loss: 2.4907\n",
      "Test loss: 2.3807\n",
      "Epoch: 17 Train loss: 2.4422\n",
      "Test loss: 2.3213\n",
      "Epoch: 18 Train loss: 2.4367\n",
      "Test loss: 2.2505\n",
      "Epoch: 19 Train loss: 2.4250\n",
      "Test loss: 2.2599\n",
      "Epoch: 20 Train loss: 2.4061\n",
      "Test loss: 2.5649\n",
      "Epoch: 21 Train loss: 2.3879\n",
      "Test loss: 2.2122\n",
      "Epoch: 22 Train loss: 2.3900\n",
      "Test loss: 2.3176\n",
      "Epoch: 23 Train loss: 2.3720\n",
      "Test loss: 2.4370\n",
      "Epoch: 24 Train loss: 2.3381\n",
      "Test loss: 2.2286\n",
      "Epoch: 25 Train loss: 2.3498\n",
      "Test loss: 2.7469\n",
      "Epoch: 26 Train loss: 2.3774\n",
      "Test loss: 2.5407\n",
      "Epoch: 27 Train loss: 2.3467\n",
      "Test loss: 2.2584\n",
      "Epoch: 28 Train loss: 2.3484\n",
      "Test loss: 2.5053\n",
      "Epoch: 29 Train loss: 2.3303\n",
      "Test loss: 2.1686\n",
      "Epoch: 30 Train loss: 2.3153\n",
      "Test loss: 2.3989\n",
      "Epoch: 31 Train loss: 2.3220\n",
      "Test loss: 2.1884\n",
      "Epoch: 32 Train loss: 2.2959\n",
      "Test loss: 2.1947\n",
      "Epoch: 33 Train loss: 2.3118\n",
      "Test loss: 2.1929\n",
      "Epoch: 34 Train loss: 2.2891\n",
      "Test loss: 2.3944\n",
      "Epoch: 35 Train loss: 2.3047\n",
      "Test loss: 2.3222\n",
      "Epoch: 36 Train loss: 2.2746\n",
      "Test loss: 2.3195\n",
      "Epoch: 37 Train loss: 2.2767\n",
      "Test loss: 2.1636\n",
      "Epoch: 38 Train loss: 2.2760\n",
      "Test loss: 2.3554\n",
      "Epoch: 39 Train loss: 2.2754\n",
      "Test loss: 2.1880\n",
      "Epoch: 40 Train loss: 2.2648\n",
      "Test loss: 2.1567\n",
      "Epoch: 41 Train loss: 2.2559\n",
      "Test loss: 2.2365\n",
      "Epoch: 42 Train loss: 2.2730\n",
      "Test loss: 2.1882\n",
      "Epoch: 43 Train loss: 2.2531\n",
      "Test loss: 2.1811\n",
      "Epoch: 44 Train loss: 2.2705\n",
      "Test loss: 2.4891\n",
      "Epoch: 45 Train loss: 2.2471\n",
      "Test loss: 2.2872\n",
      "Epoch: 46 Train loss: 2.2410\n",
      "Test loss: 2.1603\n",
      "Epoch: 47 Train loss: 2.2470\n",
      "Test loss: 2.2348\n",
      "Epoch: 48 Train loss: 2.2307\n",
      "Test loss: 2.5335\n",
      "Epoch: 49 Train loss: 2.2377\n",
      "Test loss: 2.2921\n",
      "Epoch: 50 Train loss: 2.2457\n",
      "Test loss: 2.3652\n",
      "Epoch: 51 Train loss: 2.2429\n",
      "Test loss: 2.4388\n",
      "Epoch: 52 Train loss: 2.2380\n",
      "Test loss: 2.2719\n",
      "Epoch: 53 Train loss: 2.2277\n",
      "Test loss: 2.1799\n",
      "Epoch: 54 Train loss: 2.2302\n",
      "Test loss: 2.1982\n",
      "Epoch: 55 Train loss: 2.2144\n",
      "Test loss: 2.1304\n",
      "Epoch: 56 Train loss: 2.2295\n",
      "Test loss: 2.2715\n",
      "Epoch: 57 Train loss: 2.2218\n",
      "Test loss: 2.1691\n",
      "Epoch: 58 Train loss: 2.2259\n",
      "Test loss: 2.1421\n",
      "Epoch: 59 Train loss: 2.2188\n",
      "Test loss: 2.3552\n",
      "Epoch: 60 Train loss: 2.2199\n",
      "Test loss: 2.2236\n",
      "Epoch: 61 Train loss: 2.2352\n",
      "Test loss: 2.4139\n",
      "Epoch: 62 Train loss: 2.2098\n",
      "Test loss: 2.1253\n",
      "Epoch: 63 Train loss: 2.2093\n",
      "Test loss: 2.3122\n",
      "Epoch: 64 Train loss: 2.2073\n",
      "Test loss: 2.1196\n",
      "Epoch: 65 Train loss: 2.2126\n",
      "Test loss: 2.1193\n",
      "Epoch: 66 Train loss: 2.1960\n",
      "Test loss: 2.1561\n",
      "Epoch: 67 Train loss: 2.2024\n",
      "Test loss: 2.1302\n",
      "Epoch: 68 Train loss: 2.2219\n",
      "Test loss: 2.2173\n",
      "Epoch: 69 Train loss: 2.2044\n",
      "Test loss: 2.1308\n",
      "Epoch: 70 Train loss: 2.1931\n",
      "Test loss: 2.3194\n",
      "Epoch: 71 Train loss: 2.2017\n",
      "Test loss: 2.1418\n",
      "Epoch: 72 Train loss: 2.2036\n",
      "Test loss: 2.1770\n",
      "Epoch: 73 Train loss: 2.2106\n",
      "Test loss: 2.3125\n",
      "Epoch: 74 Train loss: 2.1937\n",
      "Test loss: 2.1209\n",
      "Epoch: 75 Train loss: 2.1921\n",
      "Test loss: 2.2148\n",
      "Epoch: 76 Train loss: 2.1970\n",
      "Test loss: 2.1152\n",
      "Epoch: 77 Train loss: 2.1925\n",
      "Test loss: 2.1375\n",
      "Epoch: 78 Train loss: 2.1836\n",
      "Test loss: 2.1829\n",
      "Epoch: 79 Train loss: 2.1968\n",
      "Test loss: 2.2347\n",
      "Epoch: 80 Train loss: 2.1984\n",
      "Test loss: 2.2578\n",
      "Epoch: 81 Train loss: 2.1836\n",
      "Test loss: 2.3067\n",
      "Epoch: 82 Train loss: 2.1952\n",
      "Test loss: 2.2945\n",
      "Epoch: 83 Train loss: 2.2039\n",
      "Test loss: 2.1176\n",
      "Epoch: 84 Train loss: 2.1946\n",
      "Test loss: 2.3487\n",
      "Epoch: 85 Train loss: 2.1724\n",
      "Test loss: 2.1909\n",
      "Epoch: 86 Train loss: 2.1743\n",
      "Test loss: 2.1120\n",
      "Epoch: 87 Train loss: 2.1913\n",
      "Test loss: 2.1195\n",
      "Epoch: 88 Train loss: 2.1750\n",
      "Test loss: 2.2099\n",
      "Epoch: 89 Train loss: 2.1902\n",
      "Test loss: 2.1426\n",
      "Epoch: 90 Train loss: 2.1692\n",
      "Test loss: 2.2655\n",
      "Epoch: 91 Train loss: 2.1815\n",
      "Test loss: 2.3056\n",
      "Epoch: 92 Train loss: 2.1696\n",
      "Test loss: 2.2515\n",
      "Epoch: 93 Train loss: 2.1792\n",
      "Test loss: 2.1314\n",
      "Epoch: 94 Train loss: 2.1768\n",
      "Test loss: 2.1281\n",
      "Epoch: 95 Train loss: 2.1813\n",
      "Test loss: 2.2437\n",
      "Epoch: 96 Train loss: 2.1680\n",
      "Test loss: 2.3555\n",
      "Epoch: 97 Train loss: 2.1728\n",
      "Test loss: 2.1462\n",
      "Epoch: 98 Train loss: 2.1654\n",
      "Test loss: 2.1370\n",
      "Epoch: 99 Train loss: 2.1701\n",
      "Test loss: 2.1624\n",
      "Epoch: 100 Train loss: 2.1782\n",
      "Test loss: 2.1835\n",
      "CPU times: user 19min 6s, sys: 26.3 s, total: 19min 32s\n",
      "Wall time: 17min 51s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "trainer(train_loader, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), './model/betaVAE.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('./model/betaVAE.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim.lr_scheduler import StepLR\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, 3, 1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, 1)\n",
    "        self.dropout1 = nn.Dropout2d(0.25)\n",
    "        self.dropout2 = nn.Dropout2d(0.5)\n",
    "        self.fc1 = nn.Linear(12544, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = self.dropout1(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.fc2(x)\n",
    "        output = F.log_softmax(x, dim=1)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, device, train_loader, optimizer, epoch):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "#         data_hat,_,_ = vae_model(data)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.nll_loss(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, vae_model, device, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            data_hat,_,_ = vae_model(data)\n",
    "            output = model(data_hat)\n",
    "            test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n",
    "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_batch_size = 1000\n",
    "batch_size = 64\n",
    "epochs = 2\n",
    "lr = 0.1\n",
    "gamma = 0.7\n",
    "seed = 1\n",
    "no_cuda = False\n",
    "log_interval = 1000\n",
    "save_model = True\n",
    "use_cuda = True\n",
    "torch.manual_seed(seed)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else 'cpu')\n",
    "kwargs = {'num_workers': 1, 'pin_memory': True} if use_cuda else {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_classification = Net().to(device)\n",
    "optimizer = optim.Adadelta(model_classification.parameters(), lr=lr)\n",
    "\n",
    "scheduler = StepLR(optimizer, step_size=1, gamma=gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.316932\n",
      "\n",
      "Test set: Average loss: 0.1901, Accuracy: 9473/10000 (95%)\n",
      "\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.346134\n",
      "\n",
      "Test set: Average loss: 0.1682, Accuracy: 9560/10000 (96%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, epochs + 1):\n",
    "    train(model_classification, device, train_loader, optimizer, epoch)\n",
    "    test(model_classification, model, device, test_loader)\n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class siamese(nn.Module):\n",
    "    def __init__(self, image_size):\n",
    "        super(siamese, self).__init__()\n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.ReflectionPad2d(1),\n",
    "            nn.Conv2d(3, 4, kernel_size=3),\n",
    "            nn.LeakyReLU(0.1, inplace=True),\n",
    "            nn.BatchNorm2d(4),\n",
    "            nn.Dropout2d(p=.2),\n",
    "\n",
    "            nn.ReflectionPad2d(1),\n",
    "            nn.Conv2d(4, 8, kernel_size=3),\n",
    "            nn.LeakyReLU(0.1, inplace=True),\n",
    "            nn.BatchNorm2d(8),\n",
    "            nn.Dropout2d(p=.2),\n",
    "\n",
    "            nn.ReflectionPad2d(1),\n",
    "            nn.Conv2d(8, 8, kernel_size=3),\n",
    "            nn.LeakyReLU(0.1, inplace=True),\n",
    "            nn.BatchNorm2d(8),\n",
    "            nn.Dropout2d(p=.2)\n",
    "        )\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(8 * image_size * image_size, 500),\n",
    "            nn.LeakyReLU(0.1, inplace=True),\n",
    "\n",
    "            nn.Linear(500, 500),\n",
    "            nn.LeakyReLU(0.1, inplace=True),\n",
    "\n",
    "            nn.Linear(500, 15)\n",
    "        )\n",
    "    \n",
    "    def discriminator_embedding(self, x):\n",
    "        output = self.cnn(x)\n",
    "        output = output,view(output.size()[0], -1)\n",
    "        output = self.fc(output)\n",
    "        return output\n",
    "    \n",
    "    def forward(self, generator_model, original_input, phase='train'):\n",
    "        if phase is 'train':\n",
    "            generator_model.train()\n",
    "        else:\n",
    "            generator_model.eval()\n",
    "        intermediate_input = generator_model(original_input)\n",
    "        output_original = discriminator_embedding(original_input)\n",
    "        output_generator = discriminator_embedding(intermediate_input)\n",
    "        return output_original, output_generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SiameseLoss(nn.Module):\n",
    "    def __init__(self, margin):\n",
    "        super(Loss, self).__init__()\n",
    "        self.margin = margin\n",
    "    \n",
    "    def forward(self, output1, output2, label):\n",
    "        \"\"\"Define the computation performed at every call.\"\"\"\n",
    "        euclidean_distance = F.pairwise_distance(output1, output2)\n",
    "        distance_from_margin = torch.clamp(torch.pow(euclidean_distance, 2) - self.margin, max=50.0)\n",
    "        exp_distance_from_margin = torch.exp(distance_from_margin)\n",
    "        distance_based_loss = (1.0 + math.exp(-self.margin)) / (1.0 + exp_distance_from_margin)\n",
    "        similar_loss = -0.5 * (1 - label) * torch.log(distance_based_loss)\n",
    "        dissimilar_loss = -0.5 * label * torch.log(1.0 - distance_based_loss)\n",
    "        return torch.mean(similar_loss + dissimilar_loss)\n",
    "    \n",
    "    def predict(self, output1, output2, threshold_factor=0.5):\n",
    "        \"\"\"Predict a dissimilarity label given two embeddings.\n",
    "        Return `1` if dissimilar.\n",
    "        \"\"\"\n",
    "        return F.pairwise_distance(output1, output2) > self.margin * threshold_factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DistanceBasedLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Distance based loss function.\n",
    "    For reference see:\n",
    "    Hadsell et al., CVPR'06\n",
    "    Chopra et al., CVPR'05\n",
    "    Vo and Hays, ECCV'16\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, margin):\n",
    "        \"\"\"Set parameters of distance-based loss function.\"\"\"\n",
    "        super(DistanceBasedLoss, self).__init__()\n",
    "        self.margin = margin\n",
    "\n",
    "    def forward(self, output1, output2, label):\n",
    "        \"\"\"Define the computation performed at every call.\"\"\"\n",
    "        euclidean_distance = F.pairwise_distance(output1, output2)\n",
    "        distance_from_margin = torch.clamp(torch.pow(euclidean_distance, 2) - self.margin, max=50.0)\n",
    "        exp_distance_from_margin = torch.exp(distance_from_margin)\n",
    "        distance_based_loss = (1.0 + math.exp(-self.margin)) / (1.0 + exp_distance_from_margin)\n",
    "        similar_loss = -0.5 * (1 - label) * torch.log(distance_based_loss)\n",
    "        dissimilar_loss = -0.5 * label * torch.log(1.0 - distance_based_loss)\n",
    "        return torch.mean(similar_loss + dissimilar_loss)\n",
    "\n",
    "    def predict(self, output1, output2, threshold_factor=0.5):\n",
    "        \"\"\"Predict a dissimilarity label given two embeddings.\n",
    "        Return `1` if dissimilar.\n",
    "        \"\"\"\n",
    "        return F.pairwise_distance(output1, output2) > self.margin * threshold_factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContrastiveLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Contrastive loss function.\n",
    "    Based on: http://yann.lecun.com/exdb/publis/pdf/hadsell-chopra-lecun-06.pdf\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, margin):\n",
    "        \"\"\"Set parameters of contrastive loss function.\"\"\"\n",
    "        super(ContrastiveLoss, self).__init__()\n",
    "        self.margin = margin\n",
    "\n",
    "    def forward(self, output1, output2, label):\n",
    "        \"\"\"Define the computation performed at every call.\"\"\"\n",
    "        euclidean_distance = F.pairwise_distance(output1, output2)\n",
    "        clamped = torch.clamp(self.margin - euclidean_distance, min=0.0)\n",
    "        similar_loss = (1 - label) * 0.5 * torch.pow(euclidean_distance, 2)\n",
    "        dissimilar_loss = label * 0.5 * torch.pow(clamped, 2)\n",
    "        contrastive_loss = similar_loss + dissimilar_loss\n",
    "\n",
    "        return torch.mean(contrastive_loss)\n",
    "\n",
    "    def predict(self, output1, output2, threshold_factor=0.5):\n",
    "        \"\"\"Predict a dissimilarity label given two embeddings.\n",
    "        Return `1` if dissimilar.\n",
    "        \"\"\"\n",
    "        return F.pairwise_distance(output1, output2) > self.margin * threshold_factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import exp\n",
    "\n",
    "\n",
    "def gaussian(window_size, sigma):\n",
    "    \"\"\"Compute gaussian window, that is a tensor with values of the bell curve.\"\"\"\n",
    "    gauss = torch.Tensor(\n",
    "        [exp(-(x - window_size//2)**2/float(2*sigma**2)) for x in range(window_size)])\n",
    "    return gauss/gauss.sum()\n",
    "\n",
    "\n",
    "def create_window(window_size, channel):\n",
    "    \"\"\"Generate a two dimensional window with desired number of channels.\"\"\"\n",
    "    _1D_window = gaussian(window_size, 1.5).unsqueeze(1)\n",
    "    _2D_window = _1D_window.mm(_1D_window.t()).float().unsqueeze(0).unsqueeze(0)\n",
    "    window = torch.Tensor(_2D_window.expand(channel, 1, window_size, window_size).contiguous())\n",
    "    return window\n",
    "\n",
    "\n",
    "def _ssim(img1, img2, window, window_size, channel, size_average=True):\n",
    "    \"\"\"Compute the structural similarity index between two images.\"\"\"\n",
    "    mu1 = F.conv2d(img1, window, padding=window_size//2, groups=channel)\n",
    "    mu2 = F.conv2d(img2, window, padding=window_size//2, groups=channel)\n",
    "\n",
    "    mu1_sq = mu1.pow(2)\n",
    "    mu2_sq = mu2.pow(2)\n",
    "    mu1_mu2 = mu1*mu2\n",
    "\n",
    "    sigma1_sq = F.conv2d(img1*img1, window, padding=window_size//2, groups=channel) - mu1_sq\n",
    "    sigma2_sq = F.conv2d(img2*img2, window, padding=window_size//2, groups=channel) - mu2_sq\n",
    "    sigma12 = F.conv2d(img1*img2, window, padding=window_size//2, groups=channel) - mu1_mu2\n",
    "\n",
    "    C1 = 0.01**2\n",
    "    C2 = 0.03**2\n",
    "\n",
    "    ssim_map = ((2*mu1_mu2 + C1)*(2*sigma12 + C2))/((mu1_sq + mu2_sq + C1) *\n",
    "                                                    (sigma1_sq + sigma2_sq + C2))\n",
    "\n",
    "    if size_average:\n",
    "        return ssim_map.mean()\n",
    "    else:\n",
    "        return ssim_map.mean(1).mean(1).mean(1)\n",
    "\n",
    "\n",
    "class SSIM(nn.Module):\n",
    "    \"\"\"Wrapper class used to compute the structural similarity index.\"\"\"\n",
    "\n",
    "    def __init__(self, window_size=11, size_average=True):\n",
    "        super(SSIM, self).__init__()\n",
    "        self.window_size = window_size\n",
    "        self.size_average = size_average\n",
    "        self.channel = 1\n",
    "        self.window = create_window(window_size, self.channel)\n",
    "\n",
    "    def forward(self, img1, img2):\n",
    "        \"\"\"Execute the computation of the structural similarity index.\"\"\"\n",
    "        (_, channel, _, _) = img1.size()\n",
    "\n",
    "        if channel == self.channel and self.window.data.type() == img1.data.type():\n",
    "            window = self.window\n",
    "        else:\n",
    "            window = create_window(self.window_size, channel)\n",
    "\n",
    "            if img1.is_cuda:\n",
    "                window = window.cuda(img1.get_device())\n",
    "            window = window.type_as(img1)\n",
    "\n",
    "            self.window = window\n",
    "            self.channel = channel\n",
    "\n",
    "        return _ssim(img1, img2, window, self.window_size, channel, self.size_average)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_dataset = datasets.MNIST('../../data', \n",
    "                   train=True, \n",
    "                   download=True, \n",
    "                   transform=transforms.Compose([\n",
    "                       transforms.Resize(32),\n",
    "                       transforms.ToTensor()\n",
    "                   ]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_dataset_test = datasets.MNIST('../../data', train=False, download=True, transform=transforms.Compose([\n",
    "                       transforms.Resize(32),\n",
    "                       transforms.ToTensor()\n",
    "                   ]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StandardTransform\n",
       "Transform: Compose(\n",
       "               Resize(size=32, interpolation=PIL.Image.BILINEAR)\n",
       "               ToTensor()\n",
       "           )"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist_dataset_test.transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data.sampler import BatchSampler\n",
    "\n",
    "\n",
    "class SiameseMNIST(Dataset):\n",
    "    \"\"\"\n",
    "    Train: For each sample creates randomly a positive or a negative pair\n",
    "    Test: Creates fixed pairs for testing\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, mnist_dataset):\n",
    "        self.mnist_dataset = mnist_dataset\n",
    "\n",
    "        self.train = self.mnist_dataset.train\n",
    "        self.transform = self.mnist_dataset.transform\n",
    "\n",
    "        if self.train:\n",
    "            self.train_labels = self.mnist_dataset.train_labels\n",
    "            self.train_data = self.mnist_dataset.train_data\n",
    "            self.labels_set = set(self.train_labels.numpy())\n",
    "            self.label_to_indices = {label: np.where(self.train_labels.numpy() == label)[0]\n",
    "                                     for label in self.labels_set}\n",
    "        else:\n",
    "            # generate fixed pairs for testing\n",
    "            self.test_labels = self.mnist_dataset.test_labels\n",
    "            self.test_data = self.mnist_dataset.test_data\n",
    "            self.labels_set = set(self.test_labels.numpy())\n",
    "            self.label_to_indices = {label: np.where(self.test_labels.numpy() == label)[0]\n",
    "                                     for label in self.labels_set}\n",
    "\n",
    "            random_state = np.random.RandomState(29)\n",
    "\n",
    "            positive_pairs = [[i,\n",
    "                               random_state.choice(self.label_to_indices[self.test_labels[i].item()]),\n",
    "                               1]\n",
    "                              for i in range(0, len(self.test_data), 2)]\n",
    "\n",
    "            negative_pairs = [[i,\n",
    "                               random_state.choice(self.label_to_indices[\n",
    "                                                       np.random.choice(\n",
    "                                                           list(self.labels_set - set([self.test_labels[i].item()]))\n",
    "                                                       )\n",
    "                                                   ]),\n",
    "                               0]\n",
    "                              for i in range(1, len(self.test_data), 2)]\n",
    "            self.test_pairs = positive_pairs + negative_pairs\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        if self.train:\n",
    "            target = np.random.randint(0, 2)\n",
    "            img1, label1 = self.train_data[index], self.train_labels[index].item()\n",
    "            if target == 1:\n",
    "                siamese_index = index\n",
    "                while siamese_index == index:\n",
    "                    siamese_index = np.random.choice(self.label_to_indices[label1])\n",
    "            else:\n",
    "                siamese_label = np.random.choice(list(self.labels_set - set([label1])))\n",
    "                siamese_index = np.random.choice(self.label_to_indices[siamese_label])\n",
    "            img2 = self.train_data[siamese_index]\n",
    "        else:\n",
    "            img1 = self.test_data[self.test_pairs[index][0]]\n",
    "            img2 = self.test_data[self.test_pairs[index][1]]\n",
    "            target = self.test_pairs[index][2]\n",
    "\n",
    "        img1 = Image.fromarray(img1.numpy(), mode='L')\n",
    "        img2 = Image.fromarray(img2.numpy(), mode='L')\n",
    "        if self.transform is not None:\n",
    "            img1 = self.transform(img1)\n",
    "            img2 = self.transform(img2)\n",
    "        return target, img1, img2\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.mnist_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/bvaa/lib/python3.6/site-packages/torchvision/datasets/mnist.py:43: UserWarning: train_labels has been renamed targets\n",
      "  warnings.warn(\"train_labels has been renamed targets\")\n",
      "/opt/miniconda3/envs/bvaa/lib/python3.6/site-packages/torchvision/datasets/mnist.py:53: UserWarning: train_data has been renamed data\n",
      "  warnings.warn(\"train_data has been renamed data\")\n",
      "/opt/miniconda3/envs/bvaa/lib/python3.6/site-packages/torchvision/datasets/mnist.py:48: UserWarning: test_labels has been renamed targets\n",
      "  warnings.warn(\"test_labels has been renamed targets\")\n",
      "/opt/miniconda3/envs/bvaa/lib/python3.6/site-packages/torchvision/datasets/mnist.py:58: UserWarning: test_data has been renamed data\n",
      "  warnings.warn(\"test_data has been renamed data\")\n"
     ]
    }
   ],
   "source": [
    "mnist_siamese_dataloader = SiameseMNIST(mnist_dataset)\n",
    "mnist_siamese_dataloader_test = SiameseMNIST(mnist_dataset_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VAE(\n",
       "  (encoder): Encoder(\n",
       "    (conv1): Conv2d(1, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (conv2): Conv2d(32, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (conv3): Conv2d(32, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (lin1): Linear(in_features=512, out_features=256, bias=True)\n",
       "    (lin2): Linear(in_features=256, out_features=256, bias=True)\n",
       "    (mu_logvar_gen): Linear(in_features=256, out_features=24, bias=True)\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (lin1): Linear(in_features=12, out_features=256, bias=True)\n",
       "    (lin2): Linear(in_features=256, out_features=256, bias=True)\n",
       "    (lin3): Linear(in_features=256, out_features=512, bias=True)\n",
       "    (convT1): ConvTranspose2d(32, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (convT2): ConvTranspose2d(32, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (convT3): ConvTranspose2d(32, 1, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Config:\n",
    "    def __init__(self, image_size=28, mode='train', model_path='./model/', \n",
    "                 generate_path='./model/', num_epochs=100, distance_weight=1.0, dataset='MNIST', \n",
    "                 data_loader=mnist_siamese_dataloader, tensorboard=True, generator=model):\n",
    "        self.mode = mode\n",
    "        self.image_size = image_size\n",
    "        self.model_path = model_path\n",
    "        self.generate_path = generate_path\n",
    "        self.dataset = dataset\n",
    "        self.num_epochs = num_epochs\n",
    "        self.distance_weight = distance_weight\n",
    "        self.tensorboard = tensorboard\n",
    "        self.generator = generator\n",
    "        self.data_loader = data_loader\n",
    "\n",
    "config = Config()\n",
    "config.generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SiameseGanSolver(object):\n",
    "    \"\"\"Solving GAN-like neural network with siamese discriminator.\"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        \"\"\"Set parameters of neural network and its training.\"\"\"\n",
    "        self.ssim_loss = SSIM()\n",
    "        self.generator = config.generator\n",
    "        self.discriminator = None\n",
    "        self.distance_based_loss = None\n",
    "\n",
    "        self.g_optimizer = None\n",
    "        self.d_optimizer = None\n",
    "\n",
    "        self.g_conv_dim = 128\n",
    "\n",
    "        self.beta1 = 0.9\n",
    "        self.beta2 = 0.999\n",
    "        self.learning_rate = 0.0001\n",
    "        self.image_size = config.image_size\n",
    "        self.num_epochs = config.num_epochs\n",
    "        self.distance_weight = config.distance_weight\n",
    "\n",
    "        self.data_loader = config.data_loader\n",
    "        self.generate_path = config.generate_path\n",
    "        self.model_path = config.model_path\n",
    "        self.tensorboard = config.tensorboard\n",
    "\n",
    "        if self.tensorboard:\n",
    "            self.tb_writer = tensorboardX.SummaryWriter(\n",
    "                filename_suffix='_%s_%s' % (config.distance_weight, config.dataset))\n",
    "            self.tb_graph_added = False\n",
    "\n",
    "        self.build_model()\n",
    "\n",
    "    def build_model(self):\n",
    "        \"\"\"Build generator and discriminator.\"\"\"\n",
    "#         self.generator = Generator(self.g_conv_dim, noise=self.noise, residual=self.residual)\n",
    "        self.discriminator = siamese(self.image_size)\n",
    "        self.distance_based_loss = DistanceBasedLoss(2.0)\n",
    "\n",
    "        self.g_optimizer = torch.optim.Adam(\n",
    "            self.generator.parameters(), self.learning_rate, [self.beta1, self.beta2])\n",
    "        self.d_optimizer = torch.optim.Adam(\n",
    "            self.discriminator.parameters(), self.learning_rate, [self.beta1, self.beta2])\n",
    "\n",
    "        if torch.cuda.is_available():\n",
    "            self.generator.cuda()\n",
    "            self.discriminator.cuda()\n",
    "            self.distance_based_loss.cuda()\n",
    "\n",
    "    def train(self):\n",
    "        \"\"\"Train generator and discriminator in minimax game.\"\"\"\n",
    "        # Prepare tensorboard writer\n",
    "        if self.tensorboard:\n",
    "            step = 0\n",
    "\n",
    "        for epoch in range(self.num_epochs):\n",
    "            print(str(epoch) + \" \" + str(datetime.now()))\n",
    "\n",
    "            for label, images0, images1 in self.data_loader:\n",
    "                images0 = to_variable(images0)\n",
    "                images1 = to_variable(images1)\n",
    "                label = to_variable(label)\n",
    "\n",
    "                # Train discriminator to recognize identity of real images\n",
    "                output0, output1 = self.discriminator(images0, images1)\n",
    "                d_real_loss = self.distance_based_loss(output0, output1, label)\n",
    "\n",
    "                # Backpropagation\n",
    "                self.distance_based_loss.zero_grad()\n",
    "                self.discriminator.zero_grad()\n",
    "                d_real_loss.backward()\n",
    "                self.d_optimizer.step()\n",
    "\n",
    "                # Train discriminator to recognize identity of fake(privatized) images\n",
    "                privatized_imgs = self.generator(images0)\n",
    "                output0, output1 = self.discriminator(images0, privatized_imgs)\n",
    "\n",
    "                # Discriminator wants to minimize Euclidean distance between\n",
    "                # original & privatized versions, hence label = 0\n",
    "                d_fake_loss = self.distance_based_loss(output0, output1, 0)\n",
    "                distance = 1.0 - self.ssim_loss(privatized_imgs, images0)\n",
    "                d_fake_loss += self.distance_weight * distance\n",
    "\n",
    "                # Backpropagation\n",
    "                self.distance_based_loss.zero_grad()\n",
    "                self.discriminator.zero_grad()\n",
    "                self.generator.zero_grad()\n",
    "                d_fake_loss.backward()\n",
    "                self.d_optimizer.step()\n",
    "\n",
    "                # Train generator to fool discriminator\n",
    "                # Generator wants to push the distance between original & privatized\n",
    "                # right to the margin, hence label = 1\n",
    "                privatized_imgs = self.generator(images0)\n",
    "                output0, output1 = self.discriminator(images0, privatized_imgs)\n",
    "                g_loss = self.distance_based_loss(output0, output1, 1)\n",
    "                distance = 1.0 - self.ssim_loss(privatized_imgs, images0)\n",
    "                g_loss += self.distance_weight * distance\n",
    "\n",
    "                # Backpropagation\n",
    "                self.distance_based_loss.zero_grad()\n",
    "                self.discriminator.zero_grad()\n",
    "                self.generator.zero_grad()\n",
    "                g_loss.backward()\n",
    "                self.g_optimizer.step()\n",
    "\n",
    "                # Write losses to tensorboard\n",
    "                if self.tensorboard:\n",
    "                    self.tb_writer.add_scalar('phase0/discriminator_real_loss',\n",
    "                                              d_real_loss.data[0], step)\n",
    "                    self.tb_writer.add_scalar('phase0/discriminator_fake_loss',\n",
    "                                              d_fake_loss.data[0], step)\n",
    "                    self.tb_writer.add_scalar('phase0/generator_loss',\n",
    "                                              g_loss.data[0], step)\n",
    "                    self.tb_writer.add_scalar('phase0/distance_loss',\n",
    "                                              distance.data[0], step)\n",
    "\n",
    "                    step += 1\n",
    "\n",
    "            # Monitor training after each epoch\n",
    "            if self.tensorboard:\n",
    "                self._monitor_phase_0(self.tb_writer, step)\n",
    "\n",
    "            # At the end save generator and discriminator to files\n",
    "            if (epoch + 1) % 10 == 0:\n",
    "                g_path = os.path.join(self.model_path, 'generator-%d.pt' % (epoch+1))\n",
    "                torch.save(self.generator.state_dict(), g_path)\n",
    "                d_path = os.path.join(self.model_path, 'discriminator-%d.pt' % (epoch+1))\n",
    "                torch.save(self.discriminator.state_dict(), d_path)\n",
    "\n",
    "        if self.tensorboard:\n",
    "            self.tb_writer.close()\n",
    "\n",
    "    def _monitor_phase_0(self, writer, step, n_images=10):\n",
    "        \"\"\"Monitor discriminator's accuracy, generate preview images of generator.\"\"\"\n",
    "        # Measure accuracy of identity verification by discriminator\n",
    "        correct_pairs = 0\n",
    "        total_pairs = 0\n",
    "\n",
    "        for label, images0, images1 in self.data_loader:\n",
    "            images0 = to_variable(images0)\n",
    "            images1 = to_variable(images1)\n",
    "            label = to_variable(label)\n",
    "\n",
    "            # Predict label = 1 if outputs are dissimilar (distance > margin)\n",
    "            privatized_images0 = self.generator(images0)\n",
    "            output0, output1 = self.discriminator(privatized_images0, images1)\n",
    "            predictions = self.distance_based_loss.predict(output0, output1)\n",
    "            predictions = predictions.type(label.data.type())\n",
    "\n",
    "            correct_pairs += (predictions == label).sum().data[0]\n",
    "            total_pairs += len(predictions == label)\n",
    "\n",
    "            if total_pairs > 1000:\n",
    "                break\n",
    "\n",
    "        # Write accuracy to tensorboard\n",
    "        accuracy = correct_pairs / total_pairs\n",
    "        writer.add_scalar('phase0/discriminator_accuracy', accuracy, step)\n",
    "\n",
    "        # Generate previews of privatized images\n",
    "        reals, fakes = [], []\n",
    "        for _, image, _ in self.data_loader.dataset:\n",
    "            image = image.unsqueeze(0)\n",
    "            reals.append(denorm(to_variable(image).data)[0])\n",
    "            fakes.append(denorm(self.generator(to_variable(image)).data)[0])\n",
    "            if len(reals) == n_images:\n",
    "                break\n",
    "\n",
    "        # Write images to tensorboard\n",
    "        real_previews = torchvision.utils.make_grid(reals, nrow=n_images)\n",
    "        fake_previews = torchvision.utils.make_grid(fakes, nrow=n_images)\n",
    "        img = torchvision.utils.make_grid([real_previews, fake_previews], nrow=1)\n",
    "        writer.add_image('Previews', img, step)\n",
    "\n",
    "    def generate(self):\n",
    "        \"\"\"Generate privatized images.\"\"\"\n",
    "        # Load trained parameters (generator)\n",
    "        g_path = os.path.join(self.model_path, 'generator-%d.pkl' % self.num_epochs)\n",
    "        self.generator.load_state_dict(torch.load(g_path))\n",
    "        self.generator.eval()\n",
    "\n",
    "        # Generate the images\n",
    "        for relative_path, image in self.data_loader:\n",
    "            fake_image = self.generator(to_variable(image))\n",
    "            fake_path = os.path.join(self.generate_path, relative_path[0])\n",
    "            if not os.path.exists(os.path.dirname(fake_path)):\n",
    "                os.makedirs(os.path.dirname(fake_path))\n",
    "            torchvision.utils.save_image(fake_image.data, fake_path, nrow=1)\n",
    "\n",
    "    def check_discriminator_accuracy(self):\n",
    "        \"\"\"Measure discriminator's accuracy.\"\"\"\n",
    "        # Measure accuracy of identity verification by discriminator\n",
    "        correct_pairs = 0\n",
    "        total_pairs = 0\n",
    "\n",
    "        g_path = os.path.join(self.model_path, 'generator-%d.pkl' % self.num_epochs)\n",
    "        self.generator.load_state_dict(torch.load(g_path))\n",
    "        self.generator.eval()\n",
    "\n",
    "        d_path = os.path.join(self.model_path, 'discriminator-%d.pkl' % self.num_epochs)\n",
    "        self.discriminator.load_state_dict(torch.load(d_path))\n",
    "        self.discriminator.eval()\n",
    "\n",
    "        for label, images0, images1 in self.data_loader:\n",
    "            images0 = to_variable(images0)\n",
    "            images1 = to_variable(images1)\n",
    "            label = to_variable(label)\n",
    "\n",
    "            # Predict label = 1 if outputs are dissimilar (distance > margin)\n",
    "            privatized_images0 = self.generator(images0)\n",
    "            output0, output1 = self.discriminator(privatized_images0, images1)\n",
    "            predictions = self.distance_based_loss.predict(output0, output1)\n",
    "            predictions = predictions.type(label.data.type())\n",
    "\n",
    "            correct_pairs += (predictions == label).sum().data[0]\n",
    "            total_pairs += len(predictions)\n",
    "\n",
    "        accuracy = correct_pairs / total_pairs\n",
    "        print('distance weight = %f' % self.distance_weight)\n",
    "        print('accuracy = %f' % accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def denorm(image):\n",
    "    \"\"\"Convert image range (-1, 1) to (0, 1).\"\"\"\n",
    "    out = (image + 1) / 2\n",
    "    return out.clamp(0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_variable(tensor):\n",
    "    \"\"\"Convert tensor to variable.\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        tensor = tensor.cuda()\n",
    "    return torch.Tensor(tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorboardX\n",
    "solver = SiameseGanSolver(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "solver.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
