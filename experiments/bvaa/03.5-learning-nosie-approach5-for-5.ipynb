{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from torchvision.utils import save_image\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from timeit import default_timer\n",
    "from collections import defaultdict\n",
    "from tqdm import trange\n",
    "\n",
    "class Trainer():\n",
    "    def __init__(self, model, optimizer, loss_f, device, save_dir=\"results\", is_progress_bar=True):\n",
    "            self.device = device\n",
    "            self.model = model.to(device)\n",
    "            self.loss_f = loss_f\n",
    "            self.optimizer = optimizer\n",
    "            self.save_dir = save_dir\n",
    "            self.is_progress_bar = is_progress_bar\n",
    "            \n",
    "    def __call__(self, data_loader, epochs=10, checkpoint_every=10):\n",
    "        start = default_timer()\n",
    "        self.model.train()\n",
    "        for epoch in range(epochs):\n",
    "            storer = defaultdict(list)\n",
    "            mean_epoch_loss = self._train_epoch(data_loader, storer, epoch)\n",
    "            mean_epoch_loss = self._test_epoch(data_loader, storer, epoch)\n",
    "            with torch.no_grad():\n",
    "                sample = torch.randn(64, self.model.latent_dim).to(device)\n",
    "                sample = self.model.decoder(sample).cpu()  # make sure on cpu\n",
    "                save_image(sample.view(64, 1, 32, 32),\n",
    "                           './results/samples/' + str(epoch) + '.png')\n",
    "            \n",
    "    def _train_epoch(self, data_loader, storer, epoch):\n",
    "        epoch_loss = 0.\n",
    "        kwargs = dict(desc=\"Epoch {}\".format(epoch + 1), leave=False,\n",
    "                      disable=not self.is_progress_bar)\n",
    "        with trange(len(data_loader), **kwargs) as t:\n",
    "            for _, (data, _) in enumerate(data_loader):\n",
    "                iter_loss = self._train_iteration(data, storer)\n",
    "                epoch_loss += iter_loss\n",
    "                t.set_postfix(loss=iter_loss)\n",
    "                t.update()\n",
    "        mean_epoch_loss = epoch_loss / len(data_loader)\n",
    "        return mean_epoch_loss\n",
    "    \n",
    "    def _train_iteration(self, data, storer):\n",
    "        batch_size, channel, height, width = data.size()\n",
    "        data = data.to(self.device)\n",
    "        recon_batch, latent_dist, latent_sample = self.model(data)\n",
    "        loss = self.loss_f(data, recon_batch, latent_dist, self.model.training, \n",
    "                           storer, latent_sample=latent_sample)\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "            \n",
    "        return loss.item()\n",
    "    \n",
    "    def _test_epoch(self, data_loader, storer, epoch):\n",
    "        epoch_loss = 0.\n",
    "        kwargs = dict(desc=\"Epoch {}\".format(epoch + 1), leave=False,\n",
    "                      disable=not self.is_progress_bar)\n",
    "        with trange(len(data_loader), **kwargs) as t:\n",
    "            for _, (data, _) in enumerate(data_loader):\n",
    "                iter_loss = self._train_iteration(data, storer)\n",
    "                epoch_loss += iter_loss\n",
    "                t.set_postfix(loss=iter_loss)\n",
    "                t.update()\n",
    "        mean_epoch_loss = epoch_loss / len(data_loader)\n",
    "        return mean_epoch_loss\n",
    "    \n",
    "    def _test_iteration(self, data, storer):\n",
    "        batch_size, channel, height, width = data.size()\n",
    "        data = data.to(self.device)\n",
    "        recon_batch, latent_dist, latent_sample = self.model(data)\n",
    "        loss = self.loss_f(data, recon_batch, latent_dist, self.model.training, \n",
    "                               storer, latent_sample=latent_sample)\n",
    "            \n",
    "        return loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOSSES = [\"betaH\", \"betaB\"]\n",
    "RECON_DIST = [\"bernoulli\", \"laplace\", \"gaussian\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "mnist_dataset = datasets.MNIST('/home/data/bvaa', \n",
    "                   train=True, \n",
    "                   download=True, \n",
    "                   transform=transforms.Compose([\n",
    "                       transforms.Resize(32),\n",
    "                       transforms.ToTensor()\n",
    "                   ]))\n",
    "\n",
    "mnist_dataset_test = datasets.MNIST('/home/data/bvaa', train=False, download=True, transform=transforms.Compose([\n",
    "                       transforms.Resize(32),\n",
    "                       transforms.ToTensor()\n",
    "                   ]))\n",
    "\n",
    "train_loader = DataLoader(mnist_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "test_loader = DataLoader(mnist_dataset_test, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = enumerate(test_loader)\n",
    "batch_idx, (example_data, example_targets) = next(examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vae import VAE\n",
    "from encoder import Encoder\n",
    "from decoder import Decoder\n",
    "from losses import get_loss_fn\n",
    "from torch import optim\n",
    "\n",
    "latent_dim = 12\n",
    "img_size = [1,32,32]\n",
    "\n",
    "lr = 5e-4\n",
    "\n",
    "betaB_args = {\"rec_dist\": \"bernoulli\",\n",
    "              \"reg_anneal\": 10000, \n",
    "              \"betaH_B\": 4,\n",
    "              \"betaB_initC\": 0,\n",
    "              \"betaB_finC\": 25,\n",
    "              \"betaB_G\": 100\n",
    "             }\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "loss_f = get_loss_fn(\"betaB\", n_data=len(train_loader.dataset), device=device, **betaB_args)\n",
    "\n",
    "encoder = Encoder(img_size, latent_dim)\n",
    "decoder = Decoder(img_size, latent_dim)\n",
    "\n",
    "generator_model = VAE(img_size, latent_dim, encoder, decoder).to(device)\n",
    "optimizer = optim.Adam(generator_model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer = Trainer(generator_model, optimizer, loss_f, device, logger=logger,\\\n",
    "#                   save_dir=exp_dir, is_progress_bar=False)\n",
    "# epochs = 100\n",
    "# checkpoint_every = 10\n",
    "# trainer(train_loader, epochs=epochs, checkpoint_every=checkpoint_every)\n",
    "# torch.save(trainer.model.state_dict(), 'test.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generator_model.load_state_dict(torch.load('models/test.pt'))\n",
    "# generator_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Classifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Classifier, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, 3, 1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, 1)\n",
    "        self.dropout1 = nn.Dropout2d(0.25)\n",
    "        self.dropout2 = nn.Dropout2d(0.5)\n",
    "        self.fc1 = nn.Linear(9216, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = self.dropout1(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.fc2(x)\n",
    "        output = F.softmax(x, dim=1)\n",
    "        return output\n",
    "\n",
    "classifier = Classifier()\n",
    "classifier.load_state_dict(torch.load('models/mnist_cnn_non_log.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AddNoise():\n",
    "    def __init__(self, shape, device, noise=None, percent_noise=0.1):\n",
    "        self.device = device\n",
    "        self.percent_noise = percent_noise\n",
    "        if noise is None:\n",
    "            self.noise = torch.randn(shape)\n",
    "        else:\n",
    "            self.noise = noise\n",
    "\n",
    "    def noisy(self, data):\n",
    "        x = self.noise.to(self.device) * self.percent_noise + data * (1 - self.percent_noise)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Noise for Testing: \n",
      " tensor([[ 0.7344,  0.0049,  0.1108, -0.9742,  0.7806,  0.4000, -0.6167, -1.0133,\n",
      "          0.4483, -0.2564, -0.7429,  0.7341]])\n"
     ]
    }
   ],
   "source": [
    "sample_noise = torch.randn(torch.Size([1, 12]))\n",
    "backup_sample_noise = sample_noise\n",
    "print(\"Sample Noise for Testing: \\n\", sample_noise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Decoder(\n",
       "  (lin1): Linear(in_features=12, out_features=256, bias=True)\n",
       "  (lin2): Linear(in_features=256, out_features=256, bias=True)\n",
       "  (lin3): Linear(in_features=256, out_features=512, bias=True)\n",
       "  (convT1): ConvTranspose2d(32, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "  (convT2): ConvTranspose2d(32, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "  (convT3): ConvTranspose2d(32, 1, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder = generator_model.decoder\n",
    "decoder.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Classifier(\n",
       "  (conv1): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (dropout1): Dropout2d(p=0.25, inplace=False)\n",
       "  (dropout2): Dropout2d(p=0.5, inplace=False)\n",
       "  (fc1): Linear(in_features=9216, out_features=128, bias=True)\n",
       "  (fc2): Linear(in_features=128, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import exp\n",
    "\n",
    "def gaussian(window_size, sigma):\n",
    "    gauss = torch.Tensor([exp(-(x - window_size//2)**2/float(2*sigma**2)) for x in range(window_size)])\n",
    "    return gauss/gauss.sum()\n",
    "\n",
    "def create_window(window_size, channel):\n",
    "    _1D_window = gaussian(window_size, 1.5).unsqueeze(1)\n",
    "    _2D_window = _1D_window.mm(_1D_window.t()).float().unsqueeze(0).unsqueeze(0)\n",
    "    window = torch.Tensor(_2D_window.expand(channel, 1, window_size, window_size).contiguous())\n",
    "    return window\n",
    "\n",
    "def _ssim(img1, img2, window, window_size, channel, size_average = True):\n",
    "    mu1 = F.conv2d(img1, window, padding = window_size//2, groups = channel)\n",
    "    mu2 = F.conv2d(img2, window, padding = window_size//2, groups = channel)\n",
    "\n",
    "    mu1_sq = mu1.pow(2)\n",
    "    mu2_sq = mu2.pow(2)\n",
    "    mu1_mu2 = mu1*mu2\n",
    "\n",
    "    sigma1_sq = F.conv2d(img1*img1, window, padding = window_size//2, groups = channel) - mu1_sq\n",
    "    sigma2_sq = F.conv2d(img2*img2, window, padding = window_size//2, groups = channel) - mu2_sq\n",
    "    sigma12 = F.conv2d(img1*img2, window, padding = window_size//2, groups = channel) - mu1_mu2\n",
    "\n",
    "    C1 = 0.01**2\n",
    "    C2 = 0.03**2\n",
    "\n",
    "    ssim_map = ((2*mu1_mu2 + C1)*(2*sigma12 + C2))/((mu1_sq + mu2_sq + C1)*(sigma1_sq + sigma2_sq + C2))\n",
    "\n",
    "    if size_average:\n",
    "        return ssim_map.mean()\n",
    "    else:\n",
    "        return ssim_map.mean(1).mean(1).mean(1)\n",
    "\n",
    "class SSIM(nn.Module):\n",
    "    def __init__(self, window_size = 11, size_average = True):\n",
    "        super(SSIM, self).__init__()\n",
    "        self.window_size = window_size\n",
    "        self.size_average = size_average\n",
    "        self.channel = 1\n",
    "        self.window = create_window(window_size, self.channel)\n",
    "\n",
    "    def forward(self, img1, img2):\n",
    "        (_, channel, _, _) = img1.size()\n",
    "\n",
    "        if channel == self.channel and self.window.data.type() == img1.data.type():\n",
    "            window = self.window\n",
    "        else:\n",
    "            window = create_window(self.window_size, channel)\n",
    "            \n",
    "            if img1.is_cuda:\n",
    "                window = window.cuda(img1.get_device())\n",
    "            window = window.type_as(img1)\n",
    "            \n",
    "            self.window = window\n",
    "            self.channel = channel\n",
    "\n",
    "\n",
    "        return _ssim(img1, img2, window, self.window_size, channel, self.size_average)\n",
    "\n",
    "def ssim(img1, img2, window_size = 11, size_average = True):\n",
    "    (_, channel, _, _) = img1.size()\n",
    "    window = create_window(window_size, channel)\n",
    "    \n",
    "    if img1.is_cuda:\n",
    "        window = window.cuda(img1.get_device())\n",
    "    window = window.type_as(img1)\n",
    "    \n",
    "    return _ssim(img1, img2, window, window_size, channel, size_average)\n",
    "\n",
    "ssim_loss = SSIM(window_size = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constrained Translator\n",
    "class Translator(nn.Module):\n",
    "    def __init__(self, hidden_layers=[5, 10, 5], latent_dim=latent_dim):\n",
    "        super(Translator, self).__init__()\n",
    "        self.hidden_layers = hidden_layers\n",
    "        self.hidden_layers.insert(0, latent_dim)\n",
    "        self.hidden_layers.append(latent_dim)\n",
    "        self.layers = []\n",
    "        \n",
    "        for i in range(len(self.hidden_layers)-1):\n",
    "            self.layers.append(nn.Linear(self.hidden_layers[i], self.hidden_layers[i+1]))\n",
    "        \n",
    "        self.layers = nn.ModuleList(self.layers)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        l_sample = x\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        noised_sample = x\n",
    "        noised_sample = 1 * ((l_sample - l_sample.min())/(l_sample.max() - l_sample.min())) + 1e-3 * ((noised_sample - noised_sample.min())/(noised_sample.max() - noised_sample.min()))\n",
    "        return noised_sample\n",
    "\n",
    "translator = Translator().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from skimage.metrics import structural_similarity as ssim\n",
    "\n",
    "def create_logits(target_label, pred, confidence=0.5, device=device):\n",
    "    logits = torch.zeros(pred.shape, dtype=torch.float64)\n",
    "    pred_labels = [int(element.item()) for element in torch.argmax(pred, dim=1)]\n",
    "#     print(\"length\", len(pred_labels))\n",
    "#     print(\"pred_lables:\",pred_labels)\n",
    "#     print(logits.shape)\n",
    "    logits[range(logits.shape[0]), pred_labels] = torch.DoubleTensor([1-confidence]*pred.shape[0])\n",
    "    logits[range(logits.shape[0]), [target_label]*pred.shape[0]] += torch.DoubleTensor([confidence]*pred.shape[0])\n",
    "    return logits.to(device)\n",
    "\n",
    "def structural(org_image, noised_image):\n",
    "    batch_size, channels, width, height = org_image.shape\n",
    "    loss1 = 0\n",
    "    for b_ in range(batch_size):\n",
    "        ch_loss = 0\n",
    "        for ch_ in range(channels):\n",
    "            ch_loss += 1-ssim(org_image[b_][ch_].detach().cpu().numpy(), noised_image[b_][ch_].detach().cpu().numpy())\n",
    "        loss1 += ch_loss/channels\n",
    "    return loss1\n",
    "            \n",
    "class T_Loss(nn.Module):\n",
    "    def __init__(self, decoder=decoder, classifier=classifier):\n",
    "        super(T_Loss, self).__init__()\n",
    "        self.decoder = decoder\n",
    "        self.classifier = classifier\n",
    "    def forward(self, x, org_x, target_label):\n",
    "        \n",
    "        org_image = self.decoder(org_x)\n",
    "        noised_image = self.decoder(x)\n",
    "        batch_size, channels, width, height = org_image.shape\n",
    "        loss1 = ssim_loss(org_image, noised_image)\n",
    "#         print(type(loss1))\n",
    "#         print(loss1)\n",
    "#         loss1 = torch.Tensor(loss1 / batch_size).to(device)\n",
    "#         print(\"loss1:\", loss1)\n",
    "        preds = self.classifier(F.upsample(noised_image, (28,28), mode='bilinear', align_corners=True))\n",
    "#         print(\"preds:\",preds)\n",
    "        target = create_logits(target_label, preds)\n",
    "#         print(target)\n",
    "        loss2 = nn.BCELoss(reduction='sum')(preds, target.float())\n",
    "#         print(type(loss2))\n",
    "#         print(\"loss1:\",100*(1-loss1))\n",
    "#         print(\"loss2:\",loss2)\n",
    "        loss = 100*(1-loss1) + loss2\n",
    "        \n",
    "        \n",
    "        out_labels = preds.argmax(dim=1, keepdim=True)\n",
    "#         print(out_labels)\n",
    "#         print(torch.empty(out_labels.shape).fill_(target_label))\n",
    "#         print(preds)\n",
    "#         correct = out_labels.eq(torch.Tensor([target_label]*out_labels.shape[0]).to(device)).sum()\n",
    "        correct = out_labels.eq(torch.empty(out_labels.shape).fill_(target_label).to(device)).sum()\n",
    "#         print(out_labels.shape)\n",
    "#         print(correct)\n",
    "#         print(torch.Tensor([target_label]*out_labels.shape[0]))\n",
    "#         print(out_labels)\n",
    "        return loss, correct.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "T_Loss(\n",
       "  (decoder): Decoder(\n",
       "    (lin1): Linear(in_features=12, out_features=256, bias=True)\n",
       "    (lin2): Linear(in_features=256, out_features=256, bias=True)\n",
       "    (lin3): Linear(in_features=256, out_features=512, bias=True)\n",
       "    (convT1): ConvTranspose2d(32, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (convT2): ConvTranspose2d(32, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (convT3): ConvTranspose2d(32, 1, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "  )\n",
       "  (classifier): Classifier(\n",
       "    (conv1): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (dropout1): Dropout2d(p=0.25, inplace=False)\n",
       "    (dropout2): Dropout2d(p=0.5, inplace=False)\n",
       "    (fc1): Linear(in_features=9216, out_features=128, bias=True)\n",
       "    (fc2): Linear(in_features=128, out_features=10, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tloss = T_Loss().to(device)\n",
    "tloss.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "938"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'translator' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-185092d6cb20>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mattack_log_interval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0malt_target\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtranslator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtranslator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'translator' is not defined"
     ]
    }
   ],
   "source": [
    "attack_log_interval = 1\n",
    "alt_target = 2\n",
    "translator.train()\n",
    "optimizer = optim.Adam(translator.parameters(), lr=1e-3)\n",
    "for epoch in tqdm(range(10)):\n",
    "    epoch_loss = 0\n",
    "    epoch_correct = 0\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data = torch.FloatTensor(data).to(device)\n",
    "        \n",
    "        _, l_dist, l_sample = generator_model(data)\n",
    "        \n",
    "        noised_sample = translator(l_sample)\n",
    "        loss, correct = tloss(noised_sample, l_sample, alt_target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "#         print(correct)\n",
    "        epoch_loss += loss.item()\n",
    "        epoch_correct += correct\n",
    "        \n",
    "    if (epoch+1) % attack_log_interval == 0:\n",
    "        print('Train Epoch: {}\\tLoss: {:.6f}\\tCorrect: {}'.format(\n",
    "            epoch, epoch_loss/batch_idx, epoch_correct))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = enumerate(test_loader)\n",
    "batch_idx, (example_data, example_targets) = next(examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(i):\n",
    "    _, l_dist, l_sample = generator_model(example_data[i].unsqueeze_(0).to(device))\n",
    "    noised_sample = translator(l_sample)\n",
    "    print(noised_sample)\n",
    "    print(l_sample)\n",
    "    noised_sample = 2 * ((l_sample - l_sample.min())/(l_sample.max() - l_sample.min())) + 1e-2 * ((noised_sample - noised_sample.min())/(noised_sample.max() - noised_sample.min()))\n",
    "    final = generator_model.decoder(noised_sample)\n",
    "    pred = torch.argmax(classifier(F.upsample(final, (28,28), mode='bilinear', align_corners=True)))\n",
    "    print(\"Prediction: \", pred.item())\n",
    "    plt.figure(figsize=(10,10))\n",
    "    plt.subplot(1,2,1)\n",
    "    plt.imshow(example_data[i][0].detach().cpu().numpy(), cmap='gray', interpolation='none')\n",
    "    plt.subplot(1,2,2)\n",
    "    plt.imshow(final[0][0].detach().cpu().numpy(), cmap='gray', interpolation='none')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[6.5797e-01, 2.0007e+00, 1.0788e+00, 6.4583e-01, 1.0000e-03, 5.6235e-01,\n",
      "         1.0111e+00, 8.4665e-01, 1.0911e+00, 5.5779e-01, 3.4675e-01, 1.9165e-01]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor([[ 0.0505,  1.8535,  0.6157,  0.0346, -0.8329, -0.0789,  0.5243,  0.3033,\n",
      "          0.6313, -0.0837, -0.3673, -0.5759]], device='cuda:0',\n",
      "       grad_fn=<AddBackward0>)\n",
      "Prediction:  2\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "test(11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[6.0582e-02, 2.0006e+00, 1.0793e+00, 6.8188e-01, 7.4677e-01, 7.2310e-01,\n",
      "         1.7885e+00, 7.5323e-01, 7.6668e-01, 9.4047e-01, 1.3631e+00, 3.4093e-04]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor([[-1.0774,  1.4952,  0.2737, -0.2530, -0.1683, -0.1996,  1.2138, -0.1593,\n",
      "         -0.1418,  0.0900,  0.6504, -1.1573]], device='cuda:0',\n",
      "       grad_fn=<AddBackward0>)\n",
      "Prediction:  2\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "test(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
