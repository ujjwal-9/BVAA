{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train a JointVAE model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get MNIST data\n",
    "Build a simple JointVAE model on MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "VAE_TRAIN = False\n",
    "FIND_DIST = False\n",
    "FOLDER = \"dist2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataloaders import get_mnist_dataloaders_attack, get_mnist_dataloaders\n",
    "# train_loader, test_loader = get_mnist_dataloaders_attack(2, 5, train_batch_size=64, test_batch_size=64, path_to_data='/home/data/bvaa')\n",
    "train_loader, test_loader = get_mnist_dataloaders(batch_size=64, path_to_data='/home/data/bvaa')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets, transforms\n",
    "all_transforms = transforms.Compose([\n",
    "        transforms.Resize(32),\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "train_data = datasets.MNIST('/home/data/bvaa/', train=True, download=True,\n",
    "                                transform=all_transforms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define latent distribution of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Latent distribution will be joint distribution of 10 gaussian normal distributions\n",
    "# and one 10 dimensional Gumbel Softmax distribution\n",
    "latent_spec = {'cont': 15,\n",
    "               'disc': [15]}\n",
    "latent_dim = 30"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import VAE\n",
    "\n",
    "model = VAE(latent_spec=latent_spec, img_size=(1, 32, 32), use_cuda=True).cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import optim\n",
    "\n",
    "# Build optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=5e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Classifier(\n",
       "  (conv1): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (dropout1): Dropout2d(p=0.25, inplace=False)\n",
       "  (dropout2): Dropout2d(p=0.5, inplace=False)\n",
       "  (fc1): Linear(in_features=9216, out_features=128, bias=True)\n",
       "  (fc2): Linear(in_features=128, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Classifier, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, 3, 1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, 1)\n",
    "        self.dropout1 = nn.Dropout2d(0.25)\n",
    "        self.dropout2 = nn.Dropout2d(0.5)\n",
    "        self.fc1 = nn.Linear(9216, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = self.dropout1(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.fc2(x)\n",
    "        output = F.softmax(x, dim=1)\n",
    "        return output\n",
    "\n",
    "classifier = Classifier().cuda()\n",
    "classifier.load_state_dict(torch.load('../VAE/models/mnist_cnn_non_log.pt'))\n",
    "classifier.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from training import Trainer\n",
    "\n",
    "# Define the capacities\n",
    "# Continuous channels\n",
    "cont_capacity = [0.0, 6.0, 25000, 30.0]  # Starting at a capacity of 0.0, increase this to 5.0\n",
    "                                         # over 25000 iterations with a gamma of 30.0\n",
    "# Discrete channels\n",
    "disc_capacity = [0.0, 6.0, 25000, 30.0]  # Starting at a capacity of 0.0, increase this to 5.0\n",
    "                                         # over 25000 iterations with a gamma of 30.0\n",
    "\n",
    "# Build a trainer\n",
    "trainer = Trainer(model, optimizer,\n",
    "                  cont_capacity=cont_capacity,\n",
    "                  disc_capacity=disc_capacity, use_cuda=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Build a visualizer which will be passed to trainer to visualize progress during training\n",
    "# from visualize import Visualizer\n",
    "\n",
    "# viz = Visualizer(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model for 10 epochs\n",
    "# Note this should really be a 100 epochs and trained on a GPU, but this is just to demo\n",
    "\n",
    "# trainer.train(train_loader, epochs=100, save_training_gif=('./training.gif', viz))\n",
    "if VAE_TRAIN:\n",
    "    trainer.train(train_loader, epochs=100)\n",
    "    torch.save(model.state_dict(), 'models/vae_new.pth')\n",
    "else:\n",
    "    model.load_state_dict(torch.load('models/vae_new.pth'))\n",
    "    model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "    \n",
    "for param in classifier.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_index = {}\n",
    "for i in range(len(train_data)):\n",
    "    index = train_data[i][1]\n",
    "    if index not in list_index.keys():\n",
    "        list_index[index] = [i]\n",
    "    else:\n",
    "        list_index[index].append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ecaa0a64d3f34ab68923c275843a2b36",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=10.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "l_sample_list = {}\n",
    "def get_average_latent_space(list_index):\n",
    "    for i in tqdm(list_index.keys()):\n",
    "        for j in list_index[i]:\n",
    "            output, l_dist = model(train_data[j][0].unsqueeze(0).cuda())\n",
    "            l_sample_x = model.reparameterize(l_dist)\n",
    "            if i not in l_sample_list.keys():\n",
    "                l_sample_list[i] = [l_sample_x]\n",
    "            else:\n",
    "                l_sample_list[i].append(l_sample_x)\n",
    "        l_sample_list[i] = {'mean':torch.mean(torch.stack(l_sample_list[i]), dim=0),\n",
    "                           'std':torch.std(torch.stack(l_sample_list[i]), dim=0)}\n",
    "        \n",
    "get_average_latent_space(list_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def check_distribution(digit, plot=False):\n",
    "    alpha = torch.clamp(torch.randn(l_sample_list[digit]['std'].shape).cuda(), min=-0.8, max=0.8)\n",
    "    test_sample = l_sample_list[digit]['mean'] + F.normalize(alpha)*l_sample_list[digit]['std']\n",
    "    test = model.decode(test_sample)\n",
    "    preds = torch.argmax(classifier(F.upsample(test, (28,28), mode='bilinear', align_corners=True)))\n",
    "#     print(preds.item())\n",
    "    if plot:\n",
    "        plt.imshow(model.decode(test_sample)[0,0].cpu().detach().numpy(), cmap='gray', interpolation='none')\n",
    "    return test_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def digits_add(digit1, digit2, alpha=1, plot=False):\n",
    "    l_1 = check_distribution(digit1)\n",
    "    l_2 = check_distribution(digit2)\n",
    "    if plot:\n",
    "        test = model.decode(l_1+alpha*l_2)\n",
    "        plt.imshow(test[0,0].cpu().detach().numpy(), cmap='gray', interpolation='none')\n",
    "        preds = classifier(F.upsample(test, (28,28), mode='bilinear', align_corners=True))\n",
    "        print(preds.dtype)\n",
    "        print(torch.argmax(preds).item())\n",
    "    return torch.norm(l_1-l_2), l_1, l_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_avg_dist(digit1, digit2, exp=10000):\n",
    "    cos = nn.CosineSimilarity(dim=1, eps=1e-6)\n",
    "    distance = []\n",
    "    similarity = []\n",
    "    for i in range(exp):\n",
    "        dist, l1, l2 = digits_add(digit1,digit2)\n",
    "        similar = cos(l1,l2)\n",
    "        distance.append(dist.item())\n",
    "        similarity.append(similar.item())\n",
    "    a = round(sum(distance)/len(distance),5)\n",
    "    b = round(sum(similarity)/len(similarity),5)\n",
    "    return a,b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def digit_to_study(digit):\n",
    "    distance = {}\n",
    "    similarity = {}\n",
    "    for i in tqdm(range(10)):\n",
    "        distance[i], similarity[i] = get_avg_dist(digit,i)\n",
    "    return distance, similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_avg_dist(1,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_avg_dist(1,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_avg_dist(2,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.float32\n",
      "2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/bvaa/lib/python3.6/site-packages/torch/nn/functional.py:2416: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.\n",
      "  warnings.warn(\"nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor(1.3718, device='cuda:0'),\n",
       " tensor([[ 3.3309e-01, -3.8314e-02, -6.5622e-02, -1.4633e-02, -6.5476e-03,\n",
       "           1.7618e-01, -1.0145e-01,  9.3876e-02,  1.5382e-02,  2.1972e-02,\n",
       "          -4.2854e-02,  3.1037e-01,  8.6930e-02, -1.3633e-01,  2.9071e-02,\n",
       "           1.7645e-01, -6.9900e-03,  9.1620e-01,  3.7423e-03, -3.1916e-03,\n",
       "           0.0000e+00,  1.6067e-02,  4.6149e-03, -5.0899e-03,  1.0491e-02,\n",
       "          -8.8939e-04, -6.6431e-03,  1.7275e-02,  5.4688e-03, -3.2309e-03]],\n",
       "        device='cuda:0'),\n",
       " tensor([[-1.7060e-01, -3.8732e-01, -7.7967e-02, -1.3907e-02, -8.0544e-03,\n",
       "           1.2041e-01, -8.5630e-02,  7.8320e-03,  2.5647e-02,  7.1336e-03,\n",
       "          -5.2860e-02, -2.1018e-01,  1.2110e-01,  1.5485e-01,  1.1197e-02,\n",
       "           3.5397e-01,  1.2906e-03,  1.5734e-02,  1.4710e-02, -1.0223e-02,\n",
       "           0.0000e+00,  1.0877e-02,  3.2934e-02,  1.6164e-03,  8.7062e-03,\n",
       "           3.3928e-03,  1.1256e-04, -7.9940e-03, -9.5443e-03,  5.3846e-01]],\n",
       "        device='cuda:0'))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "digits_add(2,3,0.56,True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# digits_add(1,3,0.3,True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# digits_add(1,2,True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# digit_to_study(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# digit_to_study(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check_distribution(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if FIND_DIST is True:\n",
    "#     l_means = []\n",
    "#     l_stds = []\n",
    "#     for i in l_sample_list.keys():\n",
    "#         l_means.append(l_sample_list[i]['mean'])\n",
    "#         l_stds.append(l_sample_list[i]['std'])\n",
    "\n",
    "#     l_mean_tensor = torch.stack(l_means).squeeze(1)\n",
    "#     # print(l_mean_tensor.shape)\n",
    "\n",
    "#     l_std_tensor = torch.stack(l_stds).squeeze(1)\n",
    "#     # print(l_std_tensor.shape)\n",
    "\n",
    "#     torch.save(l_mean_tensor, 'tensor/latent_mean.pt')\n",
    "#     torch.save(l_std_tensor, 'tensor/latent_std.pt')\n",
    "\n",
    "# else:\n",
    "#     l_mean_tensor = torch.load('tensor/latent_mean.pt')\n",
    "#     l_std_tensor = torch.load('tensor/latent_std.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = enumerate(test_loader)\n",
    "batch_idx, (example_data,target) = next(examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check(i, digit1, digit2):\n",
    "    \n",
    "    alpha = torch.clamp(torch.randn(l_sample_list[digit1]['std'].shape).cuda(), min=-1, max=1)\n",
    "    test_sample = l_sample_list[digit1]['mean'] + F.normalize(alpha)*l_sample_list[digit1]['std']\n",
    "    \n",
    "    alpha = torch.clamp(torch.randn(l_sample_list[digit2]['std'].shape).cuda(), min=-1, max=1)\n",
    "    test_sample += l_sample_list[digit2]['mean'] + F.normalize(alpha)*l_sample_list[digit2]['std']\n",
    "    \n",
    "    example_img = example_data[i,:,:,:].unsqueeze(0).cuda()\n",
    "    output, l_dist = model(example_img)\n",
    "    l_sample = model.reparameterize(l_dist)\n",
    "    test_sample = l_sample + 0.4*test_sample\n",
    "    test = model.decode(test_sample)\n",
    "    preds = torch.argmax(classifier(F.upsample(test, (28,28), mode='bilinear', align_corners=True)))\n",
    "    print(preds)\n",
    "    plt.imshow(model.decode(test_sample)[0,0].cpu().detach().numpy(), cmap='gray', interpolation='none')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1, device='cuda:0')\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "check(2,1,7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def plot_64(data):\n",
    "#     l_sample = None\n",
    "#     for i in range(data.shape[0]):\n",
    "#         output, l_dist_x = model(data[i,:,:,:].unsqueeze(0).cuda())\n",
    "#         l_sample_x = model.reparameterize(l_dist_x)\n",
    "#         if l_sample is None:\n",
    "#             l_sample = l_sample_x\n",
    "#         else:\n",
    "#             l_sample += l_sample_x\n",
    "#     l_sample = l_sample / data.shape[0]\n",
    "#     new_output = model.decode(l_sample)\n",
    "#     plt.imshow(new_output[0][0].detach().cpu().numpy(), cmap='gray', interpolation='none')\n",
    "#     pred = classifier(F.upsample(new_output, (28,28), mode='bilinear', align_corners=True))\n",
    "#     print(pred)\n",
    "#     print(torch.argmax(pred))\n",
    "#     return l_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_batch_avg(data):\n",
    "#     l_sample = None\n",
    "#     for i in range(data.shape[0]):\n",
    "#         output, l_dist_x = model(data[i,:,:,:].unsqueeze(0).cuda())\n",
    "#         l_sample_x = model.reparameterize(l_dist_x)\n",
    "#         if l_sample is None:\n",
    "#             l_sample = l_sample_x\n",
    "#         else:\n",
    "#             l_sample += l_sample_x\n",
    "#     l_sample = l_sample / data.shape[0]\n",
    "# #     new_output = model.decode(l_sample)\n",
    "# #     plt.imshow(new_output[0][0].detach().cpu().numpy(), cmap='gray', interpolation='none')\n",
    "# #     pred = classifier(F.upsample(new_output, (28,28), mode='bilinear', align_corners=True))\n",
    "# #     print(pred)\n",
    "# #     print(torch.argmax(pred))\n",
    "#     return l_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def plot_avg_mnist(img1, data):\n",
    "# #     new_l_sample = None\n",
    "# #     count = len(list_to_process)\n",
    "#     output, l_dist_x = model(img1.cuda())\n",
    "#     l_sample_x = model.reparameterize(l_dist_x)\n",
    "#     l_sample_y = get_batch_avg(data)\n",
    "# #     output, l_dist_y = model(img2.cuda())\n",
    "# #     l_sample_y = model.reparameterize(l_dist_y)\n",
    "    \n",
    "#     l_sample = 1*l_sample_x + 0.4*l_sample_y\n",
    "    \n",
    "#     new_output = model.decode(l_sample)\n",
    "# #     for i in list_to_process:\n",
    "# #         example_img = example_data[i,:,:,:].unsqueeze(0).cuda()\n",
    "# #         output, l_dist = model(example_img)\n",
    "# #         l_sample = model.reparameterize(l_dist)\n",
    "# #         if new_l_sample is None:\n",
    "# #             new_l_sample = l_sample\n",
    "# #         else:\n",
    "# #             new_l_sample += l_sample\n",
    "# #     new_l_sample = new_l_sample / count\n",
    "# #     new_output = model.decode(new_l_sample)\n",
    "#     plt.imshow(new_output[0][0].detach().cpu().numpy(), cmap='gray', interpolation='none')\n",
    "#     pred = classifier(F.upsample(new_output, (28,28), mode='bilinear', align_corners=True))\n",
    "#     print(pred)\n",
    "#     print(torch.argmax(pred))\n",
    "#     print(pred[0,2].item(), pred[0,5].item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def plot_mnist(list_to_process):\n",
    "#     new_l_sample = None\n",
    "#     count = len(list_to_process)\n",
    "#     for i in list_to_process:\n",
    "#         example_img = example_data[i,:,:,:].unsqueeze(0).cuda()\n",
    "#         output, l_dist = model(example_img)\n",
    "#         l_sample = model.reparameterize(l_dist)\n",
    "#         if new_l_sample is None:\n",
    "#             new_l_sample = l_sample\n",
    "#         else:\n",
    "#             new_l_sample += l_sample\n",
    "#     new_l_sample = new_l_sample / count\n",
    "#     new_output = model.decode(new_l_sample)\n",
    "#     plt.imshow(new_output[0][0].detach().cpu().numpy(), cmap='gray', interpolation='none')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def check(i, j, alpha, beta):\n",
    "#     im1 = example_data[i,:,:,:].unsqueeze(0).cuda()\n",
    "#     im2 = example_data[j,:,:,:].unsqueeze(0).cuda()\n",
    "#     out1, l_dist1 = model(im1)\n",
    "#     out2, l_dist2 = model(im2)\n",
    "#     l_sample1 = model.reparameterize(l_dist1)\n",
    "#     l_sample2 = model.reparameterize(l_dist2)\n",
    "#     l_sample = alpha*l_sample1 + beta*l_sample2\n",
    "#     new_out = model.decode(l_sample)\n",
    "# #     new_out1 = model.decode(l_sample1)\n",
    "# #     new_out2 = model.decode(l_sample2)\n",
    "#     plt.figure(figsize=(10,15))\n",
    "#     plt.subplot(1,3,1)\n",
    "#     plt.imshow(example_data[i][0].detach().cpu().numpy(), cmap='gray', interpolation='none')\n",
    "#     plt.subplot(1,3,2)\n",
    "#     plt.imshow(example_data[j][0].detach().cpu().numpy(), cmap='gray', interpolation='none')\n",
    "#     plt.subplot(1,3,3)\n",
    "#     plt.imshow(new_out[0][0].detach().cpu().numpy(), cmap='gray', interpolation='none')\n",
    "#     print(torch.argmax(classifier(F.upsample(new_out, (28,28), mode='bilinear', align_corners=True))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# alpha=1\n",
    "# beta = 1.2\n",
    "# check(19,11, alpha, beta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Attack(nn.Module):\n",
    "#     def __init__(self, attack_digit=attack_digit, target_digit=target_digit, vae=model, classifier=classifier, avg_latent=l_sample_list):\n",
    "#         super(self, Attack).__init__()\n",
    "#         self.classifier = classifier\n",
    "#         self.classifier.eval()\n",
    "#         self.vae = vae\n",
    "#         self.vae.eval()\n",
    "#         self.avg_latent = avg_latent\n",
    "#         self.attack_digit = attack_digit\n",
    "#         self.target_digit = target_digit\n",
    "#         self.hidden_layers = hidden_layers\n",
    "#         self.hidden_layers.insert(0, latent_dim)\n",
    "#         self.hidden_layers.append(latent_dim)\n",
    "#         self.layers = []\n",
    "        \n",
    "#         for i in range(len(self.hidden_layers)-1):\n",
    "#             self.layers.append(nn.Linear(self.hidden_layers[i], self.hidden_layers[i+1]))\n",
    "        \n",
    "#         self.layers = nn.ModuleList(self.layers)\n",
    "        \n",
    "#     def forward(self, x, y):\n",
    "#         _, l_dist_x = self.vae(x)\n",
    "#         _, l_dist_y = self.vae(y)\n",
    "#         l_sample_x = self.vae.reparameterize(l_dist_x)\n",
    "#         l_sample_y = self.vae.reparameterize(l_dist_y)\n",
    "#         noised_sample = l_sample\n",
    "#         for layer in self.layers:\n",
    "#             noised_sample = layer(noised_sample)\n",
    "#         noised_images = self.vae.decoder(noised_sample)\n",
    "#         preds = self.classifier(F.upsample(noised_image, (28,28), mode='bilinear', align_corners=True))\n",
    "#         return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import exp\n",
    "\n",
    "def gaussian(window_size, sigma):\n",
    "    gauss = torch.Tensor([exp(-(x - window_size//2)**2/float(2*sigma**2)) for x in range(window_size)])\n",
    "    return gauss/gauss.sum()\n",
    "\n",
    "def create_window(window_size, channel):\n",
    "    _1D_window = gaussian(window_size, 1.5).unsqueeze(1)\n",
    "    _2D_window = _1D_window.mm(_1D_window.t()).float().unsqueeze(0).unsqueeze(0)\n",
    "    window = torch.Tensor(_2D_window.expand(channel, 1, window_size, window_size).contiguous())\n",
    "    return window\n",
    "\n",
    "def _ssim(img1, img2, window, window_size, channel, size_average = True):\n",
    "    mu1 = F.conv2d(img1, window, padding = window_size//2, groups = channel)\n",
    "    mu2 = F.conv2d(img2, window, padding = window_size//2, groups = channel)\n",
    "\n",
    "    mu1_sq = mu1.pow(2)\n",
    "    mu2_sq = mu2.pow(2)\n",
    "    mu1_mu2 = mu1*mu2\n",
    "\n",
    "    sigma1_sq = F.conv2d(img1*img1, window, padding = window_size//2, groups = channel) - mu1_sq\n",
    "    sigma2_sq = F.conv2d(img2*img2, window, padding = window_size//2, groups = channel) - mu2_sq\n",
    "    sigma12 = F.conv2d(img1*img2, window, padding = window_size//2, groups = channel) - mu1_mu2\n",
    "\n",
    "    C1 = 0.01**2\n",
    "    C2 = 0.03**2\n",
    "\n",
    "    ssim_map = ((2*mu1_mu2 + C1)*(2*sigma12 + C2))/((mu1_sq + mu2_sq + C1)*(sigma1_sq + sigma2_sq + C2))\n",
    "\n",
    "    if size_average:\n",
    "        return ssim_map.mean()\n",
    "    else:\n",
    "        return ssim_map.mean(1).mean(1).mean(1)\n",
    "\n",
    "class SSIM(nn.Module):\n",
    "    def __init__(self, window_size = 11, size_average = True):\n",
    "        super(SSIM, self).__init__()\n",
    "        self.window_size = window_size\n",
    "        self.size_average = size_average\n",
    "        self.channel = 1\n",
    "        self.window = create_window(window_size, self.channel)\n",
    "\n",
    "    def forward(self, img1, img2):\n",
    "        (_, channel, _, _) = img1.size()\n",
    "\n",
    "        if channel == self.channel and self.window.data.type() == img1.data.type():\n",
    "            window = self.window\n",
    "        else:\n",
    "            window = create_window(self.window_size, channel)\n",
    "            \n",
    "            if img1.is_cuda:\n",
    "                window = window.cuda(img1.get_device())\n",
    "            window = window.type_as(img1)\n",
    "            \n",
    "            self.window = window\n",
    "            self.channel = channel\n",
    "\n",
    "\n",
    "        return _ssim(img1, img2, window, self.window_size, channel, self.size_average)\n",
    "\n",
    "def ssim(img1, img2, window_size = 11, size_average = True):\n",
    "    (_, channel, _, _) = img1.size()\n",
    "    window = create_window(window_size, channel)\n",
    "    \n",
    "    if img1.is_cuda:\n",
    "        window = window.cuda(img1.get_device())\n",
    "    window = window.type_as(img1)\n",
    "    \n",
    "    return _ssim(img1, img2, window, window_size, channel, size_average)\n",
    "\n",
    "ssim_loss = SSIM(window_size = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cuda = True\n",
    "device = torch.device('cuda' if use_cuda else 'cpu')\n",
    "\n",
    "# Constrained Translator\n",
    "class Noise(nn.Module):\n",
    "    def __init__(self, hidden_layers=[50, 100, latent_dim], latent_dim=latent_dim):\n",
    "        super(Noise, self).__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "        self.hidden_layers = hidden_layers\n",
    "        self.hidden_layers.insert(0, latent_dim)\n",
    "        self.layers = []\n",
    "        \n",
    "        for i in range(len(self.hidden_layers)-1):\n",
    "            self.layers.append(nn.Linear(self.hidden_layers[i], self.hidden_layers[i+1]))\n",
    "        \n",
    "        self.layers = nn.ModuleList(self.layers)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constrained Cofficients\n",
    "class Cofficients(nn.Module):\n",
    "    def __init__(self, hidden_layers=[40, 20, 10, 1], latent_dim=latent_dim):\n",
    "        super(Cofficients, self).__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "        self.hidden_layers = hidden_layers\n",
    "        self.hidden_layers.insert(0, latent_dim)\n",
    "        self.layers = []\n",
    "        \n",
    "        for i in range(len(self.hidden_layers)-1):\n",
    "            self.layers.append(nn.Linear(self.hidden_layers[i], self.hidden_layers[i+1]))\n",
    "        \n",
    "        self.layers = nn.ModuleList(self.layers)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# use_cuda = True\n",
    "# device = torch.device('cuda' if use_cuda else 'cpu')\n",
    "\n",
    "\n",
    "# def create_logits(preds, device=device):\n",
    "#     preds = preds.cpu()\n",
    "#     logits = preds.cpu()\n",
    "    \n",
    "#     sorted_ = torch.argsort(preds, dim=1, descending=True)\n",
    "#     first = sorted_[:,0]\n",
    "#     second = sorted_[:,1]\n",
    "#     print(first)\n",
    "#     print(second)\n",
    "    \n",
    "#     p_first = preds.gather(1, first.view(-1,1))\n",
    "#     p_second = preds.gather(1, second.view(-1,1))\n",
    "#     means = torch.mean(torch.stack([p_first, p_second]), dim=0).squeeze(1)\n",
    "#     print(means)\n",
    "#     diff = 0.1*(first - second)\n",
    "#     print(diff)\n",
    "#     print((means-diff).shape)\n",
    "#     j = torch.arange(logits.size(0)).long()\n",
    "#     logits[j, first] = torch.FloatTensor(means - diff)\n",
    "#     logits[j, second] = torch.FloatTensor(means + diff)\n",
    "# #     print(logits[0])\n",
    "#     return logits.to(device), first, second\n",
    "\n",
    "# a = torch.randn((10,4)).cuda()\n",
    "# b = create_logits(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from skimage.metrics import structural_similarity as ssim\n",
    "\n",
    "use_cuda = True\n",
    "device = torch.device('cuda' if use_cuda else 'cpu')\n",
    "\n",
    "\n",
    "def create_logits(preds, device=device):\n",
    "    preds = preds.cpu()\n",
    "    logits = preds\n",
    "    \n",
    "    sorted_ = torch.argsort(preds, dim=1, descending=True)\n",
    "    first = sorted_[:,0]\n",
    "    second = sorted_[:,1]\n",
    "    \n",
    "    p_first = preds.gather(1, first.view(-1,1))\n",
    "    p_second = preds.gather(1, second.view(-1,1))\n",
    "    \n",
    "    means = torch.mean(torch.stack([p_first, p_second]), dim=0).squeeze(1)\n",
    "    diff = 0.1*(first - second)\n",
    "    \n",
    "    j = torch.arange(logits.size(0)).long()\n",
    "    \n",
    "    logits[j, first] = torch.FloatTensor(means - diff)\n",
    "    logits[j, second] = torch.FloatTensor(means + diff)\n",
    "    \n",
    "    return logits.to(device), first, second\n",
    "\n",
    "def structural(org_image, noised_image):\n",
    "    batch_size, channels, width, height = org_image.shape\n",
    "    loss1 = 0\n",
    "    for b_ in range(batch_size):\n",
    "        ch_loss = 0\n",
    "        for ch_ in range(channels):\n",
    "            ch_loss += 1-ssim(org_image[b_][ch_].detach().cpu().numpy(), noised_image[b_][ch_].detach().cpu().numpy())\n",
    "        loss1 += ch_loss/channels\n",
    "    return loss1\n",
    "            \n",
    "class T_Loss(nn.Module):\n",
    "    def __init__(self, alt_target, decoder=model.decode, classifier=classifier,\n",
    "                 latent_dim=latent_dim, l_samples=l_sample_list,\n",
    "                 classes=len(train_data.classes)):\n",
    "        super(T_Loss, self).__init__()\n",
    "        self.decoder = decoder\n",
    "        self.classifier = classifier\n",
    "#         self.means = means\n",
    "#         self.stds = stds\n",
    "        self.l_samples = l_samples\n",
    "        self.classes = classes\n",
    "        self.latent_dim = latent_dim\n",
    "        self.alt_target = alt_target\n",
    "        \n",
    "        \n",
    "    def forward(self, coffs, noises, org_x, targets):\n",
    "        \n",
    "        org_image = self.decoder(org_x)\n",
    "        \n",
    "        preds = self.classifier(F.upsample(org_image, (28,28), mode='bilinear', align_corners=True))\n",
    "        \n",
    "        alt_target, first, second = create_logits(preds)\n",
    "        \n",
    "        means = []\n",
    "        stds = []\n",
    "        \n",
    "        for key in second:\n",
    "            means.append(self.l_samples[key.item()]['mean'])\n",
    "            stds.append(self.l_samples[key.item()]['std'])\n",
    "        \n",
    "        means = torch.stack(means)\n",
    "        stds = torch.stack(stds)\n",
    "        noise_latent = means.squeeze(1) + torch.clamp(noises, min=-3, max=3) * stds.squeeze(1)\n",
    "        noise_latent = F.normalize(coffs) * noise_latent\n",
    "        \n",
    "        noised_sample = org_x + 0.6*noise_latent\n",
    "        \n",
    "        noised_image = self.decoder(noised_sample)\n",
    "        \n",
    "        preds = self.classifier(F.upsample(noised_image, (28,28), mode='bilinear', align_corners=True))\n",
    "        \n",
    "#         loss1 = ssim_loss(org_image, noised_image)\n",
    "        \n",
    "        loss2 = nn.BCELoss(reduction='sum')(preds, alt_target.float())\n",
    "        \n",
    "        loss3 = torch.norm(org_image-noised_image, p=2)  \n",
    "\n",
    "#         loss = 400*(1-loss1) + 0.5*loss2 + 20*loss3\n",
    "        loss = 0.5*loss2 + 20*loss3\n",
    "#         loss = loss2 + 20*loss3\n",
    "#         loss.requires_grad = True\n",
    "        \n",
    "        out_labels = preds.argmax(dim=1, keepdim=True)\n",
    "#         print(out_labels)\n",
    "#         print(torch.empty(out_labels.shape).fill_(target_label))\n",
    "#         print(preds)\n",
    "#         correct = out_labels.eq(torch.Tensor([target_label]*out_labels.shape[0]).to(device)).sum()\n",
    "#         print(out_labels.shape)\n",
    "#         print((out_labels.squeeze(1)==targets.cuda()).sum())\n",
    "        correct = (out_labels.squeeze(1)==targets.cuda()).sum()\n",
    "#         print(out_labels.shape)\n",
    "#         print(correct)\n",
    "#         print(torch.Tensor([target_label]*out_labels.shape[0]))\n",
    "#         print(out_labels)\n",
    "        return loss, correct.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "T_Loss(\n",
       "  (classifier): Classifier(\n",
       "    (conv1): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (dropout1): Dropout2d(p=0.25, inplace=False)\n",
       "    (dropout2): Dropout2d(p=0.5, inplace=False)\n",
       "    (fc1): Linear(in_features=9216, out_features=128, bias=True)\n",
       "    (fc2): Linear(in_features=128, out_features=10, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alt_target = 5\n",
    "noise = Noise().to(device)\n",
    "cofficient = Cofficients().to(device)\n",
    "tloss = T_Loss(alt_target=5).to(device)\n",
    "tloss.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in noise.parameters():\n",
    "    param.requires_grad=True\n",
    "    \n",
    "for param in cofficient.parameters():\n",
    "    param.requires_grad=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count_digit = 0\n",
    "# for i in range(len(train_data)):\n",
    "#     if train_data[i][1]==5:\n",
    "#         count_digit += 1\n",
    "# print(count_digit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da3b13e9482e412aa2956745f8868692",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=150.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1\tLoss: 647.762249\tCorrect: 43501\n",
      "Train Epoch: 2\tLoss: 556.010368\tCorrect: 48396\n",
      "Train Epoch: 3\tLoss: 542.347683\tCorrect: 48833\n",
      "Train Epoch: 4\tLoss: 532.720365\tCorrect: 48768\n",
      "Train Epoch: 5\tLoss: 525.081668\tCorrect: 48585\n",
      "Train Epoch: 6\tLoss: 524.732426\tCorrect: 48222\n",
      "Train Epoch: 7\tLoss: 523.251293\tCorrect: 48170\n",
      "Train Epoch: 8\tLoss: 516.330376\tCorrect: 48623\n",
      "Train Epoch: 9\tLoss: 515.736216\tCorrect: 48497\n",
      "Train Epoch: 10\tLoss: 515.257098\tCorrect: 48313\n",
      "Train Epoch: 11\tLoss: 514.352359\tCorrect: 48591\n",
      "Train Epoch: 12\tLoss: 523.336765\tCorrect: 48001\n",
      "Train Epoch: 13\tLoss: 514.376433\tCorrect: 48328\n",
      "Train Epoch: 14\tLoss: 509.549922\tCorrect: 48643\n",
      "Train Epoch: 15\tLoss: 507.864562\tCorrect: 48681\n",
      "Train Epoch: 16\tLoss: 505.845902\tCorrect: 48724\n",
      "Train Epoch: 17\tLoss: 509.712556\tCorrect: 48434\n",
      "Train Epoch: 18\tLoss: 507.220670\tCorrect: 48710\n",
      "Train Epoch: 19\tLoss: 504.930542\tCorrect: 48743\n",
      "Train Epoch: 20\tLoss: 502.936244\tCorrect: 48717\n",
      "Train Epoch: 21\tLoss: 503.463399\tCorrect: 48783\n",
      "Train Epoch: 22\tLoss: 506.877240\tCorrect: 48502\n",
      "Train Epoch: 23\tLoss: 500.761214\tCorrect: 48865\n",
      "Train Epoch: 24\tLoss: 499.690953\tCorrect: 48740\n",
      "Train Epoch: 25\tLoss: 507.315242\tCorrect: 48373\n",
      "Train Epoch: 26\tLoss: 503.118296\tCorrect: 48514\n",
      "Train Epoch: 27\tLoss: 499.210952\tCorrect: 48795\n",
      "Train Epoch: 28\tLoss: 497.266452\tCorrect: 48718\n",
      "Train Epoch: 29\tLoss: 498.309427\tCorrect: 48645\n",
      "Train Epoch: 30\tLoss: 504.617851\tCorrect: 48495\n",
      "Train Epoch: 31\tLoss: 505.964263\tCorrect: 48263\n",
      "Train Epoch: 32\tLoss: 500.664295\tCorrect: 48452\n",
      "Train Epoch: 33\tLoss: 497.062141\tCorrect: 48616\n",
      "Train Epoch: 34\tLoss: 499.558675\tCorrect: 48516\n",
      "Train Epoch: 35\tLoss: 494.918224\tCorrect: 48832\n",
      "Train Epoch: 36\tLoss: 502.782494\tCorrect: 48308\n",
      "Train Epoch: 37\tLoss: 500.382593\tCorrect: 48531\n",
      "Train Epoch: 38\tLoss: 497.568457\tCorrect: 48706\n",
      "Train Epoch: 39\tLoss: 497.992967\tCorrect: 48853\n",
      "Train Epoch: 40\tLoss: 493.342164\tCorrect: 48936\n",
      "Train Epoch: 41\tLoss: 496.107744\tCorrect: 48680\n",
      "Train Epoch: 42\tLoss: 504.576104\tCorrect: 48091\n",
      "Train Epoch: 43\tLoss: 499.264549\tCorrect: 48481\n",
      "Train Epoch: 44\tLoss: 496.683690\tCorrect: 48571\n",
      "Train Epoch: 45\tLoss: 497.402483\tCorrect: 48628\n",
      "Train Epoch: 46\tLoss: 492.286312\tCorrect: 48864\n",
      "Train Epoch: 47\tLoss: 490.468220\tCorrect: 48885\n",
      "Train Epoch: 48\tLoss: 497.219415\tCorrect: 48644\n",
      "Train Epoch: 49\tLoss: 491.438389\tCorrect: 49022\n",
      "Train Epoch: 50\tLoss: 499.212488\tCorrect: 48492\n",
      "Train Epoch: 51\tLoss: 493.072612\tCorrect: 48781\n",
      "Train Epoch: 52\tLoss: 494.629162\tCorrect: 48642\n",
      "Train Epoch: 53\tLoss: 490.890916\tCorrect: 48977\n",
      "Train Epoch: 54\tLoss: 488.949940\tCorrect: 48922\n",
      "Train Epoch: 55\tLoss: 493.808114\tCorrect: 48792\n",
      "Train Epoch: 56\tLoss: 493.118507\tCorrect: 48812\n",
      "Train Epoch: 57\tLoss: 490.000122\tCorrect: 49110\n",
      "Train Epoch: 58\tLoss: 492.172550\tCorrect: 48758\n",
      "Train Epoch: 59\tLoss: 492.424653\tCorrect: 48744\n",
      "Train Epoch: 60\tLoss: 491.732169\tCorrect: 48777\n",
      "Train Epoch: 61\tLoss: 488.493048\tCorrect: 48971\n",
      "Train Epoch: 62\tLoss: 488.750441\tCorrect: 48898\n",
      "Train Epoch: 63\tLoss: 492.733163\tCorrect: 48871\n",
      "Train Epoch: 64\tLoss: 490.631474\tCorrect: 48932\n",
      "Train Epoch: 65\tLoss: 488.446036\tCorrect: 49092\n",
      "Train Epoch: 66\tLoss: 486.595735\tCorrect: 49094\n",
      "Train Epoch: 67\tLoss: 490.979491\tCorrect: 48811\n",
      "Train Epoch: 68\tLoss: 499.061845\tCorrect: 48477\n",
      "Train Epoch: 69\tLoss: 492.792456\tCorrect: 48643\n",
      "Train Epoch: 70\tLoss: 490.218996\tCorrect: 48805\n",
      "Train Epoch: 71\tLoss: 487.990222\tCorrect: 48990\n",
      "Train Epoch: 72\tLoss: 488.314495\tCorrect: 49032\n",
      "Train Epoch: 73\tLoss: 493.286339\tCorrect: 48621\n",
      "Train Epoch: 74\tLoss: 490.798707\tCorrect: 48729\n",
      "Train Epoch: 75\tLoss: 490.577382\tCorrect: 48770\n",
      "Train Epoch: 76\tLoss: 487.700985\tCorrect: 49144\n",
      "Train Epoch: 77\tLoss: 489.939342\tCorrect: 48696\n",
      "Train Epoch: 78\tLoss: 491.367733\tCorrect: 48818\n",
      "Train Epoch: 79\tLoss: 487.835784\tCorrect: 49017\n",
      "Train Epoch: 80\tLoss: 488.825827\tCorrect: 49060\n",
      "Train Epoch: 81\tLoss: 485.355146\tCorrect: 49232\n",
      "Train Epoch: 82\tLoss: 488.141168\tCorrect: 49025\n",
      "Train Epoch: 83\tLoss: 487.053177\tCorrect: 49230\n",
      "Train Epoch: 84\tLoss: 487.576003\tCorrect: 49304\n",
      "Train Epoch: 85\tLoss: 484.897856\tCorrect: 49492\n",
      "Train Epoch: 86\tLoss: 492.340313\tCorrect: 48816\n",
      "Train Epoch: 87\tLoss: 488.027554\tCorrect: 49128\n",
      "Train Epoch: 88\tLoss: 489.119012\tCorrect: 49193\n",
      "Train Epoch: 89\tLoss: 485.625790\tCorrect: 49495\n",
      "Train Epoch: 90\tLoss: 484.180779\tCorrect: 49512\n",
      "Train Epoch: 91\tLoss: 491.300393\tCorrect: 48840\n",
      "Train Epoch: 92\tLoss: 488.541240\tCorrect: 49105\n",
      "Train Epoch: 93\tLoss: 486.304280\tCorrect: 49494\n",
      "Train Epoch: 94\tLoss: 484.576086\tCorrect: 49481\n"
     ]
    }
   ],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "attack_log_interval = 1\n",
    "\n",
    "noise.train()\n",
    "cofficient.train()\n",
    "\n",
    "optimizer1 = optim.Adam(noise.parameters(), lr=1e-4)\n",
    "# optimizer1 = torch.optim.SGD(noise.parameters(), lr=1e-4, momentum=0.9)\n",
    "\n",
    "# optimizer2 = torch.optim.SGD(cofficient.parameters(), lr=1e-4, momentum=0.9)\n",
    "optimizer2 = optim.Adam(cofficient.parameters(), lr=1e-4)\n",
    "\n",
    "\n",
    "# scheduler1 = torch.optim.lr_scheduler.CyclicLR(optimizer1, base_lr=1e-7, max_lr=0.1)\n",
    "# scheduler2 = torch.optim.lr_scheduler.CyclicLR(optimizer2, base_lr=1e-7, max_lr=0.1)\n",
    "\n",
    "for epoch in tqdm(range(150)):\n",
    "    epoch_loss = 0\n",
    "    epoch_correct = 0\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        \n",
    "        data = torch.FloatTensor(data).to(device)\n",
    "        _, l_dist = model(data)\n",
    "        l_sample = model.reparameterize(l_dist)\n",
    "        \n",
    "        n = noise(l_sample)\n",
    "        c = cofficient(l_sample)\n",
    "\n",
    "        optimizer1.zero_grad()\n",
    "        optimizer2.zero_grad()\n",
    "        \n",
    "        loss, correct = tloss(c, n, l_sample, target)\n",
    "\n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer1.step()\n",
    "        optimizer2.step()\n",
    "\n",
    "#         scheduler1.step()\n",
    "#         scheduler2.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "    \n",
    "        epoch_correct += correct\n",
    "        \n",
    "    if (epoch+1) % attack_log_interval == 0:\n",
    "        print('Train Epoch: {}\\tLoss: {:.6f}\\tCorrect: {}'.format(\n",
    "            epoch+1, epoch_loss/batch_idx, epoch_correct))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(noise, 'models/{}/noise.pt'.format(FOLDER))\n",
    "torch.save(cofficient, 'models/{}/coff.pt'.format(FOLDER))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise.eval()\n",
    "cofficient.eval()\n",
    "total_correct = 0\n",
    "total_test = 0\n",
    "for batch_idx, (data, target) in enumerate(test_loader):\n",
    "    total_test += data.shape[0]\n",
    "    data = torch.FloatTensor(data).to(device)\n",
    "\n",
    "    _, l_dist = model(data)\n",
    "    l_sample = model.reparameterize(l_dist)\n",
    "    \n",
    "    noise_ = noise(l_sample)\n",
    "    coff_ = cofficient(l_sample)\n",
    "    \n",
    "    loss, correct = tloss(coff_, noise_, l_sample, alt_target)\n",
    "    total_correct += correct\n",
    "#     print(correct)\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "#         print(correct)\n",
    "#         epoch_loss += loss.item()\n",
    "    \n",
    "\n",
    "#     if (epoch+1) % attack_log_interval == 0:\n",
    "#         print('Train Epoch: \\tCorrect: {}'.format(\n",
    "#             epoch, epoch_correct))\n",
    "print(total_correct)\n",
    "print(\"Accuracy: \", 100*(total_correct/total_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(i, means=l_mean_tensor, stds=l_std_tensor, latent_dim=latent_dim, classes=len(train_data.classes)):\n",
    "    _, l_dist = model(example_data[i].unsqueeze_(0).to(device))\n",
    "    l_sample = model.reparameterize(l_dist)\n",
    "    noises = noise(l_sample)\n",
    "    coffs = cofficient(l_sample)\n",
    "#     print(coffs.shape)\n",
    "#     print(noises.shape)\n",
    "    noised_latent = means + noises.reshape(noises.shape[0], classes, latent_dim)*stds\n",
    "    noised_latent = coffs[:,:,None]*noised_latent\n",
    "#     print(noised_latent.shape)\n",
    "#     print(l_sample.shape)\n",
    "#     print(torch.transpose(coff[:,None].cuda()*avg_latent.T, 1, 2).sum(1).shape)\n",
    "#     noised_latent = l_sample + 2e-1*torch.transpose(coff[:,None].cuda()*avg_latent.T, 1, 2).sum(1)\n",
    "#     print(noised_latent.shape)\n",
    "#     print(noised_sample)\n",
    "#     print(l_sample)\n",
    "#     noised_sample = 1 * ((l_sample - l_sample.min())/(l_sample.max() - l_sample.min())) + 1e-2 * ((noised_sample - noised_sample.min())/(noised_sample.max() - noised_sample.min()))\n",
    "#     noised_sample = 1 * l_sample + 2e-2 * noised_sample\n",
    "#     noised_sample = l_sample + 1e-7 * noised_sample\n",
    "    final = model.decode(l_sample+1e-12*noised_latent.sum(dim=1))\n",
    "#     print(final.shape)\n",
    "    pred_org = torch.argmax(classifier(F.upsample(example_data[i,:,:,:].unsqueeze(0).cuda(), (28,28), mode='bilinear', align_corners=True)))\n",
    "    pred = torch.argmax(classifier(F.upsample(final, (28,28), mode='bilinear', align_corners=True)), dim=1)\n",
    "#     print(pred)\n",
    "    plt.figure(figsize=(10,10))\n",
    "    plt.subplot(1,2,1)\n",
    "    plt.imshow(example_data[i][0].detach().cpu().numpy(), cmap='gray', interpolation='none')\n",
    "    plt.subplot(1,2,2)\n",
    "    plt.imshow(final[0][0].detach().cpu().numpy(), cmap='gray', interpolation='none')\n",
    "    plt.title(\"Prediction: {}, {}\".format(pred_org.item(), pred.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(64):\n",
    "    test(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.transpose(torch.clamp(coff[:,None].cuda(), min=-0.8, max=0.3)*self.avg_latent.T, 1, 2).sum(1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
