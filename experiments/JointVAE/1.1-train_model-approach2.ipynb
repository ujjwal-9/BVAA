{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train a JointVAE model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get MNIST data\n",
    "Build a simple JointVAE model on MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataloaders import get_mnist_dataloaders\n",
    "train_loader, test_loader = get_mnist_dataloaders(batch_size=64, path_to_data='/home/data/bvaa')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define latent distribution of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Latent distribution will be joint distribution of 10 gaussian normal distributions\n",
    "# and one 10 dimensional Gumbel Softmax distribution\n",
    "latent_spec = {'cont': 10,\n",
    "               'disc': [10]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import VAE\n",
    "\n",
    "model = VAE(latent_spec=latent_spec, img_size=(1, 32, 32), use_cuda=True).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VAE(\n",
      "  (img_to_features): Sequential(\n",
      "    (0): Conv2d(1, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "    (1): ReLU()\n",
      "    (2): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "    (3): ReLU()\n",
      "    (4): Conv2d(64, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "    (5): ReLU()\n",
      "  )\n",
      "  (features_to_hidden): Sequential(\n",
      "    (0): Linear(in_features=1024, out_features=256, bias=True)\n",
      "    (1): ReLU()\n",
      "  )\n",
      "  (fc_mean): Linear(in_features=256, out_features=10, bias=True)\n",
      "  (fc_log_var): Linear(in_features=256, out_features=10, bias=True)\n",
      "  (fc_alphas): ModuleList(\n",
      "    (0): Linear(in_features=256, out_features=10, bias=True)\n",
      "  )\n",
      "  (latent_to_features): Sequential(\n",
      "    (0): Linear(in_features=20, out_features=256, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=256, out_features=1024, bias=True)\n",
      "    (3): ReLU()\n",
      "  )\n",
      "  (features_to_img): Sequential(\n",
      "    (0): ConvTranspose2d(64, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "    (1): ReLU()\n",
      "    (2): ConvTranspose2d(32, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "    (3): ReLU()\n",
      "    (4): ConvTranspose2d(32, 1, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "    (5): Sigmoid()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import optim\n",
    "\n",
    "# Build optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=5e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from training import Trainer\n",
    "\n",
    "# Define the capacities\n",
    "# Continuous channels\n",
    "cont_capacity = [0.0, 5.0, 25000, 30.0]  # Starting at a capacity of 0.0, increase this to 5.0\n",
    "                                         # over 25000 iterations with a gamma of 30.0\n",
    "# Discrete channels\n",
    "disc_capacity = [0.0, 5.0, 25000, 30.0]  # Starting at a capacity of 0.0, increase this to 5.0\n",
    "                                         # over 25000 iterations with a gamma of 30.0\n",
    "\n",
    "# Build a trainer\n",
    "trainer = Trainer(model, optimizer,\n",
    "                  cont_capacity=cont_capacity,\n",
    "                  disc_capacity=disc_capacity, use_cuda=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a visualizer which will be passed to trainer to visualize progress during training\n",
    "from visualize import Visualizer\n",
    "\n",
    "viz = Visualizer(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Classifier, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, 3, 1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, 1)\n",
    "        self.dropout1 = nn.Dropout2d(0.25)\n",
    "        self.dropout2 = nn.Dropout2d(0.5)\n",
    "        self.fc1 = nn.Linear(9216, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = self.dropout1(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.fc2(x)\n",
    "        output = F.softmax(x, dim=1)\n",
    "        return output\n",
    "\n",
    "classifier = Classifier().cuda()\n",
    "classifier.load_state_dict(torch.load('../models/mnist_cnn_non_log.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e3a2b58b7f3446da60bf3b9631e020e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/60000\tLoss: 692.196\n",
      "3200/60000\tLoss: 476.622\n",
      "6400/60000\tLoss: 294.531\n",
      "9600/60000\tLoss: 277.153\n",
      "12800/60000\tLoss: 274.091\n",
      "16000/60000\tLoss: 271.774\n",
      "19200/60000\tLoss: 271.365\n",
      "22400/60000\tLoss: 272.160\n",
      "25600/60000\tLoss: 269.560\n",
      "28800/60000\tLoss: 268.279\n",
      "32000/60000\tLoss: 269.986\n",
      "35200/60000\tLoss: 270.278\n",
      "38400/60000\tLoss: 268.315\n",
      "41600/60000\tLoss: 267.491\n",
      "44800/60000\tLoss: 265.501\n",
      "48000/60000\tLoss: 263.714\n",
      "51200/60000\tLoss: 263.661\n",
      "54400/60000\tLoss: 266.223\n",
      "57600/60000\tLoss: 259.941\n",
      "Epoch: 1 Average loss: 281.51\n",
      "0/60000\tLoss: 253.746\n",
      "3200/60000\tLoss: 260.812\n",
      "6400/60000\tLoss: 259.698\n",
      "9600/60000\tLoss: 258.423\n",
      "12800/60000\tLoss: 256.708\n",
      "16000/60000\tLoss: 258.778\n",
      "19200/60000\tLoss: 254.996\n",
      "22400/60000\tLoss: 254.506\n",
      "25600/60000\tLoss: 255.170\n",
      "28800/60000\tLoss: 254.019\n",
      "32000/60000\tLoss: 252.399\n",
      "35200/60000\tLoss: 252.563\n",
      "38400/60000\tLoss: 253.227\n",
      "41600/60000\tLoss: 250.613\n",
      "44800/60000\tLoss: 250.778\n",
      "48000/60000\tLoss: 252.472\n",
      "51200/60000\tLoss: 251.961\n",
      "54400/60000\tLoss: 250.416\n",
      "57600/60000\tLoss: 247.928\n",
      "Epoch: 2 Average loss: 254.11\n",
      "0/60000\tLoss: 260.455\n",
      "3200/60000\tLoss: 249.049\n",
      "6400/60000\tLoss: 250.026\n",
      "9600/60000\tLoss: 247.754\n",
      "12800/60000\tLoss: 247.699\n",
      "16000/60000\tLoss: 246.343\n",
      "19200/60000\tLoss: 247.196\n",
      "22400/60000\tLoss: 244.382\n",
      "25600/60000\tLoss: 245.944\n",
      "28800/60000\tLoss: 243.162\n",
      "32000/60000\tLoss: 242.808\n",
      "35200/60000\tLoss: 242.012\n",
      "38400/60000\tLoss: 241.175\n",
      "41600/60000\tLoss: 243.196\n",
      "44800/60000\tLoss: 241.120\n",
      "48000/60000\tLoss: 242.088\n",
      "51200/60000\tLoss: 241.332\n",
      "54400/60000\tLoss: 240.040\n",
      "57600/60000\tLoss: 241.706\n",
      "Epoch: 3 Average loss: 244.23\n",
      "0/60000\tLoss: 248.052\n",
      "3200/60000\tLoss: 239.225\n",
      "6400/60000\tLoss: 240.784\n",
      "9600/60000\tLoss: 238.978\n",
      "12800/60000\tLoss: 239.345\n",
      "16000/60000\tLoss: 237.421\n",
      "19200/60000\tLoss: 236.131\n",
      "22400/60000\tLoss: 237.563\n",
      "25600/60000\tLoss: 237.696\n",
      "28800/60000\tLoss: 236.912\n",
      "32000/60000\tLoss: 237.409\n",
      "35200/60000\tLoss: 236.371\n",
      "38400/60000\tLoss: 233.045\n",
      "41600/60000\tLoss: 235.753\n",
      "44800/60000\tLoss: 235.929\n",
      "48000/60000\tLoss: 233.557\n",
      "51200/60000\tLoss: 233.509\n",
      "54400/60000\tLoss: 234.470\n",
      "57600/60000\tLoss: 231.230\n",
      "Epoch: 4 Average loss: 236.37\n",
      "0/60000\tLoss: 258.315\n",
      "3200/60000\tLoss: 231.426\n",
      "6400/60000\tLoss: 232.302\n",
      "9600/60000\tLoss: 230.478\n",
      "12800/60000\tLoss: 233.104\n",
      "16000/60000\tLoss: 229.291\n",
      "19200/60000\tLoss: 230.655\n",
      "22400/60000\tLoss: 229.800\n",
      "25600/60000\tLoss: 226.646\n",
      "28800/60000\tLoss: 227.701\n",
      "32000/60000\tLoss: 225.379\n",
      "35200/60000\tLoss: 226.134\n",
      "38400/60000\tLoss: 224.855\n",
      "41600/60000\tLoss: 224.724\n",
      "44800/60000\tLoss: 224.218\n",
      "48000/60000\tLoss: 224.865\n",
      "51200/60000\tLoss: 223.710\n",
      "54400/60000\tLoss: 222.640\n",
      "57600/60000\tLoss: 222.819\n",
      "Epoch: 5 Average loss: 227.29\n",
      "0/60000\tLoss: 234.867\n",
      "3200/60000\tLoss: 222.908\n",
      "6400/60000\tLoss: 222.928\n",
      "9600/60000\tLoss: 222.553\n",
      "12800/60000\tLoss: 220.990\n",
      "16000/60000\tLoss: 222.302\n",
      "19200/60000\tLoss: 219.515\n",
      "22400/60000\tLoss: 218.627\n",
      "25600/60000\tLoss: 220.364\n",
      "28800/60000\tLoss: 221.153\n",
      "32000/60000\tLoss: 218.783\n",
      "35200/60000\tLoss: 218.654\n",
      "38400/60000\tLoss: 218.161\n",
      "41600/60000\tLoss: 217.909\n",
      "44800/60000\tLoss: 217.453\n",
      "48000/60000\tLoss: 217.382\n",
      "51200/60000\tLoss: 216.884\n",
      "54400/60000\tLoss: 217.093\n",
      "57600/60000\tLoss: 216.723\n",
      "Epoch: 6 Average loss: 219.51\n",
      "0/60000\tLoss: 213.926\n",
      "3200/60000\tLoss: 215.655\n",
      "6400/60000\tLoss: 213.763\n",
      "9600/60000\tLoss: 215.393\n",
      "12800/60000\tLoss: 214.186\n",
      "16000/60000\tLoss: 213.754\n",
      "19200/60000\tLoss: 213.417\n",
      "22400/60000\tLoss: 214.236\n",
      "25600/60000\tLoss: 215.093\n",
      "28800/60000\tLoss: 214.939\n",
      "32000/60000\tLoss: 210.646\n",
      "35200/60000\tLoss: 210.845\n",
      "38400/60000\tLoss: 213.376\n",
      "41600/60000\tLoss: 212.229\n",
      "44800/60000\tLoss: 210.689\n",
      "48000/60000\tLoss: 211.516\n",
      "51200/60000\tLoss: 211.715\n",
      "54400/60000\tLoss: 210.454\n",
      "57600/60000\tLoss: 209.645\n",
      "Epoch: 7 Average loss: 212.84\n",
      "0/60000\tLoss: 204.203\n",
      "3200/60000\tLoss: 209.649\n",
      "6400/60000\tLoss: 209.291\n",
      "9600/60000\tLoss: 208.526\n",
      "12800/60000\tLoss: 208.528\n",
      "16000/60000\tLoss: 209.134\n",
      "19200/60000\tLoss: 206.921\n",
      "22400/60000\tLoss: 209.078\n",
      "25600/60000\tLoss: 207.419\n",
      "28800/60000\tLoss: 206.373\n",
      "32000/60000\tLoss: 206.599\n",
      "35200/60000\tLoss: 204.992\n",
      "38400/60000\tLoss: 206.422\n",
      "41600/60000\tLoss: 207.322\n",
      "44800/60000\tLoss: 205.138\n",
      "48000/60000\tLoss: 207.590\n",
      "51200/60000\tLoss: 206.188\n",
      "54400/60000\tLoss: 208.464\n",
      "57600/60000\tLoss: 205.505\n",
      "Epoch: 8 Average loss: 207.38\n",
      "0/60000\tLoss: 203.506\n",
      "3200/60000\tLoss: 202.082\n",
      "6400/60000\tLoss: 203.514\n",
      "9600/60000\tLoss: 204.446\n",
      "12800/60000\tLoss: 203.985\n",
      "16000/60000\tLoss: 204.281\n",
      "19200/60000\tLoss: 203.556\n",
      "22400/60000\tLoss: 202.711\n",
      "25600/60000\tLoss: 203.793\n",
      "28800/60000\tLoss: 203.916\n",
      "32000/60000\tLoss: 202.570\n",
      "35200/60000\tLoss: 203.228\n",
      "38400/60000\tLoss: 201.238\n",
      "41600/60000\tLoss: 200.670\n",
      "44800/60000\tLoss: 200.718\n",
      "48000/60000\tLoss: 201.702\n",
      "51200/60000\tLoss: 199.761\n",
      "54400/60000\tLoss: 201.870\n",
      "57600/60000\tLoss: 201.515\n",
      "Epoch: 9 Average loss: 202.52\n",
      "0/60000\tLoss: 203.109\n",
      "3200/60000\tLoss: 201.402\n",
      "6400/60000\tLoss: 198.446\n",
      "9600/60000\tLoss: 199.829\n",
      "12800/60000\tLoss: 199.635\n",
      "16000/60000\tLoss: 199.996\n",
      "19200/60000\tLoss: 199.895\n",
      "22400/60000\tLoss: 199.794\n",
      "25600/60000\tLoss: 199.047\n",
      "28800/60000\tLoss: 198.018\n",
      "32000/60000\tLoss: 197.896\n",
      "35200/60000\tLoss: 199.073\n",
      "38400/60000\tLoss: 196.547\n",
      "41600/60000\tLoss: 198.187\n",
      "44800/60000\tLoss: 198.339\n",
      "48000/60000\tLoss: 199.247\n",
      "51200/60000\tLoss: 197.418\n",
      "54400/60000\tLoss: 196.437\n",
      "57600/60000\tLoss: 195.506\n",
      "Epoch: 10 Average loss: 198.65\n",
      "0/60000\tLoss: 186.564\n",
      "3200/60000\tLoss: 197.389\n",
      "6400/60000\tLoss: 196.520\n",
      "9600/60000\tLoss: 197.250\n",
      "12800/60000\tLoss: 195.845\n",
      "16000/60000\tLoss: 196.668\n",
      "19200/60000\tLoss: 194.589\n",
      "22400/60000\tLoss: 195.596\n",
      "25600/60000\tLoss: 195.238\n",
      "28800/60000\tLoss: 195.879\n",
      "32000/60000\tLoss: 195.853\n",
      "35200/60000\tLoss: 197.615\n",
      "38400/60000\tLoss: 197.411\n",
      "41600/60000\tLoss: 194.598\n",
      "44800/60000\tLoss: 193.799\n",
      "48000/60000\tLoss: 195.004\n",
      "51200/60000\tLoss: 193.844\n",
      "54400/60000\tLoss: 196.497\n",
      "57600/60000\tLoss: 194.430\n",
      "Epoch: 11 Average loss: 195.90\n",
      "0/60000\tLoss: 205.089\n",
      "3200/60000\tLoss: 196.017\n",
      "6400/60000\tLoss: 194.127\n",
      "9600/60000\tLoss: 197.259\n",
      "12800/60000\tLoss: 196.168\n",
      "16000/60000\tLoss: 193.767\n",
      "19200/60000\tLoss: 195.891\n",
      "22400/60000\tLoss: 195.268\n",
      "25600/60000\tLoss: 195.625\n",
      "28800/60000\tLoss: 194.687\n",
      "32000/60000\tLoss: 196.675\n",
      "35200/60000\tLoss: 192.973\n",
      "38400/60000\tLoss: 194.943\n",
      "41600/60000\tLoss: 194.384\n",
      "44800/60000\tLoss: 195.620\n",
      "48000/60000\tLoss: 194.262\n",
      "51200/60000\tLoss: 195.303\n",
      "54400/60000\tLoss: 197.803\n",
      "57600/60000\tLoss: 195.208\n",
      "Epoch: 12 Average loss: 195.44\n",
      "0/60000\tLoss: 202.413\n",
      "3200/60000\tLoss: 197.345\n",
      "6400/60000\tLoss: 195.174\n",
      "9600/60000\tLoss: 197.060\n",
      "12800/60000\tLoss: 197.236\n",
      "16000/60000\tLoss: 196.883\n",
      "19200/60000\tLoss: 195.970\n",
      "22400/60000\tLoss: 196.921\n",
      "25600/60000\tLoss: 195.452\n",
      "28800/60000\tLoss: 197.874\n",
      "32000/60000\tLoss: 196.249\n",
      "35200/60000\tLoss: 196.587\n",
      "38400/60000\tLoss: 196.820\n",
      "41600/60000\tLoss: 196.353\n",
      "44800/60000\tLoss: 195.807\n",
      "48000/60000\tLoss: 193.779\n",
      "51200/60000\tLoss: 196.417\n",
      "54400/60000\tLoss: 195.032\n",
      "57600/60000\tLoss: 193.239\n",
      "Epoch: 13 Average loss: 196.09\n",
      "0/60000\tLoss: 193.966\n",
      "3200/60000\tLoss: 195.699\n",
      "6400/60000\tLoss: 193.621\n",
      "9600/60000\tLoss: 193.784\n",
      "12800/60000\tLoss: 195.064\n",
      "16000/60000\tLoss: 194.780\n",
      "19200/60000\tLoss: 194.757\n",
      "22400/60000\tLoss: 194.452\n",
      "25600/60000\tLoss: 193.596\n",
      "28800/60000\tLoss: 191.809\n",
      "32000/60000\tLoss: 193.561\n",
      "35200/60000\tLoss: 192.583\n",
      "38400/60000\tLoss: 192.242\n",
      "41600/60000\tLoss: 196.380\n",
      "44800/60000\tLoss: 193.998\n",
      "48000/60000\tLoss: 193.133\n",
      "51200/60000\tLoss: 193.047\n",
      "54400/60000\tLoss: 191.865\n",
      "57600/60000\tLoss: 193.172\n",
      "Epoch: 14 Average loss: 193.79\n",
      "0/60000\tLoss: 181.067\n",
      "3200/60000\tLoss: 194.705\n",
      "6400/60000\tLoss: 192.443\n",
      "9600/60000\tLoss: 192.964\n",
      "12800/60000\tLoss: 190.887\n",
      "16000/60000\tLoss: 192.507\n",
      "19200/60000\tLoss: 191.690\n",
      "22400/60000\tLoss: 192.096\n",
      "25600/60000\tLoss: 190.758\n",
      "28800/60000\tLoss: 194.621\n",
      "32000/60000\tLoss: 191.087\n",
      "35200/60000\tLoss: 191.938\n",
      "38400/60000\tLoss: 193.275\n",
      "41600/60000\tLoss: 191.423\n",
      "44800/60000\tLoss: 193.133\n",
      "48000/60000\tLoss: 190.266\n",
      "51200/60000\tLoss: 190.007\n",
      "54400/60000\tLoss: 188.087\n",
      "57600/60000\tLoss: 191.814\n",
      "Epoch: 15 Average loss: 191.87\n",
      "0/60000\tLoss: 190.105\n",
      "3200/60000\tLoss: 191.721\n",
      "6400/60000\tLoss: 190.789\n",
      "9600/60000\tLoss: 188.225\n",
      "12800/60000\tLoss: 189.974\n",
      "16000/60000\tLoss: 190.899\n",
      "19200/60000\tLoss: 190.629\n",
      "22400/60000\tLoss: 190.360\n",
      "25600/60000\tLoss: 187.867\n",
      "28800/60000\tLoss: 189.307\n",
      "32000/60000\tLoss: 190.838\n",
      "35200/60000\tLoss: 190.697\n",
      "38400/60000\tLoss: 188.171\n",
      "41600/60000\tLoss: 190.707\n",
      "44800/60000\tLoss: 188.224\n",
      "48000/60000\tLoss: 190.071\n",
      "51200/60000\tLoss: 188.705\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "54400/60000\tLoss: 190.815\n",
      "57600/60000\tLoss: 190.668\n",
      "Epoch: 16 Average loss: 189.99\n",
      "0/60000\tLoss: 201.360\n",
      "3200/60000\tLoss: 187.981\n",
      "6400/60000\tLoss: 190.454\n",
      "9600/60000\tLoss: 190.468\n",
      "12800/60000\tLoss: 188.367\n",
      "16000/60000\tLoss: 186.763\n",
      "19200/60000\tLoss: 188.374\n",
      "22400/60000\tLoss: 188.492\n",
      "25600/60000\tLoss: 189.466\n",
      "28800/60000\tLoss: 188.550\n",
      "32000/60000\tLoss: 187.876\n",
      "35200/60000\tLoss: 188.318\n",
      "38400/60000\tLoss: 188.265\n",
      "41600/60000\tLoss: 187.115\n",
      "44800/60000\tLoss: 188.523\n",
      "48000/60000\tLoss: 187.621\n",
      "51200/60000\tLoss: 187.073\n",
      "54400/60000\tLoss: 186.137\n",
      "57600/60000\tLoss: 187.641\n",
      "Epoch: 17 Average loss: 188.22\n",
      "0/60000\tLoss: 185.206\n",
      "3200/60000\tLoss: 185.260\n",
      "6400/60000\tLoss: 187.668\n",
      "9600/60000\tLoss: 187.012\n",
      "12800/60000\tLoss: 187.100\n",
      "16000/60000\tLoss: 188.005\n",
      "19200/60000\tLoss: 186.866\n",
      "22400/60000\tLoss: 187.294\n",
      "25600/60000\tLoss: 187.118\n",
      "28800/60000\tLoss: 184.677\n",
      "32000/60000\tLoss: 187.477\n",
      "35200/60000\tLoss: 185.723\n",
      "38400/60000\tLoss: 186.343\n",
      "41600/60000\tLoss: 187.612\n",
      "44800/60000\tLoss: 186.196\n",
      "48000/60000\tLoss: 186.654\n",
      "51200/60000\tLoss: 185.748\n",
      "54400/60000\tLoss: 185.459\n",
      "57600/60000\tLoss: 184.127\n",
      "Epoch: 18 Average loss: 186.45\n",
      "0/60000\tLoss: 180.640\n",
      "3200/60000\tLoss: 185.957\n",
      "6400/60000\tLoss: 184.376\n",
      "9600/60000\tLoss: 184.226\n",
      "12800/60000\tLoss: 184.090\n",
      "16000/60000\tLoss: 185.269\n",
      "19200/60000\tLoss: 187.180\n",
      "22400/60000\tLoss: 188.078\n",
      "25600/60000\tLoss: 183.626\n",
      "28800/60000\tLoss: 185.405\n",
      "32000/60000\tLoss: 183.633\n",
      "35200/60000\tLoss: 182.888\n",
      "38400/60000\tLoss: 185.537\n",
      "41600/60000\tLoss: 184.566\n",
      "44800/60000\tLoss: 187.469\n",
      "48000/60000\tLoss: 186.274\n",
      "51200/60000\tLoss: 184.294\n",
      "54400/60000\tLoss: 183.160\n",
      "57600/60000\tLoss: 182.485\n",
      "Epoch: 19 Average loss: 184.97\n",
      "0/60000\tLoss: 177.456\n",
      "3200/60000\tLoss: 182.296\n",
      "6400/60000\tLoss: 181.509\n",
      "9600/60000\tLoss: 182.201\n",
      "12800/60000\tLoss: 181.867\n",
      "16000/60000\tLoss: 184.476\n",
      "19200/60000\tLoss: 181.401\n",
      "22400/60000\tLoss: 183.276\n",
      "25600/60000\tLoss: 183.563\n",
      "28800/60000\tLoss: 182.497\n",
      "32000/60000\tLoss: 183.401\n",
      "35200/60000\tLoss: 183.092\n",
      "38400/60000\tLoss: 184.196\n",
      "41600/60000\tLoss: 181.513\n",
      "44800/60000\tLoss: 180.941\n",
      "48000/60000\tLoss: 183.589\n",
      "51200/60000\tLoss: 183.236\n",
      "54400/60000\tLoss: 184.636\n",
      "57600/60000\tLoss: 179.869\n",
      "Epoch: 20 Average loss: 182.69\n",
      "0/60000\tLoss: 186.103\n",
      "3200/60000\tLoss: 183.744\n",
      "6400/60000\tLoss: 182.597\n",
      "9600/60000\tLoss: 180.184\n",
      "12800/60000\tLoss: 180.438\n",
      "16000/60000\tLoss: 181.638\n",
      "19200/60000\tLoss: 181.392\n",
      "22400/60000\tLoss: 181.683\n",
      "25600/60000\tLoss: 181.437\n",
      "28800/60000\tLoss: 181.799\n",
      "32000/60000\tLoss: 180.717\n",
      "35200/60000\tLoss: 181.133\n",
      "38400/60000\tLoss: 180.034\n",
      "41600/60000\tLoss: 178.650\n",
      "44800/60000\tLoss: 180.791\n",
      "48000/60000\tLoss: 180.141\n",
      "51200/60000\tLoss: 180.725\n",
      "54400/60000\tLoss: 179.523\n",
      "57600/60000\tLoss: 181.130\n",
      "Epoch: 21 Average loss: 181.07\n",
      "0/60000\tLoss: 181.693\n",
      "3200/60000\tLoss: 179.422\n",
      "6400/60000\tLoss: 181.028\n",
      "9600/60000\tLoss: 181.888\n",
      "12800/60000\tLoss: 181.353\n",
      "16000/60000\tLoss: 178.528\n",
      "19200/60000\tLoss: 179.735\n",
      "22400/60000\tLoss: 181.611\n",
      "25600/60000\tLoss: 181.017\n",
      "28800/60000\tLoss: 180.317\n",
      "32000/60000\tLoss: 179.547\n",
      "35200/60000\tLoss: 183.072\n",
      "38400/60000\tLoss: 183.301\n",
      "41600/60000\tLoss: 178.930\n",
      "44800/60000\tLoss: 176.672\n",
      "48000/60000\tLoss: 180.975\n",
      "51200/60000\tLoss: 178.521\n",
      "54400/60000\tLoss: 179.325\n",
      "57600/60000\tLoss: 178.680\n",
      "Epoch: 22 Average loss: 180.25\n",
      "0/60000\tLoss: 180.106\n",
      "3200/60000\tLoss: 176.681\n",
      "6400/60000\tLoss: 177.760\n",
      "9600/60000\tLoss: 179.826\n",
      "12800/60000\tLoss: 178.397\n",
      "16000/60000\tLoss: 178.601\n",
      "19200/60000\tLoss: 178.915\n",
      "22400/60000\tLoss: 179.121\n",
      "25600/60000\tLoss: 178.491\n",
      "28800/60000\tLoss: 177.083\n",
      "32000/60000\tLoss: 178.319\n",
      "35200/60000\tLoss: 176.935\n",
      "38400/60000\tLoss: 179.357\n",
      "41600/60000\tLoss: 179.959\n",
      "44800/60000\tLoss: 178.978\n",
      "48000/60000\tLoss: 177.183\n",
      "51200/60000\tLoss: 179.629\n",
      "54400/60000\tLoss: 176.900\n",
      "57600/60000\tLoss: 177.629\n",
      "Epoch: 23 Average loss: 178.31\n",
      "0/60000\tLoss: 170.875\n",
      "3200/60000\tLoss: 178.416\n",
      "6400/60000\tLoss: 175.437\n",
      "9600/60000\tLoss: 177.151\n",
      "12800/60000\tLoss: 176.532\n",
      "16000/60000\tLoss: 177.965\n",
      "19200/60000\tLoss: 177.522\n",
      "22400/60000\tLoss: 174.275\n",
      "25600/60000\tLoss: 176.498\n",
      "28800/60000\tLoss: 178.418\n",
      "32000/60000\tLoss: 178.357\n",
      "35200/60000\tLoss: 176.863\n",
      "38400/60000\tLoss: 179.213\n",
      "41600/60000\tLoss: 176.632\n",
      "44800/60000\tLoss: 177.071\n",
      "48000/60000\tLoss: 175.290\n",
      "51200/60000\tLoss: 177.827\n",
      "54400/60000\tLoss: 177.207\n",
      "57600/60000\tLoss: 175.188\n",
      "Epoch: 24 Average loss: 177.12\n",
      "0/60000\tLoss: 187.877\n",
      "3200/60000\tLoss: 176.824\n",
      "6400/60000\tLoss: 176.284\n",
      "9600/60000\tLoss: 178.155\n",
      "12800/60000\tLoss: 175.238\n",
      "16000/60000\tLoss: 179.352\n",
      "19200/60000\tLoss: 178.836\n",
      "22400/60000\tLoss: 179.114\n",
      "25600/60000\tLoss: 175.762\n",
      "28800/60000\tLoss: 176.952\n",
      "32000/60000\tLoss: 175.367\n",
      "35200/60000\tLoss: 178.698\n",
      "38400/60000\tLoss: 177.472\n",
      "41600/60000\tLoss: 174.938\n",
      "44800/60000\tLoss: 175.407\n",
      "48000/60000\tLoss: 177.466\n",
      "51200/60000\tLoss: 175.951\n",
      "54400/60000\tLoss: 177.513\n",
      "57600/60000\tLoss: 173.215\n",
      "Epoch: 25 Average loss: 176.80\n",
      "0/60000\tLoss: 171.900\n",
      "3200/60000\tLoss: 175.581\n",
      "6400/60000\tLoss: 174.573\n",
      "9600/60000\tLoss: 176.858\n",
      "12800/60000\tLoss: 174.974\n",
      "16000/60000\tLoss: 175.020\n",
      "19200/60000\tLoss: 175.353\n",
      "22400/60000\tLoss: 173.500\n",
      "25600/60000\tLoss: 177.425\n",
      "28800/60000\tLoss: 174.384\n",
      "32000/60000\tLoss: 175.378\n",
      "35200/60000\tLoss: 174.592\n",
      "38400/60000\tLoss: 173.635\n",
      "41600/60000\tLoss: 175.991\n",
      "44800/60000\tLoss: 175.639\n",
      "48000/60000\tLoss: 174.878\n",
      "51200/60000\tLoss: 173.464\n",
      "54400/60000\tLoss: 172.934\n",
      "57600/60000\tLoss: 174.576\n",
      "Epoch: 26 Average loss: 174.88\n",
      "0/60000\tLoss: 180.853\n",
      "3200/60000\tLoss: 174.461\n",
      "6400/60000\tLoss: 175.463\n",
      "9600/60000\tLoss: 174.637\n",
      "12800/60000\tLoss: 176.158\n",
      "16000/60000\tLoss: 172.442\n",
      "19200/60000\tLoss: 173.016\n",
      "22400/60000\tLoss: 172.719\n",
      "25600/60000\tLoss: 173.310\n",
      "28800/60000\tLoss: 178.927\n",
      "32000/60000\tLoss: 172.598\n",
      "35200/60000\tLoss: 172.378\n",
      "38400/60000\tLoss: 173.405\n",
      "41600/60000\tLoss: 171.077\n",
      "44800/60000\tLoss: 171.804\n",
      "48000/60000\tLoss: 173.811\n",
      "51200/60000\tLoss: 175.315\n",
      "54400/60000\tLoss: 174.267\n",
      "57600/60000\tLoss: 173.362\n",
      "Epoch: 27 Average loss: 173.94\n",
      "0/60000\tLoss: 180.149\n",
      "3200/60000\tLoss: 174.905\n",
      "6400/60000\tLoss: 174.892\n",
      "9600/60000\tLoss: 175.214\n",
      "12800/60000\tLoss: 170.937\n",
      "16000/60000\tLoss: 171.311\n",
      "19200/60000\tLoss: 174.352\n",
      "22400/60000\tLoss: 173.907\n",
      "25600/60000\tLoss: 173.685\n",
      "28800/60000\tLoss: 174.555\n",
      "32000/60000\tLoss: 174.852\n",
      "35200/60000\tLoss: 173.613\n",
      "38400/60000\tLoss: 171.341\n",
      "41600/60000\tLoss: 173.806\n",
      "44800/60000\tLoss: 172.982\n",
      "48000/60000\tLoss: 173.235\n",
      "51200/60000\tLoss: 173.329\n",
      "54400/60000\tLoss: 171.375\n",
      "57600/60000\tLoss: 171.878\n",
      "Epoch: 28 Average loss: 173.43\n",
      "0/60000\tLoss: 188.235\n",
      "3200/60000\tLoss: 171.918\n",
      "6400/60000\tLoss: 172.288\n",
      "9600/60000\tLoss: 171.872\n",
      "12800/60000\tLoss: 170.371\n",
      "16000/60000\tLoss: 173.768\n",
      "19200/60000\tLoss: 171.968\n",
      "22400/60000\tLoss: 170.378\n",
      "25600/60000\tLoss: 172.225\n",
      "28800/60000\tLoss: 173.249\n",
      "32000/60000\tLoss: 173.117\n",
      "35200/60000\tLoss: 170.516\n",
      "38400/60000\tLoss: 171.475\n",
      "41600/60000\tLoss: 172.152\n",
      "44800/60000\tLoss: 173.008\n",
      "48000/60000\tLoss: 171.430\n",
      "51200/60000\tLoss: 172.416\n",
      "54400/60000\tLoss: 171.163\n",
      "57600/60000\tLoss: 171.944\n",
      "Epoch: 29 Average loss: 172.15\n",
      "0/60000\tLoss: 167.853\n",
      "3200/60000\tLoss: 172.166\n",
      "6400/60000\tLoss: 170.480\n",
      "9600/60000\tLoss: 170.954\n",
      "12800/60000\tLoss: 172.422\n",
      "16000/60000\tLoss: 173.169\n",
      "19200/60000\tLoss: 172.242\n",
      "22400/60000\tLoss: 173.511\n",
      "25600/60000\tLoss: 170.923\n",
      "28800/60000\tLoss: 170.826\n",
      "32000/60000\tLoss: 172.049\n",
      "35200/60000\tLoss: 172.083\n",
      "38400/60000\tLoss: 171.301\n",
      "41600/60000\tLoss: 173.531\n",
      "44800/60000\tLoss: 171.717\n",
      "48000/60000\tLoss: 170.762\n",
      "51200/60000\tLoss: 173.446\n",
      "54400/60000\tLoss: 171.788\n",
      "57600/60000\tLoss: 171.316\n",
      "Epoch: 30 Average loss: 171.97\n",
      "0/60000\tLoss: 168.315\n",
      "3200/60000\tLoss: 172.044\n",
      "6400/60000\tLoss: 170.225\n",
      "9600/60000\tLoss: 171.668\n",
      "12800/60000\tLoss: 172.724\n",
      "16000/60000\tLoss: 170.819\n",
      "19200/60000\tLoss: 172.534\n",
      "22400/60000\tLoss: 172.534\n",
      "25600/60000\tLoss: 171.039\n",
      "28800/60000\tLoss: 172.668\n",
      "32000/60000\tLoss: 171.910\n",
      "35200/60000\tLoss: 170.509\n",
      "38400/60000\tLoss: 170.677\n",
      "41600/60000\tLoss: 171.823\n",
      "44800/60000\tLoss: 170.448\n",
      "48000/60000\tLoss: 173.759\n",
      "51200/60000\tLoss: 170.130\n",
      "54400/60000\tLoss: 171.335\n",
      "57600/60000\tLoss: 170.904\n",
      "Epoch: 31 Average loss: 171.57\n",
      "0/60000\tLoss: 159.626\n",
      "3200/60000\tLoss: 170.709\n",
      "6400/60000\tLoss: 171.926\n",
      "9600/60000\tLoss: 169.219\n",
      "12800/60000\tLoss: 169.666\n",
      "16000/60000\tLoss: 169.843\n",
      "19200/60000\tLoss: 171.615\n",
      "22400/60000\tLoss: 172.858\n",
      "25600/60000\tLoss: 173.401\n",
      "28800/60000\tLoss: 171.009\n",
      "32000/60000\tLoss: 170.165\n",
      "35200/60000\tLoss: 171.807\n",
      "38400/60000\tLoss: 169.550\n",
      "41600/60000\tLoss: 170.941\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44800/60000\tLoss: 170.867\n",
      "48000/60000\tLoss: 170.830\n",
      "51200/60000\tLoss: 171.081\n",
      "54400/60000\tLoss: 172.997\n",
      "57600/60000\tLoss: 170.664\n",
      "Epoch: 32 Average loss: 171.13\n",
      "0/60000\tLoss: 175.206\n",
      "3200/60000\tLoss: 172.120\n",
      "6400/60000\tLoss: 170.699\n",
      "9600/60000\tLoss: 171.922\n",
      "12800/60000\tLoss: 169.345\n",
      "16000/60000\tLoss: 170.089\n",
      "19200/60000\tLoss: 171.710\n",
      "22400/60000\tLoss: 169.262\n",
      "25600/60000\tLoss: 169.148\n",
      "28800/60000\tLoss: 172.718\n",
      "32000/60000\tLoss: 170.685\n",
      "35200/60000\tLoss: 172.162\n",
      "38400/60000\tLoss: 172.256\n",
      "41600/60000\tLoss: 172.126\n",
      "44800/60000\tLoss: 171.197\n",
      "48000/60000\tLoss: 170.575\n",
      "51200/60000\tLoss: 170.966\n",
      "54400/60000\tLoss: 170.501\n",
      "57600/60000\tLoss: 169.711\n",
      "Epoch: 33 Average loss: 171.05\n",
      "0/60000\tLoss: 163.144\n",
      "3200/60000\tLoss: 170.598\n",
      "6400/60000\tLoss: 170.754\n",
      "9600/60000\tLoss: 172.196\n",
      "12800/60000\tLoss: 171.983\n",
      "16000/60000\tLoss: 170.617\n",
      "19200/60000\tLoss: 169.974\n",
      "22400/60000\tLoss: 169.630\n",
      "25600/60000\tLoss: 170.381\n",
      "28800/60000\tLoss: 170.485\n",
      "32000/60000\tLoss: 170.012\n",
      "35200/60000\tLoss: 173.006\n",
      "38400/60000\tLoss: 168.947\n",
      "41600/60000\tLoss: 169.285\n",
      "44800/60000\tLoss: 168.372\n",
      "48000/60000\tLoss: 171.174\n",
      "51200/60000\tLoss: 169.779\n",
      "54400/60000\tLoss: 170.310\n",
      "57600/60000\tLoss: 169.978\n",
      "Epoch: 34 Average loss: 170.48\n",
      "0/60000\tLoss: 176.401\n",
      "3200/60000\tLoss: 171.710\n",
      "6400/60000\tLoss: 168.761\n",
      "9600/60000\tLoss: 169.445\n",
      "12800/60000\tLoss: 171.367\n",
      "16000/60000\tLoss: 170.469\n",
      "19200/60000\tLoss: 172.211\n",
      "22400/60000\tLoss: 170.634\n",
      "25600/60000\tLoss: 170.962\n",
      "28800/60000\tLoss: 170.655\n",
      "32000/60000\tLoss: 170.782\n",
      "35200/60000\tLoss: 170.087\n",
      "38400/60000\tLoss: 169.724\n",
      "41600/60000\tLoss: 170.408\n",
      "44800/60000\tLoss: 170.393\n",
      "48000/60000\tLoss: 169.146\n",
      "51200/60000\tLoss: 167.845\n",
      "54400/60000\tLoss: 170.564\n",
      "57600/60000\tLoss: 169.677\n",
      "Epoch: 35 Average loss: 170.32\n",
      "0/60000\tLoss: 169.451\n",
      "3200/60000\tLoss: 170.589\n",
      "6400/60000\tLoss: 169.545\n",
      "9600/60000\tLoss: 169.900\n",
      "12800/60000\tLoss: 169.910\n",
      "16000/60000\tLoss: 167.532\n",
      "19200/60000\tLoss: 170.458\n",
      "22400/60000\tLoss: 172.447\n",
      "25600/60000\tLoss: 169.534\n",
      "28800/60000\tLoss: 170.418\n",
      "32000/60000\tLoss: 168.831\n",
      "35200/60000\tLoss: 168.710\n",
      "38400/60000\tLoss: 171.099\n",
      "41600/60000\tLoss: 170.528\n",
      "44800/60000\tLoss: 171.356\n",
      "48000/60000\tLoss: 168.027\n",
      "51200/60000\tLoss: 170.931\n",
      "54400/60000\tLoss: 173.152\n",
      "57600/60000\tLoss: 170.206\n",
      "Epoch: 36 Average loss: 170.25\n",
      "0/60000\tLoss: 178.698\n",
      "3200/60000\tLoss: 170.035\n",
      "6400/60000\tLoss: 167.932\n",
      "9600/60000\tLoss: 168.853\n",
      "12800/60000\tLoss: 168.629\n",
      "16000/60000\tLoss: 169.383\n",
      "19200/60000\tLoss: 170.082\n",
      "22400/60000\tLoss: 169.107\n",
      "25600/60000\tLoss: 168.749\n",
      "28800/60000\tLoss: 172.239\n",
      "32000/60000\tLoss: 171.867\n",
      "35200/60000\tLoss: 171.491\n",
      "38400/60000\tLoss: 168.951\n",
      "41600/60000\tLoss: 170.921\n",
      "44800/60000\tLoss: 170.107\n",
      "48000/60000\tLoss: 168.413\n",
      "51200/60000\tLoss: 171.070\n",
      "54400/60000\tLoss: 166.291\n",
      "57600/60000\tLoss: 169.638\n",
      "Epoch: 37 Average loss: 169.81\n",
      "0/60000\tLoss: 175.583\n",
      "3200/60000\tLoss: 168.742\n",
      "6400/60000\tLoss: 169.957\n",
      "9600/60000\tLoss: 170.190\n",
      "12800/60000\tLoss: 170.104\n",
      "16000/60000\tLoss: 171.973\n",
      "19200/60000\tLoss: 170.161\n",
      "22400/60000\tLoss: 170.529\n",
      "25600/60000\tLoss: 167.360\n",
      "28800/60000\tLoss: 168.073\n",
      "32000/60000\tLoss: 168.795\n",
      "35200/60000\tLoss: 168.342\n",
      "38400/60000\tLoss: 169.078\n",
      "41600/60000\tLoss: 170.077\n",
      "44800/60000\tLoss: 170.240\n",
      "48000/60000\tLoss: 169.802\n",
      "51200/60000\tLoss: 169.417\n",
      "54400/60000\tLoss: 167.101\n",
      "57600/60000\tLoss: 168.680\n",
      "Epoch: 38 Average loss: 169.53\n",
      "0/60000\tLoss: 163.000\n",
      "3200/60000\tLoss: 168.309\n",
      "6400/60000\tLoss: 168.525\n",
      "9600/60000\tLoss: 170.029\n",
      "12800/60000\tLoss: 168.403\n",
      "16000/60000\tLoss: 168.373\n",
      "19200/60000\tLoss: 168.876\n",
      "22400/60000\tLoss: 170.840\n",
      "25600/60000\tLoss: 169.193\n",
      "28800/60000\tLoss: 168.644\n",
      "32000/60000\tLoss: 168.054\n",
      "35200/60000\tLoss: 168.724\n",
      "38400/60000\tLoss: 167.942\n",
      "41600/60000\tLoss: 168.025\n",
      "44800/60000\tLoss: 170.646\n",
      "48000/60000\tLoss: 171.082\n",
      "51200/60000\tLoss: 169.825\n",
      "54400/60000\tLoss: 170.276\n",
      "57600/60000\tLoss: 170.454\n",
      "Epoch: 39 Average loss: 169.24\n",
      "0/60000\tLoss: 155.927\n",
      "3200/60000\tLoss: 169.034\n",
      "6400/60000\tLoss: 169.729\n",
      "9600/60000\tLoss: 168.229\n",
      "12800/60000\tLoss: 172.023\n",
      "16000/60000\tLoss: 171.287\n",
      "19200/60000\tLoss: 171.621\n",
      "22400/60000\tLoss: 170.032\n",
      "25600/60000\tLoss: 169.259\n",
      "28800/60000\tLoss: 169.039\n",
      "32000/60000\tLoss: 169.423\n",
      "35200/60000\tLoss: 167.560\n",
      "38400/60000\tLoss: 171.504\n",
      "41600/60000\tLoss: 171.491\n",
      "44800/60000\tLoss: 171.228\n",
      "48000/60000\tLoss: 168.347\n",
      "51200/60000\tLoss: 170.122\n",
      "54400/60000\tLoss: 167.850\n",
      "57600/60000\tLoss: 169.165\n",
      "Epoch: 40 Average loss: 169.86\n",
      "0/60000\tLoss: 164.305\n",
      "3200/60000\tLoss: 169.505\n",
      "6400/60000\tLoss: 168.980\n",
      "9600/60000\tLoss: 167.071\n",
      "12800/60000\tLoss: 169.419\n",
      "16000/60000\tLoss: 168.459\n",
      "19200/60000\tLoss: 168.841\n",
      "22400/60000\tLoss: 168.670\n",
      "25600/60000\tLoss: 167.587\n",
      "28800/60000\tLoss: 168.867\n",
      "32000/60000\tLoss: 167.968\n",
      "35200/60000\tLoss: 165.816\n",
      "38400/60000\tLoss: 169.518\n",
      "41600/60000\tLoss: 169.361\n",
      "44800/60000\tLoss: 169.474\n",
      "48000/60000\tLoss: 167.831\n",
      "51200/60000\tLoss: 168.697\n",
      "54400/60000\tLoss: 167.937\n",
      "57600/60000\tLoss: 169.279\n",
      "Epoch: 41 Average loss: 168.62\n",
      "0/60000\tLoss: 172.200\n",
      "3200/60000\tLoss: 167.441\n",
      "6400/60000\tLoss: 167.730\n",
      "9600/60000\tLoss: 168.725\n",
      "12800/60000\tLoss: 167.631\n",
      "16000/60000\tLoss: 169.386\n",
      "19200/60000\tLoss: 168.033\n",
      "22400/60000\tLoss: 169.811\n",
      "25600/60000\tLoss: 167.631\n",
      "28800/60000\tLoss: 169.860\n",
      "32000/60000\tLoss: 170.494\n",
      "35200/60000\tLoss: 170.530\n",
      "38400/60000\tLoss: 169.346\n",
      "41600/60000\tLoss: 167.780\n",
      "44800/60000\tLoss: 168.773\n",
      "48000/60000\tLoss: 168.013\n",
      "51200/60000\tLoss: 167.452\n",
      "54400/60000\tLoss: 166.868\n",
      "57600/60000\tLoss: 170.408\n",
      "Epoch: 42 Average loss: 168.74\n",
      "0/60000\tLoss: 177.019\n",
      "3200/60000\tLoss: 168.185\n",
      "6400/60000\tLoss: 170.733\n",
      "9600/60000\tLoss: 168.514\n",
      "12800/60000\tLoss: 169.207\n",
      "16000/60000\tLoss: 170.313\n",
      "19200/60000\tLoss: 168.480\n",
      "22400/60000\tLoss: 168.648\n",
      "25600/60000\tLoss: 169.068\n",
      "28800/60000\tLoss: 168.412\n",
      "32000/60000\tLoss: 168.875\n",
      "35200/60000\tLoss: 168.967\n",
      "38400/60000\tLoss: 169.254\n",
      "41600/60000\tLoss: 168.242\n",
      "44800/60000\tLoss: 169.152\n",
      "48000/60000\tLoss: 170.714\n",
      "51200/60000\tLoss: 166.750\n",
      "54400/60000\tLoss: 166.742\n",
      "57600/60000\tLoss: 166.661\n",
      "Epoch: 43 Average loss: 168.87\n",
      "0/60000\tLoss: 163.665\n",
      "3200/60000\tLoss: 168.908\n",
      "6400/60000\tLoss: 172.282\n",
      "9600/60000\tLoss: 168.973\n",
      "12800/60000\tLoss: 169.275\n",
      "16000/60000\tLoss: 169.469\n",
      "19200/60000\tLoss: 170.828\n",
      "22400/60000\tLoss: 166.843\n",
      "25600/60000\tLoss: 166.807\n",
      "28800/60000\tLoss: 172.033\n",
      "32000/60000\tLoss: 167.782\n",
      "35200/60000\tLoss: 166.891\n",
      "38400/60000\tLoss: 167.567\n",
      "41600/60000\tLoss: 167.837\n",
      "44800/60000\tLoss: 167.611\n",
      "48000/60000\tLoss: 167.603\n",
      "51200/60000\tLoss: 171.352\n",
      "54400/60000\tLoss: 167.511\n",
      "57600/60000\tLoss: 167.985\n",
      "Epoch: 44 Average loss: 168.78\n",
      "0/60000\tLoss: 171.903\n",
      "3200/60000\tLoss: 170.196\n",
      "6400/60000\tLoss: 169.222\n",
      "9600/60000\tLoss: 170.333\n",
      "12800/60000\tLoss: 168.265\n",
      "16000/60000\tLoss: 169.337\n",
      "19200/60000\tLoss: 168.051\n",
      "22400/60000\tLoss: 167.214\n",
      "25600/60000\tLoss: 167.504\n",
      "28800/60000\tLoss: 171.782\n",
      "32000/60000\tLoss: 169.897\n",
      "35200/60000\tLoss: 168.281\n",
      "38400/60000\tLoss: 166.561\n",
      "41600/60000\tLoss: 169.440\n",
      "44800/60000\tLoss: 166.750\n",
      "48000/60000\tLoss: 168.677\n",
      "51200/60000\tLoss: 167.932\n",
      "54400/60000\tLoss: 170.270\n",
      "57600/60000\tLoss: 168.607\n",
      "Epoch: 45 Average loss: 168.86\n",
      "0/60000\tLoss: 173.069\n",
      "3200/60000\tLoss: 167.927\n",
      "6400/60000\tLoss: 167.323\n",
      "9600/60000\tLoss: 166.608\n",
      "12800/60000\tLoss: 167.815\n",
      "16000/60000\tLoss: 169.100\n",
      "19200/60000\tLoss: 166.494\n",
      "22400/60000\tLoss: 169.503\n",
      "25600/60000\tLoss: 168.851\n",
      "28800/60000\tLoss: 168.717\n",
      "32000/60000\tLoss: 167.800\n",
      "35200/60000\tLoss: 166.070\n",
      "38400/60000\tLoss: 168.741\n",
      "41600/60000\tLoss: 169.051\n",
      "44800/60000\tLoss: 168.260\n",
      "48000/60000\tLoss: 167.283\n",
      "51200/60000\tLoss: 168.788\n",
      "54400/60000\tLoss: 167.936\n",
      "57600/60000\tLoss: 168.995\n",
      "Epoch: 46 Average loss: 168.18\n",
      "0/60000\tLoss: 167.669\n",
      "3200/60000\tLoss: 168.153\n",
      "6400/60000\tLoss: 167.413\n",
      "9600/60000\tLoss: 168.659\n",
      "12800/60000\tLoss: 167.681\n",
      "16000/60000\tLoss: 167.160\n",
      "19200/60000\tLoss: 170.368\n",
      "22400/60000\tLoss: 167.809\n",
      "25600/60000\tLoss: 167.173\n",
      "28800/60000\tLoss: 168.291\n",
      "32000/60000\tLoss: 169.076\n",
      "35200/60000\tLoss: 168.969\n",
      "38400/60000\tLoss: 166.869\n",
      "41600/60000\tLoss: 169.616\n",
      "44800/60000\tLoss: 168.861\n",
      "48000/60000\tLoss: 170.215\n",
      "51200/60000\tLoss: 167.833\n",
      "54400/60000\tLoss: 166.234\n",
      "57600/60000\tLoss: 169.765\n",
      "Epoch: 47 Average loss: 168.46\n",
      "0/60000\tLoss: 179.612\n",
      "3200/60000\tLoss: 169.780\n",
      "6400/60000\tLoss: 166.634\n",
      "9600/60000\tLoss: 166.594\n",
      "12800/60000\tLoss: 168.055\n",
      "16000/60000\tLoss: 167.981\n",
      "19200/60000\tLoss: 169.575\n",
      "22400/60000\tLoss: 170.205\n",
      "25600/60000\tLoss: 167.268\n",
      "28800/60000\tLoss: 169.190\n",
      "32000/60000\tLoss: 167.153\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35200/60000\tLoss: 167.908\n",
      "38400/60000\tLoss: 168.849\n",
      "41600/60000\tLoss: 170.540\n",
      "44800/60000\tLoss: 166.425\n",
      "48000/60000\tLoss: 168.664\n",
      "51200/60000\tLoss: 168.012\n",
      "54400/60000\tLoss: 168.268\n",
      "57600/60000\tLoss: 167.332\n",
      "Epoch: 48 Average loss: 168.34\n",
      "0/60000\tLoss: 154.329\n",
      "3200/60000\tLoss: 167.134\n",
      "6400/60000\tLoss: 167.993\n",
      "9600/60000\tLoss: 169.527\n",
      "12800/60000\tLoss: 168.474\n",
      "16000/60000\tLoss: 168.712\n",
      "19200/60000\tLoss: 169.037\n",
      "22400/60000\tLoss: 167.863\n",
      "25600/60000\tLoss: 167.970\n",
      "28800/60000\tLoss: 167.970\n",
      "32000/60000\tLoss: 168.046\n",
      "35200/60000\tLoss: 166.377\n",
      "38400/60000\tLoss: 168.655\n",
      "41600/60000\tLoss: 167.954\n",
      "44800/60000\tLoss: 167.747\n",
      "48000/60000\tLoss: 169.697\n",
      "51200/60000\tLoss: 168.237\n",
      "54400/60000\tLoss: 169.681\n",
      "57600/60000\tLoss: 168.515\n",
      "Epoch: 49 Average loss: 168.41\n",
      "0/60000\tLoss: 171.135\n",
      "3200/60000\tLoss: 166.184\n",
      "6400/60000\tLoss: 170.078\n",
      "9600/60000\tLoss: 170.021\n",
      "12800/60000\tLoss: 168.637\n",
      "16000/60000\tLoss: 166.044\n",
      "19200/60000\tLoss: 168.673\n",
      "22400/60000\tLoss: 167.225\n",
      "25600/60000\tLoss: 167.899\n",
      "28800/60000\tLoss: 167.611\n",
      "32000/60000\tLoss: 167.142\n",
      "35200/60000\tLoss: 167.569\n",
      "38400/60000\tLoss: 166.703\n",
      "41600/60000\tLoss: 168.452\n",
      "44800/60000\tLoss: 168.306\n",
      "48000/60000\tLoss: 167.796\n",
      "51200/60000\tLoss: 168.953\n",
      "54400/60000\tLoss: 167.883\n",
      "57600/60000\tLoss: 165.161\n",
      "Epoch: 50 Average loss: 167.91\n",
      "0/60000\tLoss: 157.272\n",
      "3200/60000\tLoss: 166.862\n",
      "6400/60000\tLoss: 167.050\n",
      "9600/60000\tLoss: 168.390\n",
      "12800/60000\tLoss: 167.434\n",
      "16000/60000\tLoss: 169.385\n",
      "19200/60000\tLoss: 167.350\n",
      "22400/60000\tLoss: 166.496\n",
      "25600/60000\tLoss: 165.803\n",
      "28800/60000\tLoss: 167.878\n",
      "32000/60000\tLoss: 167.448\n",
      "35200/60000\tLoss: 167.425\n",
      "38400/60000\tLoss: 167.030\n",
      "41600/60000\tLoss: 166.989\n",
      "44800/60000\tLoss: 168.880\n",
      "48000/60000\tLoss: 168.189\n",
      "51200/60000\tLoss: 168.810\n",
      "54400/60000\tLoss: 166.322\n",
      "57600/60000\tLoss: 168.812\n",
      "Epoch: 51 Average loss: 167.76\n",
      "0/60000\tLoss: 191.574\n",
      "3200/60000\tLoss: 168.761\n",
      "6400/60000\tLoss: 166.989\n",
      "9600/60000\tLoss: 168.718\n",
      "12800/60000\tLoss: 167.807\n",
      "16000/60000\tLoss: 168.377\n",
      "19200/60000\tLoss: 167.644\n",
      "22400/60000\tLoss: 167.688\n",
      "25600/60000\tLoss: 167.044\n",
      "28800/60000\tLoss: 168.372\n",
      "32000/60000\tLoss: 168.822\n",
      "35200/60000\tLoss: 167.981\n",
      "38400/60000\tLoss: 166.191\n",
      "41600/60000\tLoss: 166.526\n",
      "44800/60000\tLoss: 168.342\n",
      "48000/60000\tLoss: 167.891\n",
      "51200/60000\tLoss: 165.843\n",
      "54400/60000\tLoss: 165.137\n",
      "57600/60000\tLoss: 167.918\n",
      "Epoch: 52 Average loss: 167.66\n",
      "0/60000\tLoss: 170.939\n",
      "3200/60000\tLoss: 167.204\n",
      "6400/60000\tLoss: 169.132\n",
      "9600/60000\tLoss: 167.844\n",
      "12800/60000\tLoss: 170.588\n",
      "16000/60000\tLoss: 169.339\n",
      "19200/60000\tLoss: 169.374\n",
      "22400/60000\tLoss: 167.422\n",
      "25600/60000\tLoss: 166.223\n",
      "28800/60000\tLoss: 166.024\n",
      "32000/60000\tLoss: 166.091\n",
      "35200/60000\tLoss: 168.342\n",
      "38400/60000\tLoss: 166.575\n",
      "41600/60000\tLoss: 164.224\n",
      "44800/60000\tLoss: 167.674\n",
      "48000/60000\tLoss: 167.403\n",
      "51200/60000\tLoss: 165.610\n",
      "54400/60000\tLoss: 167.945\n",
      "57600/60000\tLoss: 167.852\n",
      "Epoch: 53 Average loss: 167.56\n",
      "0/60000\tLoss: 161.893\n",
      "3200/60000\tLoss: 169.747\n",
      "6400/60000\tLoss: 167.742\n",
      "9600/60000\tLoss: 169.536\n",
      "12800/60000\tLoss: 168.178\n",
      "16000/60000\tLoss: 166.204\n",
      "19200/60000\tLoss: 167.239\n",
      "22400/60000\tLoss: 166.420\n",
      "25600/60000\tLoss: 166.150\n",
      "28800/60000\tLoss: 167.436\n",
      "32000/60000\tLoss: 167.474\n",
      "35200/60000\tLoss: 169.147\n",
      "38400/60000\tLoss: 171.214\n",
      "41600/60000\tLoss: 167.841\n",
      "44800/60000\tLoss: 166.709\n",
      "48000/60000\tLoss: 168.644\n",
      "51200/60000\tLoss: 166.486\n",
      "54400/60000\tLoss: 165.587\n",
      "57600/60000\tLoss: 166.172\n",
      "Epoch: 54 Average loss: 167.65\n",
      "0/60000\tLoss: 158.291\n",
      "3200/60000\tLoss: 166.067\n",
      "6400/60000\tLoss: 168.961\n",
      "9600/60000\tLoss: 165.877\n",
      "12800/60000\tLoss: 165.789\n",
      "16000/60000\tLoss: 167.526\n",
      "19200/60000\tLoss: 167.855\n",
      "22400/60000\tLoss: 170.112\n",
      "25600/60000\tLoss: 167.984\n",
      "28800/60000\tLoss: 166.778\n",
      "32000/60000\tLoss: 167.850\n",
      "35200/60000\tLoss: 166.939\n",
      "38400/60000\tLoss: 168.420\n",
      "41600/60000\tLoss: 167.069\n",
      "44800/60000\tLoss: 166.255\n",
      "48000/60000\tLoss: 167.045\n",
      "51200/60000\tLoss: 167.488\n",
      "54400/60000\tLoss: 165.903\n",
      "57600/60000\tLoss: 167.365\n",
      "Epoch: 55 Average loss: 167.37\n",
      "0/60000\tLoss: 170.685\n",
      "3200/60000\tLoss: 167.657\n",
      "6400/60000\tLoss: 167.405\n",
      "9600/60000\tLoss: 167.127\n",
      "12800/60000\tLoss: 168.191\n",
      "16000/60000\tLoss: 165.840\n",
      "19200/60000\tLoss: 165.868\n",
      "22400/60000\tLoss: 167.590\n",
      "25600/60000\tLoss: 166.185\n",
      "28800/60000\tLoss: 169.363\n",
      "32000/60000\tLoss: 168.228\n",
      "35200/60000\tLoss: 167.313\n",
      "38400/60000\tLoss: 166.809\n",
      "41600/60000\tLoss: 167.922\n",
      "44800/60000\tLoss: 167.035\n",
      "48000/60000\tLoss: 167.412\n",
      "51200/60000\tLoss: 167.635\n",
      "54400/60000\tLoss: 167.308\n",
      "57600/60000\tLoss: 165.858\n",
      "Epoch: 56 Average loss: 167.39\n",
      "0/60000\tLoss: 167.093\n",
      "3200/60000\tLoss: 168.254\n",
      "6400/60000\tLoss: 166.637\n",
      "9600/60000\tLoss: 167.138\n",
      "12800/60000\tLoss: 170.173\n",
      "16000/60000\tLoss: 167.439\n",
      "19200/60000\tLoss: 167.100\n",
      "22400/60000\tLoss: 166.043\n",
      "25600/60000\tLoss: 165.102\n",
      "28800/60000\tLoss: 167.611\n",
      "32000/60000\tLoss: 168.280\n",
      "35200/60000\tLoss: 165.875\n",
      "38400/60000\tLoss: 167.406\n",
      "41600/60000\tLoss: 166.551\n",
      "44800/60000\tLoss: 165.628\n",
      "48000/60000\tLoss: 168.375\n",
      "51200/60000\tLoss: 166.388\n",
      "54400/60000\tLoss: 166.462\n",
      "57600/60000\tLoss: 168.144\n",
      "Epoch: 57 Average loss: 167.30\n",
      "0/60000\tLoss: 160.575\n",
      "3200/60000\tLoss: 167.356\n",
      "6400/60000\tLoss: 167.225\n",
      "9600/60000\tLoss: 167.123\n",
      "12800/60000\tLoss: 166.149\n",
      "16000/60000\tLoss: 168.129\n",
      "19200/60000\tLoss: 165.755\n",
      "22400/60000\tLoss: 165.461\n",
      "25600/60000\tLoss: 166.595\n",
      "28800/60000\tLoss: 166.554\n",
      "32000/60000\tLoss: 169.172\n",
      "35200/60000\tLoss: 165.504\n",
      "38400/60000\tLoss: 167.356\n",
      "41600/60000\tLoss: 167.410\n",
      "44800/60000\tLoss: 167.721\n",
      "48000/60000\tLoss: 167.165\n",
      "51200/60000\tLoss: 169.443\n",
      "54400/60000\tLoss: 169.750\n",
      "57600/60000\tLoss: 165.269\n",
      "Epoch: 58 Average loss: 167.24\n",
      "0/60000\tLoss: 164.472\n",
      "3200/60000\tLoss: 167.515\n",
      "6400/60000\tLoss: 167.615\n",
      "9600/60000\tLoss: 166.679\n",
      "12800/60000\tLoss: 165.231\n",
      "16000/60000\tLoss: 168.315\n",
      "19200/60000\tLoss: 169.807\n",
      "22400/60000\tLoss: 166.524\n",
      "25600/60000\tLoss: 165.862\n",
      "28800/60000\tLoss: 168.731\n",
      "32000/60000\tLoss: 168.493\n",
      "35200/60000\tLoss: 168.290\n",
      "38400/60000\tLoss: 168.595\n",
      "41600/60000\tLoss: 166.102\n",
      "44800/60000\tLoss: 167.931\n",
      "48000/60000\tLoss: 165.728\n",
      "51200/60000\tLoss: 169.358\n",
      "54400/60000\tLoss: 168.727\n",
      "57600/60000\tLoss: 166.601\n",
      "Epoch: 59 Average loss: 167.58\n",
      "0/60000\tLoss: 173.841\n",
      "3200/60000\tLoss: 165.230\n",
      "6400/60000\tLoss: 167.461\n",
      "9600/60000\tLoss: 167.532\n",
      "12800/60000\tLoss: 165.792\n",
      "16000/60000\tLoss: 171.371\n",
      "19200/60000\tLoss: 168.671\n",
      "22400/60000\tLoss: 165.277\n",
      "25600/60000\tLoss: 164.787\n",
      "28800/60000\tLoss: 166.589\n",
      "32000/60000\tLoss: 166.708\n",
      "35200/60000\tLoss: 166.590\n",
      "38400/60000\tLoss: 167.808\n",
      "41600/60000\tLoss: 166.895\n",
      "44800/60000\tLoss: 167.933\n",
      "48000/60000\tLoss: 166.459\n",
      "51200/60000\tLoss: 166.992\n",
      "54400/60000\tLoss: 168.096\n",
      "57600/60000\tLoss: 167.257\n",
      "Epoch: 60 Average loss: 167.18\n",
      "0/60000\tLoss: 159.153\n",
      "3200/60000\tLoss: 165.463\n",
      "6400/60000\tLoss: 166.413\n",
      "9600/60000\tLoss: 166.014\n",
      "12800/60000\tLoss: 168.211\n",
      "16000/60000\tLoss: 165.571\n",
      "19200/60000\tLoss: 167.211\n",
      "22400/60000\tLoss: 167.514\n",
      "25600/60000\tLoss: 167.406\n",
      "28800/60000\tLoss: 166.037\n",
      "32000/60000\tLoss: 170.875\n",
      "35200/60000\tLoss: 168.290\n",
      "38400/60000\tLoss: 168.242\n",
      "41600/60000\tLoss: 166.090\n",
      "44800/60000\tLoss: 166.994\n",
      "48000/60000\tLoss: 167.896\n",
      "51200/60000\tLoss: 165.923\n",
      "54400/60000\tLoss: 166.685\n",
      "57600/60000\tLoss: 165.736\n",
      "Epoch: 61 Average loss: 167.08\n",
      "0/60000\tLoss: 156.389\n",
      "3200/60000\tLoss: 167.069\n",
      "6400/60000\tLoss: 167.812\n",
      "9600/60000\tLoss: 167.423\n",
      "12800/60000\tLoss: 165.649\n",
      "16000/60000\tLoss: 169.997\n",
      "19200/60000\tLoss: 166.163\n",
      "22400/60000\tLoss: 165.809\n",
      "25600/60000\tLoss: 167.570\n",
      "28800/60000\tLoss: 167.913\n",
      "32000/60000\tLoss: 165.531\n",
      "35200/60000\tLoss: 168.107\n",
      "38400/60000\tLoss: 165.483\n",
      "41600/60000\tLoss: 167.734\n",
      "44800/60000\tLoss: 168.487\n",
      "48000/60000\tLoss: 165.895\n",
      "51200/60000\tLoss: 165.295\n",
      "54400/60000\tLoss: 167.543\n",
      "57600/60000\tLoss: 167.001\n",
      "Epoch: 62 Average loss: 167.05\n",
      "0/60000\tLoss: 154.042\n",
      "3200/60000\tLoss: 167.881\n",
      "6400/60000\tLoss: 166.816\n",
      "9600/60000\tLoss: 170.079\n",
      "12800/60000\tLoss: 166.455\n",
      "16000/60000\tLoss: 166.604\n",
      "19200/60000\tLoss: 166.837\n",
      "22400/60000\tLoss: 165.718\n",
      "25600/60000\tLoss: 166.808\n",
      "28800/60000\tLoss: 166.861\n",
      "32000/60000\tLoss: 167.369\n",
      "35200/60000\tLoss: 166.122\n",
      "38400/60000\tLoss: 167.410\n",
      "41600/60000\tLoss: 166.782\n",
      "44800/60000\tLoss: 167.125\n",
      "48000/60000\tLoss: 167.025\n",
      "51200/60000\tLoss: 165.774\n",
      "54400/60000\tLoss: 165.769\n",
      "57600/60000\tLoss: 166.858\n",
      "Epoch: 63 Average loss: 166.99\n",
      "0/60000\tLoss: 169.182\n",
      "3200/60000\tLoss: 165.642\n",
      "6400/60000\tLoss: 168.255\n",
      "9600/60000\tLoss: 166.643\n",
      "12800/60000\tLoss: 166.134\n",
      "16000/60000\tLoss: 168.216\n",
      "19200/60000\tLoss: 166.464\n",
      "22400/60000\tLoss: 165.743\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25600/60000\tLoss: 165.484\n",
      "28800/60000\tLoss: 166.573\n",
      "32000/60000\tLoss: 166.110\n",
      "35200/60000\tLoss: 168.072\n",
      "38400/60000\tLoss: 168.169\n",
      "41600/60000\tLoss: 167.488\n",
      "44800/60000\tLoss: 165.326\n",
      "48000/60000\tLoss: 166.928\n",
      "51200/60000\tLoss: 166.050\n",
      "54400/60000\tLoss: 165.566\n",
      "57600/60000\tLoss: 166.906\n",
      "Epoch: 64 Average loss: 166.66\n",
      "0/60000\tLoss: 166.450\n",
      "3200/60000\tLoss: 165.742\n",
      "6400/60000\tLoss: 166.008\n",
      "9600/60000\tLoss: 165.476\n",
      "12800/60000\tLoss: 167.448\n",
      "16000/60000\tLoss: 165.276\n",
      "19200/60000\tLoss: 167.666\n",
      "22400/60000\tLoss: 166.249\n",
      "25600/60000\tLoss: 165.092\n",
      "28800/60000\tLoss: 166.356\n",
      "32000/60000\tLoss: 167.120\n",
      "35200/60000\tLoss: 167.257\n",
      "38400/60000\tLoss: 167.462\n",
      "41600/60000\tLoss: 167.758\n",
      "44800/60000\tLoss: 167.873\n",
      "48000/60000\tLoss: 164.917\n",
      "51200/60000\tLoss: 166.944\n",
      "54400/60000\tLoss: 166.138\n",
      "57600/60000\tLoss: 165.665\n",
      "Epoch: 65 Average loss: 166.50\n",
      "0/60000\tLoss: 168.182\n",
      "3200/60000\tLoss: 166.007\n",
      "6400/60000\tLoss: 166.639\n",
      "9600/60000\tLoss: 165.326\n",
      "12800/60000\tLoss: 166.757\n",
      "16000/60000\tLoss: 165.781\n",
      "19200/60000\tLoss: 165.793\n",
      "22400/60000\tLoss: 168.045\n",
      "25600/60000\tLoss: 166.518\n",
      "28800/60000\tLoss: 167.458\n",
      "32000/60000\tLoss: 164.977\n",
      "35200/60000\tLoss: 167.814\n",
      "38400/60000\tLoss: 167.043\n",
      "41600/60000\tLoss: 165.501\n",
      "44800/60000\tLoss: 166.186\n",
      "48000/60000\tLoss: 168.628\n",
      "51200/60000\tLoss: 166.446\n",
      "54400/60000\tLoss: 166.075\n",
      "57600/60000\tLoss: 166.764\n",
      "Epoch: 66 Average loss: 166.65\n",
      "0/60000\tLoss: 153.785\n",
      "3200/60000\tLoss: 166.412\n",
      "6400/60000\tLoss: 167.873\n",
      "9600/60000\tLoss: 164.583\n",
      "12800/60000\tLoss: 164.957\n",
      "16000/60000\tLoss: 165.791\n",
      "19200/60000\tLoss: 166.552\n",
      "22400/60000\tLoss: 167.517\n",
      "25600/60000\tLoss: 166.025\n",
      "28800/60000\tLoss: 165.548\n",
      "32000/60000\tLoss: 166.538\n",
      "35200/60000\tLoss: 167.470\n",
      "38400/60000\tLoss: 165.189\n",
      "41600/60000\tLoss: 167.125\n",
      "44800/60000\tLoss: 167.093\n",
      "48000/60000\tLoss: 170.156\n",
      "51200/60000\tLoss: 167.587\n",
      "54400/60000\tLoss: 167.967\n",
      "57600/60000\tLoss: 165.750\n",
      "Epoch: 67 Average loss: 166.76\n",
      "0/60000\tLoss: 167.049\n",
      "3200/60000\tLoss: 165.135\n",
      "6400/60000\tLoss: 165.345\n",
      "9600/60000\tLoss: 168.515\n",
      "12800/60000\tLoss: 168.232\n",
      "16000/60000\tLoss: 165.623\n",
      "19200/60000\tLoss: 162.995\n",
      "22400/60000\tLoss: 166.370\n",
      "25600/60000\tLoss: 166.242\n",
      "28800/60000\tLoss: 166.579\n",
      "32000/60000\tLoss: 167.505\n",
      "35200/60000\tLoss: 165.318\n",
      "38400/60000\tLoss: 167.179\n",
      "41600/60000\tLoss: 168.319\n",
      "44800/60000\tLoss: 167.114\n",
      "48000/60000\tLoss: 167.304\n",
      "51200/60000\tLoss: 166.948\n",
      "54400/60000\tLoss: 166.852\n",
      "57600/60000\tLoss: 166.904\n",
      "Epoch: 68 Average loss: 166.62\n",
      "0/60000\tLoss: 162.454\n",
      "3200/60000\tLoss: 166.375\n",
      "6400/60000\tLoss: 166.401\n",
      "9600/60000\tLoss: 164.370\n",
      "12800/60000\tLoss: 164.086\n",
      "16000/60000\tLoss: 165.050\n",
      "19200/60000\tLoss: 165.693\n",
      "22400/60000\tLoss: 167.699\n",
      "25600/60000\tLoss: 166.520\n",
      "28800/60000\tLoss: 164.154\n",
      "32000/60000\tLoss: 163.851\n",
      "35200/60000\tLoss: 166.006\n",
      "38400/60000\tLoss: 168.093\n",
      "41600/60000\tLoss: 167.427\n",
      "44800/60000\tLoss: 165.988\n",
      "48000/60000\tLoss: 166.445\n",
      "51200/60000\tLoss: 166.822\n",
      "54400/60000\tLoss: 166.181\n",
      "57600/60000\tLoss: 165.709\n",
      "Epoch: 69 Average loss: 166.02\n",
      "0/60000\tLoss: 167.924\n",
      "3200/60000\tLoss: 165.547\n",
      "6400/60000\tLoss: 166.322\n",
      "9600/60000\tLoss: 167.311\n",
      "12800/60000\tLoss: 165.520\n",
      "16000/60000\tLoss: 167.728\n",
      "19200/60000\tLoss: 167.450\n",
      "22400/60000\tLoss: 165.945\n",
      "25600/60000\tLoss: 166.609\n",
      "28800/60000\tLoss: 167.453\n",
      "32000/60000\tLoss: 166.273\n",
      "35200/60000\tLoss: 163.811\n",
      "38400/60000\tLoss: 166.001\n",
      "41600/60000\tLoss: 167.466\n",
      "44800/60000\tLoss: 165.240\n",
      "48000/60000\tLoss: 166.774\n",
      "51200/60000\tLoss: 164.592\n",
      "54400/60000\tLoss: 165.420\n",
      "57600/60000\tLoss: 167.483\n",
      "Epoch: 70 Average loss: 166.30\n",
      "0/60000\tLoss: 172.920\n",
      "3200/60000\tLoss: 166.456\n",
      "6400/60000\tLoss: 167.512\n",
      "9600/60000\tLoss: 167.350\n",
      "12800/60000\tLoss: 165.306\n",
      "16000/60000\tLoss: 166.674\n",
      "19200/60000\tLoss: 166.929\n",
      "22400/60000\tLoss: 166.860\n",
      "25600/60000\tLoss: 165.022\n",
      "28800/60000\tLoss: 164.702\n",
      "32000/60000\tLoss: 167.276\n",
      "35200/60000\tLoss: 163.789\n",
      "38400/60000\tLoss: 167.520\n",
      "41600/60000\tLoss: 165.900\n",
      "44800/60000\tLoss: 164.743\n",
      "48000/60000\tLoss: 167.361\n",
      "51200/60000\tLoss: 165.341\n",
      "54400/60000\tLoss: 167.364\n",
      "57600/60000\tLoss: 167.673\n",
      "Epoch: 71 Average loss: 166.39\n",
      "0/60000\tLoss: 163.122\n",
      "3200/60000\tLoss: 164.794\n",
      "6400/60000\tLoss: 165.004\n",
      "9600/60000\tLoss: 164.307\n",
      "12800/60000\tLoss: 166.684\n",
      "16000/60000\tLoss: 169.982\n",
      "19200/60000\tLoss: 167.274\n",
      "22400/60000\tLoss: 166.510\n",
      "25600/60000\tLoss: 164.735\n",
      "28800/60000\tLoss: 164.614\n",
      "32000/60000\tLoss: 168.392\n",
      "35200/60000\tLoss: 167.641\n",
      "38400/60000\tLoss: 165.842\n",
      "41600/60000\tLoss: 165.760\n",
      "44800/60000\tLoss: 165.178\n",
      "48000/60000\tLoss: 166.551\n",
      "51200/60000\tLoss: 164.491\n",
      "54400/60000\tLoss: 167.482\n",
      "57600/60000\tLoss: 167.879\n",
      "Epoch: 72 Average loss: 166.33\n",
      "0/60000\tLoss: 165.050\n",
      "3200/60000\tLoss: 165.692\n",
      "6400/60000\tLoss: 166.211\n",
      "9600/60000\tLoss: 164.752\n",
      "12800/60000\tLoss: 166.316\n",
      "16000/60000\tLoss: 165.785\n",
      "19200/60000\tLoss: 166.573\n",
      "22400/60000\tLoss: 166.009\n",
      "25600/60000\tLoss: 167.380\n",
      "28800/60000\tLoss: 168.004\n",
      "32000/60000\tLoss: 165.114\n",
      "35200/60000\tLoss: 164.629\n",
      "38400/60000\tLoss: 166.249\n",
      "41600/60000\tLoss: 165.597\n",
      "44800/60000\tLoss: 165.125\n",
      "48000/60000\tLoss: 165.812\n",
      "51200/60000\tLoss: 165.627\n",
      "54400/60000\tLoss: 167.322\n",
      "57600/60000\tLoss: 166.070\n",
      "Epoch: 73 Average loss: 166.16\n",
      "0/60000\tLoss: 175.926\n",
      "3200/60000\tLoss: 165.654\n",
      "6400/60000\tLoss: 165.496\n",
      "9600/60000\tLoss: 165.660\n",
      "12800/60000\tLoss: 168.187\n",
      "16000/60000\tLoss: 166.953\n",
      "19200/60000\tLoss: 165.083\n",
      "22400/60000\tLoss: 167.970\n",
      "25600/60000\tLoss: 165.819\n",
      "28800/60000\tLoss: 165.505\n",
      "32000/60000\tLoss: 166.359\n",
      "35200/60000\tLoss: 165.086\n",
      "38400/60000\tLoss: 164.476\n",
      "41600/60000\tLoss: 166.236\n",
      "44800/60000\tLoss: 164.224\n",
      "48000/60000\tLoss: 165.982\n",
      "51200/60000\tLoss: 167.482\n",
      "54400/60000\tLoss: 165.101\n",
      "57600/60000\tLoss: 167.970\n",
      "Epoch: 74 Average loss: 166.12\n",
      "0/60000\tLoss: 160.459\n",
      "3200/60000\tLoss: 165.606\n",
      "6400/60000\tLoss: 166.481\n",
      "9600/60000\tLoss: 166.300\n",
      "12800/60000\tLoss: 166.687\n",
      "16000/60000\tLoss: 164.972\n",
      "19200/60000\tLoss: 168.658\n",
      "22400/60000\tLoss: 165.913\n",
      "25600/60000\tLoss: 165.624\n",
      "28800/60000\tLoss: 165.056\n",
      "32000/60000\tLoss: 167.607\n",
      "35200/60000\tLoss: 164.690\n",
      "38400/60000\tLoss: 163.884\n",
      "41600/60000\tLoss: 169.592\n",
      "44800/60000\tLoss: 165.358\n",
      "48000/60000\tLoss: 166.630\n",
      "51200/60000\tLoss: 166.729\n",
      "54400/60000\tLoss: 166.782\n",
      "57600/60000\tLoss: 165.393\n",
      "Epoch: 75 Average loss: 166.26\n",
      "0/60000\tLoss: 162.394\n",
      "3200/60000\tLoss: 167.258\n",
      "6400/60000\tLoss: 165.598\n",
      "9600/60000\tLoss: 164.987\n",
      "12800/60000\tLoss: 166.584\n",
      "16000/60000\tLoss: 167.309\n",
      "19200/60000\tLoss: 165.525\n",
      "22400/60000\tLoss: 165.268\n",
      "25600/60000\tLoss: 165.304\n",
      "28800/60000\tLoss: 166.339\n",
      "32000/60000\tLoss: 165.761\n",
      "35200/60000\tLoss: 167.173\n",
      "38400/60000\tLoss: 166.158\n",
      "41600/60000\tLoss: 166.224\n",
      "44800/60000\tLoss: 165.742\n",
      "48000/60000\tLoss: 165.116\n",
      "51200/60000\tLoss: 165.097\n",
      "54400/60000\tLoss: 166.236\n",
      "57600/60000\tLoss: 166.783\n",
      "Epoch: 76 Average loss: 166.07\n",
      "0/60000\tLoss: 167.028\n",
      "3200/60000\tLoss: 165.786\n",
      "6400/60000\tLoss: 164.866\n",
      "9600/60000\tLoss: 164.507\n",
      "12800/60000\tLoss: 165.638\n",
      "16000/60000\tLoss: 165.879\n",
      "19200/60000\tLoss: 166.581\n",
      "22400/60000\tLoss: 166.603\n",
      "25600/60000\tLoss: 165.030\n",
      "28800/60000\tLoss: 166.118\n",
      "32000/60000\tLoss: 167.389\n",
      "35200/60000\tLoss: 165.269\n",
      "38400/60000\tLoss: 166.414\n",
      "41600/60000\tLoss: 165.304\n",
      "44800/60000\tLoss: 168.103\n",
      "48000/60000\tLoss: 166.039\n",
      "51200/60000\tLoss: 165.690\n",
      "54400/60000\tLoss: 165.076\n",
      "57600/60000\tLoss: 165.768\n",
      "Epoch: 77 Average loss: 166.01\n",
      "0/60000\tLoss: 165.520\n",
      "3200/60000\tLoss: 166.813\n",
      "6400/60000\tLoss: 165.079\n",
      "9600/60000\tLoss: 165.595\n",
      "12800/60000\tLoss: 165.290\n",
      "16000/60000\tLoss: 165.093\n",
      "19200/60000\tLoss: 164.728\n",
      "22400/60000\tLoss: 164.304\n",
      "25600/60000\tLoss: 166.357\n",
      "28800/60000\tLoss: 164.439\n",
      "32000/60000\tLoss: 163.743\n",
      "35200/60000\tLoss: 166.365\n",
      "38400/60000\tLoss: 165.465\n",
      "41600/60000\tLoss: 167.605\n",
      "44800/60000\tLoss: 167.390\n",
      "48000/60000\tLoss: 166.070\n",
      "51200/60000\tLoss: 165.754\n",
      "54400/60000\tLoss: 166.564\n",
      "57600/60000\tLoss: 166.795\n",
      "Epoch: 78 Average loss: 165.82\n",
      "0/60000\tLoss: 158.873\n",
      "3200/60000\tLoss: 166.242\n",
      "6400/60000\tLoss: 165.265\n",
      "9600/60000\tLoss: 164.160\n",
      "12800/60000\tLoss: 165.281\n",
      "16000/60000\tLoss: 167.339\n",
      "19200/60000\tLoss: 166.947\n",
      "22400/60000\tLoss: 165.855\n",
      "25600/60000\tLoss: 167.359\n",
      "28800/60000\tLoss: 164.059\n",
      "32000/60000\tLoss: 164.825\n",
      "35200/60000\tLoss: 165.127\n",
      "38400/60000\tLoss: 167.293\n",
      "41600/60000\tLoss: 166.057\n",
      "44800/60000\tLoss: 167.220\n",
      "48000/60000\tLoss: 166.843\n",
      "51200/60000\tLoss: 165.932\n",
      "54400/60000\tLoss: 165.361\n",
      "57600/60000\tLoss: 166.626\n",
      "Epoch: 79 Average loss: 166.11\n",
      "0/60000\tLoss: 179.386\n",
      "3200/60000\tLoss: 166.136\n",
      "6400/60000\tLoss: 166.445\n",
      "9600/60000\tLoss: 164.606\n",
      "12800/60000\tLoss: 166.433\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16000/60000\tLoss: 164.312\n",
      "19200/60000\tLoss: 165.210\n",
      "22400/60000\tLoss: 165.476\n",
      "25600/60000\tLoss: 164.158\n",
      "28800/60000\tLoss: 165.534\n",
      "32000/60000\tLoss: 168.138\n",
      "35200/60000\tLoss: 167.641\n",
      "38400/60000\tLoss: 165.886\n",
      "41600/60000\tLoss: 165.000\n",
      "44800/60000\tLoss: 167.287\n",
      "48000/60000\tLoss: 164.765\n",
      "51200/60000\tLoss: 165.333\n",
      "54400/60000\tLoss: 165.435\n",
      "57600/60000\tLoss: 164.121\n",
      "Epoch: 80 Average loss: 165.82\n",
      "0/60000\tLoss: 165.996\n",
      "3200/60000\tLoss: 164.343\n",
      "6400/60000\tLoss: 166.727\n",
      "9600/60000\tLoss: 165.240\n",
      "12800/60000\tLoss: 165.877\n",
      "16000/60000\tLoss: 166.618\n",
      "19200/60000\tLoss: 165.856\n",
      "22400/60000\tLoss: 165.266\n",
      "25600/60000\tLoss: 167.037\n",
      "28800/60000\tLoss: 166.887\n",
      "32000/60000\tLoss: 165.968\n",
      "35200/60000\tLoss: 164.026\n",
      "38400/60000\tLoss: 164.771\n",
      "41600/60000\tLoss: 166.702\n",
      "44800/60000\tLoss: 166.009\n",
      "48000/60000\tLoss: 166.808\n",
      "51200/60000\tLoss: 166.127\n",
      "54400/60000\tLoss: 163.893\n",
      "57600/60000\tLoss: 165.086\n",
      "Epoch: 81 Average loss: 165.81\n",
      "0/60000\tLoss: 161.617\n",
      "3200/60000\tLoss: 165.966\n",
      "6400/60000\tLoss: 166.391\n",
      "9600/60000\tLoss: 167.875\n",
      "12800/60000\tLoss: 167.732\n",
      "16000/60000\tLoss: 166.521\n",
      "19200/60000\tLoss: 163.978\n",
      "22400/60000\tLoss: 168.086\n",
      "25600/60000\tLoss: 162.828\n",
      "28800/60000\tLoss: 164.372\n",
      "32000/60000\tLoss: 168.052\n",
      "35200/60000\tLoss: 167.055\n",
      "38400/60000\tLoss: 163.748\n",
      "41600/60000\tLoss: 165.715\n",
      "44800/60000\tLoss: 164.492\n",
      "48000/60000\tLoss: 167.363\n",
      "51200/60000\tLoss: 166.328\n",
      "54400/60000\tLoss: 165.932\n",
      "57600/60000\tLoss: 165.405\n",
      "Epoch: 82 Average loss: 165.98\n",
      "0/60000\tLoss: 174.839\n",
      "3200/60000\tLoss: 164.812\n",
      "6400/60000\tLoss: 166.992\n",
      "9600/60000\tLoss: 167.312\n",
      "12800/60000\tLoss: 168.211\n",
      "16000/60000\tLoss: 165.298\n",
      "19200/60000\tLoss: 165.182\n",
      "22400/60000\tLoss: 165.242\n",
      "25600/60000\tLoss: 166.469\n",
      "28800/60000\tLoss: 167.228\n",
      "32000/60000\tLoss: 164.360\n",
      "35200/60000\tLoss: 165.233\n",
      "38400/60000\tLoss: 165.242\n",
      "41600/60000\tLoss: 167.787\n",
      "44800/60000\tLoss: 165.276\n",
      "48000/60000\tLoss: 163.825\n",
      "51200/60000\tLoss: 166.644\n",
      "54400/60000\tLoss: 164.652\n",
      "57600/60000\tLoss: 166.059\n",
      "Epoch: 83 Average loss: 165.94\n",
      "0/60000\tLoss: 159.754\n",
      "3200/60000\tLoss: 167.750\n",
      "6400/60000\tLoss: 164.617\n",
      "9600/60000\tLoss: 166.189\n",
      "12800/60000\tLoss: 166.189\n",
      "16000/60000\tLoss: 166.217\n",
      "19200/60000\tLoss: 164.589\n",
      "22400/60000\tLoss: 164.134\n",
      "25600/60000\tLoss: 165.381\n",
      "28800/60000\tLoss: 165.699\n",
      "32000/60000\tLoss: 162.915\n",
      "35200/60000\tLoss: 165.608\n",
      "38400/60000\tLoss: 164.976\n",
      "41600/60000\tLoss: 165.536\n",
      "44800/60000\tLoss: 167.094\n",
      "48000/60000\tLoss: 166.217\n",
      "51200/60000\tLoss: 165.994\n",
      "54400/60000\tLoss: 166.435\n",
      "57600/60000\tLoss: 165.109\n",
      "Epoch: 84 Average loss: 165.72\n",
      "0/60000\tLoss: 170.174\n",
      "3200/60000\tLoss: 164.121\n",
      "6400/60000\tLoss: 164.819\n",
      "9600/60000\tLoss: 163.771\n",
      "12800/60000\tLoss: 166.301\n",
      "16000/60000\tLoss: 164.371\n",
      "19200/60000\tLoss: 165.747\n",
      "22400/60000\tLoss: 164.852\n",
      "25600/60000\tLoss: 164.346\n",
      "28800/60000\tLoss: 169.990\n",
      "32000/60000\tLoss: 165.832\n",
      "35200/60000\tLoss: 164.594\n",
      "38400/60000\tLoss: 164.575\n",
      "41600/60000\tLoss: 164.113\n",
      "44800/60000\tLoss: 164.777\n",
      "48000/60000\tLoss: 165.990\n",
      "51200/60000\tLoss: 166.074\n",
      "54400/60000\tLoss: 164.775\n",
      "57600/60000\tLoss: 166.547\n",
      "Epoch: 85 Average loss: 165.38\n",
      "0/60000\tLoss: 160.233\n",
      "3200/60000\tLoss: 164.841\n",
      "6400/60000\tLoss: 163.006\n",
      "9600/60000\tLoss: 164.442\n",
      "12800/60000\tLoss: 165.591\n",
      "16000/60000\tLoss: 165.548\n",
      "19200/60000\tLoss: 163.548\n",
      "22400/60000\tLoss: 164.853\n",
      "25600/60000\tLoss: 166.393\n",
      "28800/60000\tLoss: 165.926\n",
      "32000/60000\tLoss: 165.059\n",
      "35200/60000\tLoss: 164.958\n",
      "38400/60000\tLoss: 164.153\n",
      "41600/60000\tLoss: 165.573\n",
      "44800/60000\tLoss: 165.034\n",
      "48000/60000\tLoss: 165.752\n",
      "51200/60000\tLoss: 166.065\n",
      "54400/60000\tLoss: 166.558\n",
      "57600/60000\tLoss: 165.593\n",
      "Epoch: 86 Average loss: 165.29\n",
      "0/60000\tLoss: 160.277\n",
      "3200/60000\tLoss: 166.281\n",
      "6400/60000\tLoss: 166.238\n",
      "9600/60000\tLoss: 164.349\n",
      "12800/60000\tLoss: 166.736\n",
      "16000/60000\tLoss: 164.028\n",
      "19200/60000\tLoss: 165.608\n",
      "22400/60000\tLoss: 165.113\n",
      "25600/60000\tLoss: 163.663\n",
      "28800/60000\tLoss: 164.703\n",
      "32000/60000\tLoss: 164.501\n",
      "35200/60000\tLoss: 164.951\n",
      "38400/60000\tLoss: 165.134\n",
      "41600/60000\tLoss: 165.176\n",
      "44800/60000\tLoss: 165.514\n",
      "48000/60000\tLoss: 165.831\n",
      "51200/60000\tLoss: 166.808\n",
      "54400/60000\tLoss: 165.117\n",
      "57600/60000\tLoss: 164.604\n",
      "Epoch: 87 Average loss: 165.34\n",
      "0/60000\tLoss: 161.204\n",
      "3200/60000\tLoss: 164.511\n",
      "6400/60000\tLoss: 164.671\n",
      "9600/60000\tLoss: 163.693\n",
      "12800/60000\tLoss: 165.057\n",
      "16000/60000\tLoss: 164.714\n",
      "19200/60000\tLoss: 163.829\n",
      "22400/60000\tLoss: 164.632\n",
      "25600/60000\tLoss: 163.704\n",
      "28800/60000\tLoss: 165.615\n",
      "32000/60000\tLoss: 165.857\n",
      "35200/60000\tLoss: 165.362\n",
      "38400/60000\tLoss: 165.700\n",
      "41600/60000\tLoss: 164.440\n",
      "44800/60000\tLoss: 165.818\n",
      "48000/60000\tLoss: 166.355\n",
      "51200/60000\tLoss: 165.493\n",
      "54400/60000\tLoss: 165.934\n",
      "57600/60000\tLoss: 167.626\n",
      "Epoch: 88 Average loss: 165.30\n",
      "0/60000\tLoss: 155.897\n",
      "3200/60000\tLoss: 165.947\n",
      "6400/60000\tLoss: 164.496\n",
      "9600/60000\tLoss: 163.472\n",
      "12800/60000\tLoss: 165.499\n",
      "16000/60000\tLoss: 167.744\n",
      "19200/60000\tLoss: 166.723\n",
      "22400/60000\tLoss: 165.169\n",
      "25600/60000\tLoss: 165.022\n",
      "28800/60000\tLoss: 165.533\n",
      "32000/60000\tLoss: 166.104\n",
      "35200/60000\tLoss: 164.648\n",
      "38400/60000\tLoss: 165.144\n",
      "41600/60000\tLoss: 165.291\n",
      "44800/60000\tLoss: 164.239\n",
      "48000/60000\tLoss: 165.923\n",
      "51200/60000\tLoss: 163.706\n",
      "54400/60000\tLoss: 165.178\n",
      "57600/60000\tLoss: 165.378\n",
      "Epoch: 89 Average loss: 165.39\n",
      "0/60000\tLoss: 164.835\n",
      "3200/60000\tLoss: 165.628\n",
      "6400/60000\tLoss: 165.058\n",
      "9600/60000\tLoss: 165.019\n",
      "12800/60000\tLoss: 164.602\n",
      "16000/60000\tLoss: 165.354\n",
      "19200/60000\tLoss: 164.922\n",
      "22400/60000\tLoss: 163.677\n",
      "25600/60000\tLoss: 166.007\n",
      "28800/60000\tLoss: 165.256\n",
      "32000/60000\tLoss: 165.714\n",
      "35200/60000\tLoss: 163.901\n",
      "38400/60000\tLoss: 162.430\n",
      "41600/60000\tLoss: 165.166\n",
      "44800/60000\tLoss: 165.394\n",
      "48000/60000\tLoss: 165.808\n",
      "51200/60000\tLoss: 164.385\n",
      "54400/60000\tLoss: 165.401\n",
      "57600/60000\tLoss: 166.218\n",
      "Epoch: 90 Average loss: 165.13\n",
      "0/60000\tLoss: 177.504\n",
      "3200/60000\tLoss: 164.211\n",
      "6400/60000\tLoss: 164.668\n",
      "9600/60000\tLoss: 165.670\n",
      "12800/60000\tLoss: 163.957\n",
      "16000/60000\tLoss: 164.907\n",
      "19200/60000\tLoss: 165.279\n",
      "22400/60000\tLoss: 166.709\n",
      "25600/60000\tLoss: 165.328\n",
      "28800/60000\tLoss: 165.273\n",
      "32000/60000\tLoss: 162.832\n",
      "35200/60000\tLoss: 164.740\n",
      "38400/60000\tLoss: 164.684\n",
      "41600/60000\tLoss: 167.106\n",
      "44800/60000\tLoss: 164.399\n",
      "48000/60000\tLoss: 164.900\n",
      "51200/60000\tLoss: 167.459\n",
      "54400/60000\tLoss: 165.302\n",
      "57600/60000\tLoss: 165.339\n",
      "Epoch: 91 Average loss: 165.25\n",
      "0/60000\tLoss: 162.653\n",
      "3200/60000\tLoss: 166.553\n",
      "6400/60000\tLoss: 165.071\n",
      "9600/60000\tLoss: 164.309\n",
      "12800/60000\tLoss: 164.183\n",
      "16000/60000\tLoss: 165.523\n",
      "19200/60000\tLoss: 162.921\n",
      "22400/60000\tLoss: 163.911\n",
      "25600/60000\tLoss: 166.278\n",
      "28800/60000\tLoss: 165.668\n",
      "32000/60000\tLoss: 165.291\n",
      "35200/60000\tLoss: 165.788\n",
      "38400/60000\tLoss: 166.823\n",
      "41600/60000\tLoss: 165.042\n",
      "44800/60000\tLoss: 163.868\n",
      "48000/60000\tLoss: 165.317\n",
      "51200/60000\tLoss: 164.487\n",
      "54400/60000\tLoss: 166.873\n",
      "57600/60000\tLoss: 165.326\n",
      "Epoch: 92 Average loss: 165.24\n",
      "0/60000\tLoss: 160.659\n",
      "3200/60000\tLoss: 165.368\n",
      "6400/60000\tLoss: 164.517\n",
      "9600/60000\tLoss: 164.835\n",
      "12800/60000\tLoss: 164.842\n",
      "16000/60000\tLoss: 165.196\n",
      "19200/60000\tLoss: 162.371\n",
      "22400/60000\tLoss: 165.254\n",
      "25600/60000\tLoss: 168.055\n",
      "28800/60000\tLoss: 166.330\n",
      "32000/60000\tLoss: 165.286\n",
      "35200/60000\tLoss: 165.479\n",
      "38400/60000\tLoss: 163.822\n",
      "41600/60000\tLoss: 164.327\n",
      "44800/60000\tLoss: 165.926\n",
      "48000/60000\tLoss: 165.517\n",
      "51200/60000\tLoss: 164.935\n",
      "54400/60000\tLoss: 164.542\n",
      "57600/60000\tLoss: 165.688\n",
      "Epoch: 93 Average loss: 165.28\n",
      "0/60000\tLoss: 166.361\n",
      "3200/60000\tLoss: 165.302\n",
      "6400/60000\tLoss: 164.812\n",
      "9600/60000\tLoss: 165.874\n",
      "12800/60000\tLoss: 163.677\n",
      "16000/60000\tLoss: 164.442\n",
      "19200/60000\tLoss: 163.604\n",
      "22400/60000\tLoss: 165.683\n",
      "25600/60000\tLoss: 167.026\n",
      "28800/60000\tLoss: 165.905\n",
      "32000/60000\tLoss: 166.689\n",
      "35200/60000\tLoss: 165.688\n",
      "38400/60000\tLoss: 165.875\n",
      "41600/60000\tLoss: 166.614\n",
      "44800/60000\tLoss: 164.909\n",
      "48000/60000\tLoss: 165.003\n",
      "51200/60000\tLoss: 165.781\n",
      "54400/60000\tLoss: 165.112\n",
      "57600/60000\tLoss: 164.584\n",
      "Epoch: 94 Average loss: 165.44\n",
      "0/60000\tLoss: 158.628\n",
      "3200/60000\tLoss: 165.656\n",
      "6400/60000\tLoss: 164.730\n",
      "9600/60000\tLoss: 163.448\n",
      "12800/60000\tLoss: 165.535\n",
      "16000/60000\tLoss: 164.732\n",
      "19200/60000\tLoss: 163.785\n",
      "22400/60000\tLoss: 163.083\n",
      "25600/60000\tLoss: 163.881\n",
      "28800/60000\tLoss: 166.659\n",
      "32000/60000\tLoss: 166.043\n",
      "35200/60000\tLoss: 166.308\n",
      "38400/60000\tLoss: 166.266\n",
      "41600/60000\tLoss: 164.561\n",
      "44800/60000\tLoss: 165.468\n",
      "48000/60000\tLoss: 164.268\n",
      "51200/60000\tLoss: 166.137\n",
      "54400/60000\tLoss: 166.828\n",
      "57600/60000\tLoss: 165.746\n",
      "Epoch: 95 Average loss: 165.22\n",
      "0/60000\tLoss: 159.754\n",
      "3200/60000\tLoss: 165.161\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6400/60000\tLoss: 166.877\n",
      "9600/60000\tLoss: 164.650\n",
      "12800/60000\tLoss: 164.442\n",
      "16000/60000\tLoss: 164.236\n",
      "19200/60000\tLoss: 162.311\n",
      "22400/60000\tLoss: 165.960\n",
      "25600/60000\tLoss: 165.467\n",
      "28800/60000\tLoss: 164.631\n",
      "32000/60000\tLoss: 164.763\n",
      "35200/60000\tLoss: 164.529\n",
      "38400/60000\tLoss: 165.262\n",
      "41600/60000\tLoss: 165.367\n",
      "44800/60000\tLoss: 163.365\n",
      "48000/60000\tLoss: 164.362\n",
      "51200/60000\tLoss: 164.226\n",
      "54400/60000\tLoss: 163.959\n",
      "57600/60000\tLoss: 163.862\n",
      "Epoch: 96 Average loss: 164.82\n",
      "0/60000\tLoss: 161.435\n",
      "3200/60000\tLoss: 165.443\n",
      "6400/60000\tLoss: 165.508\n",
      "9600/60000\tLoss: 163.420\n",
      "12800/60000\tLoss: 163.560\n",
      "16000/60000\tLoss: 164.288\n",
      "19200/60000\tLoss: 165.350\n",
      "22400/60000\tLoss: 163.662\n",
      "25600/60000\tLoss: 165.269\n",
      "28800/60000\tLoss: 162.965\n",
      "32000/60000\tLoss: 165.315\n",
      "35200/60000\tLoss: 164.721\n",
      "38400/60000\tLoss: 164.508\n",
      "41600/60000\tLoss: 167.583\n",
      "44800/60000\tLoss: 166.112\n",
      "48000/60000\tLoss: 164.136\n",
      "51200/60000\tLoss: 166.509\n",
      "54400/60000\tLoss: 165.757\n",
      "57600/60000\tLoss: 164.786\n",
      "Epoch: 97 Average loss: 165.03\n",
      "0/60000\tLoss: 164.575\n",
      "3200/60000\tLoss: 165.798\n",
      "6400/60000\tLoss: 165.821\n",
      "9600/60000\tLoss: 164.453\n",
      "12800/60000\tLoss: 165.046\n",
      "16000/60000\tLoss: 166.254\n",
      "19200/60000\tLoss: 163.625\n",
      "22400/60000\tLoss: 164.577\n",
      "25600/60000\tLoss: 165.134\n",
      "28800/60000\tLoss: 164.790\n",
      "32000/60000\tLoss: 163.106\n",
      "35200/60000\tLoss: 164.192\n",
      "38400/60000\tLoss: 164.782\n",
      "41600/60000\tLoss: 166.113\n",
      "44800/60000\tLoss: 165.887\n",
      "48000/60000\tLoss: 163.736\n",
      "51200/60000\tLoss: 162.660\n",
      "54400/60000\tLoss: 164.566\n",
      "57600/60000\tLoss: 165.743\n",
      "Epoch: 98 Average loss: 164.86\n",
      "0/60000\tLoss: 168.111\n",
      "3200/60000\tLoss: 165.012\n",
      "6400/60000\tLoss: 166.288\n",
      "9600/60000\tLoss: 165.511\n",
      "12800/60000\tLoss: 163.763\n",
      "16000/60000\tLoss: 164.701\n",
      "19200/60000\tLoss: 164.594\n",
      "22400/60000\tLoss: 164.353\n",
      "25600/60000\tLoss: 164.632\n",
      "28800/60000\tLoss: 164.083\n",
      "32000/60000\tLoss: 165.198\n",
      "35200/60000\tLoss: 164.899\n",
      "38400/60000\tLoss: 164.269\n",
      "41600/60000\tLoss: 166.718\n",
      "44800/60000\tLoss: 165.787\n",
      "48000/60000\tLoss: 165.198\n",
      "51200/60000\tLoss: 165.312\n",
      "54400/60000\tLoss: 164.853\n",
      "57600/60000\tLoss: 163.432\n",
      "Epoch: 99 Average loss: 165.00\n",
      "0/60000\tLoss: 166.360\n",
      "3200/60000\tLoss: 163.693\n",
      "6400/60000\tLoss: 164.042\n",
      "9600/60000\tLoss: 163.098\n",
      "12800/60000\tLoss: 164.749\n",
      "16000/60000\tLoss: 163.628\n",
      "19200/60000\tLoss: 164.170\n",
      "22400/60000\tLoss: 164.612\n",
      "25600/60000\tLoss: 164.236\n",
      "28800/60000\tLoss: 164.580\n",
      "32000/60000\tLoss: 165.421\n",
      "35200/60000\tLoss: 165.869\n",
      "38400/60000\tLoss: 165.578\n",
      "41600/60000\tLoss: 164.887\n",
      "44800/60000\tLoss: 163.730\n",
      "48000/60000\tLoss: 165.210\n",
      "51200/60000\tLoss: 164.491\n",
      "54400/60000\tLoss: 165.255\n",
      "57600/60000\tLoss: 166.022\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Lossy conversion from float32 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
      "Lossy conversion from float32 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
      "Lossy conversion from float32 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
      "Lossy conversion from float32 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
      "Lossy conversion from float32 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
      "Lossy conversion from float32 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
      "Lossy conversion from float32 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
      "Lossy conversion from float32 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
      "Lossy conversion from float32 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
      "Lossy conversion from float32 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
      "Lossy conversion from float32 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
      "Lossy conversion from float32 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
      "Lossy conversion from float32 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
      "Lossy conversion from float32 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
      "Lossy conversion from float32 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 100 Average loss: 164.65\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Lossy conversion from float32 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
      "Lossy conversion from float32 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
      "Lossy conversion from float32 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
      "Lossy conversion from float32 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
      "Lossy conversion from float32 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
      "Lossy conversion from float32 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
      "Lossy conversion from float32 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
      "Lossy conversion from float32 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
      "Lossy conversion from float32 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
      "Lossy conversion from float32 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
      "Lossy conversion from float32 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
      "Lossy conversion from float32 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
      "Lossy conversion from float32 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
      "Lossy conversion from float32 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
      "Lossy conversion from float32 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
      "Lossy conversion from float32 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
      "Lossy conversion from float32 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
      "Lossy conversion from float32 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
      "Lossy conversion from float32 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
      "Lossy conversion from float32 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
      "Lossy conversion from float32 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
      "Lossy conversion from float32 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
      "Lossy conversion from float32 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
      "Lossy conversion from float32 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
      "Lossy conversion from float32 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
      "Lossy conversion from float32 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
      "Lossy conversion from float32 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
      "Lossy conversion from float32 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
      "Lossy conversion from float32 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
      "Lossy conversion from float32 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
      "Lossy conversion from float32 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
      "Lossy conversion from float32 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
      "Lossy conversion from float32 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
      "Lossy conversion from float32 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
      "Lossy conversion from float32 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
      "Lossy conversion from float32 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
      "Lossy conversion from float32 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
      "Lossy conversion from float32 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
      "Lossy conversion from float32 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
      "Lossy conversion from float32 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
      "Lossy conversion from float32 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
      "Lossy conversion from float32 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
      "Lossy conversion from float32 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
      "Lossy conversion from float32 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
      "Lossy conversion from float32 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
      "Lossy conversion from float32 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
      "Lossy conversion from float32 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
      "Lossy conversion from float32 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
      "Lossy conversion from float32 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
      "Lossy conversion from float32 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
      "Lossy conversion from float32 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
      "Lossy conversion from float32 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
      "Lossy conversion from float32 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
      "Lossy conversion from float32 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
      "Lossy conversion from float32 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
      "Lossy conversion from float32 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
      "Lossy conversion from float32 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
      "Lossy conversion from float32 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
      "Lossy conversion from float32 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
      "Lossy conversion from float32 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
      "Lossy conversion from float32 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
      "Lossy conversion from float32 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
      "Lossy conversion from float32 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
      "Lossy conversion from float32 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
      "Lossy conversion from float32 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
      "Lossy conversion from float32 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
      "Lossy conversion from float32 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
      "Lossy conversion from float32 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
      "Lossy conversion from float32 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Lossy conversion from float32 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
      "Lossy conversion from float32 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
      "Lossy conversion from float32 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
      "Lossy conversion from float32 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
      "Lossy conversion from float32 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
      "Lossy conversion from float32 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
      "Lossy conversion from float32 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
      "Lossy conversion from float32 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
      "Lossy conversion from float32 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
      "Lossy conversion from float32 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
      "Lossy conversion from float32 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
      "Lossy conversion from float32 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
      "Lossy conversion from float32 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
      "Lossy conversion from float32 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
      "Lossy conversion from float32 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
      "Lossy conversion from float32 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n"
     ]
    }
   ],
   "source": [
    "# Train model for 10 epochs\n",
    "# Note this should really be a 100 epochs and trained on a GPU, but this is just to demo\n",
    "\n",
    "trainer.train(train_loader, epochs=100, save_training_gif=('./training.gif', viz))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = enumerate(test_loader)\n",
    "batch_idx, (example_data, example_targets) = next(examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([64, 1, 32, 32]), torch.Size([64]))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_data.shape, example_targets.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_img = example_data[0,:,:,:].unsqueeze(0).cuda()\n",
    "output, l_dist = model(example_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "l_sample = model.reparameterize(l_dist)\n",
    "new_l_sample = l_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 20])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l_sample.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_l_sample = l_sample\n",
    "new_l_sample = 5e-9*l_sample\n",
    "new_output = model.decode(new_l_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f137ffeb438>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(new_output[0][0].detach().cpu().numpy(), cmap='gray', interpolation='none')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f137fd7cfd0>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(output[0][0].detach().cpu().numpy(), cmap='gray', interpolation='none')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f137fb2e978>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(example_img[0][0].detach().cpu().numpy(), cmap='gray', interpolation='none')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0, device='cuda:0')"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.argmax(classifier(F.upsample(new_output, (28,28), mode='bilinear', align_corners=True)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check(i, j, alpha, beta):\n",
    "    im1 = example_data[i,:,:,:].unsqueeze(0).cuda()\n",
    "    im2 = example_data[j,:,:,:].unsqueeze(0).cuda()\n",
    "    out1, l_dist1 = model(im1)\n",
    "    out2, l_dist2 = model(im2)\n",
    "    l_sample1 = model.reparameterize(l_dist1)\n",
    "    l_sample2 = model.reparameterize(l_dist2)\n",
    "    l_sample = alpha*l_sample1 + beta*l_sample2\n",
    "    new_out = model.decode(l_sample)\n",
    "#     new_out1 = model.decode(l_sample1)\n",
    "#     new_out2 = model.decode(l_sample2)\n",
    "    plt.figure(figsize=(10,15))\n",
    "    plt.subplot(1,3,1)\n",
    "    plt.imshow(example_data[i][0].detach().cpu().numpy(), cmap='gray', interpolation='none')\n",
    "    plt.subplot(1,3,2)\n",
    "    plt.imshow(example_data[j][0].detach().cpu().numpy(), cmap='gray', interpolation='none')\n",
    "    plt.subplot(1,3,3)\n",
    "    plt.imshow(new_out[0][0].detach().cpu().numpy(), cmap='gray', interpolation='none')\n",
    "    print(torch.argmax(classifier(F.upsample(new_out, (28,28), mode='bilinear', align_corners=True))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(7, device='cuda:0')\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x1080 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "alpha=1\n",
    "beta = 1.2\n",
    "check(19,11, alpha, beta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import exp\n",
    "\n",
    "def gaussian(window_size, sigma):\n",
    "    gauss = torch.Tensor([exp(-(x - window_size//2)**2/float(2*sigma**2)) for x in range(window_size)])\n",
    "    return gauss/gauss.sum()\n",
    "\n",
    "def create_window(window_size, channel):\n",
    "    _1D_window = gaussian(window_size, 1.5).unsqueeze(1)\n",
    "    _2D_window = _1D_window.mm(_1D_window.t()).float().unsqueeze(0).unsqueeze(0)\n",
    "    window = torch.Tensor(_2D_window.expand(channel, 1, window_size, window_size).contiguous())\n",
    "    return window\n",
    "\n",
    "def _ssim(img1, img2, window, window_size, channel, size_average = True):\n",
    "    mu1 = F.conv2d(img1, window, padding = window_size//2, groups = channel)\n",
    "    mu2 = F.conv2d(img2, window, padding = window_size//2, groups = channel)\n",
    "\n",
    "    mu1_sq = mu1.pow(2)\n",
    "    mu2_sq = mu2.pow(2)\n",
    "    mu1_mu2 = mu1*mu2\n",
    "\n",
    "    sigma1_sq = F.conv2d(img1*img1, window, padding = window_size//2, groups = channel) - mu1_sq\n",
    "    sigma2_sq = F.conv2d(img2*img2, window, padding = window_size//2, groups = channel) - mu2_sq\n",
    "    sigma12 = F.conv2d(img1*img2, window, padding = window_size//2, groups = channel) - mu1_mu2\n",
    "\n",
    "    C1 = 0.01**2\n",
    "    C2 = 0.03**2\n",
    "\n",
    "    ssim_map = ((2*mu1_mu2 + C1)*(2*sigma12 + C2))/((mu1_sq + mu2_sq + C1)*(sigma1_sq + sigma2_sq + C2))\n",
    "\n",
    "    if size_average:\n",
    "        return ssim_map.mean()\n",
    "    else:\n",
    "        return ssim_map.mean(1).mean(1).mean(1)\n",
    "\n",
    "class SSIM(nn.Module):\n",
    "    def __init__(self, window_size = 11, size_average = True):\n",
    "        super(SSIM, self).__init__()\n",
    "        self.window_size = window_size\n",
    "        self.size_average = size_average\n",
    "        self.channel = 1\n",
    "        self.window = create_window(window_size, self.channel)\n",
    "\n",
    "    def forward(self, img1, img2):\n",
    "        (_, channel, _, _) = img1.size()\n",
    "\n",
    "        if channel == self.channel and self.window.data.type() == img1.data.type():\n",
    "            window = self.window\n",
    "        else:\n",
    "            window = create_window(self.window_size, channel)\n",
    "            \n",
    "            if img1.is_cuda:\n",
    "                window = window.cuda(img1.get_device())\n",
    "            window = window.type_as(img1)\n",
    "            \n",
    "            self.window = window\n",
    "            self.channel = channel\n",
    "\n",
    "\n",
    "        return _ssim(img1, img2, window, self.window_size, channel, self.size_average)\n",
    "\n",
    "def ssim(img1, img2, window_size = 11, size_average = True):\n",
    "    (_, channel, _, _) = img1.size()\n",
    "    window = create_window(window_size, channel)\n",
    "    \n",
    "    if img1.is_cuda:\n",
    "        window = window.cuda(img1.get_device())\n",
    "    window = window.type_as(img1)\n",
    "    \n",
    "    return _ssim(img1, img2, window, window_size, channel, size_average)\n",
    "\n",
    "ssim_loss = SSIM(window_size = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cuda = True\n",
    "device = torch.device('cuda' if use_cuda else 'cpu')\n",
    "# Constrained Translator\n",
    "class Translator(nn.Module):\n",
    "    def __init__(self, hidden_layers=[5, 10, 5], latent_dim=20):\n",
    "        super(Translator, self).__init__()\n",
    "        self.hidden_layers = hidden_layers\n",
    "        self.hidden_layers.insert(0, latent_dim)\n",
    "        self.hidden_layers.append(latent_dim)\n",
    "        self.layers = []\n",
    "        \n",
    "        for i in range(len(self.hidden_layers)-1):\n",
    "            self.layers.append(nn.Linear(self.hidden_layers[i], self.hidden_layers[i+1]))\n",
    "        \n",
    "        self.layers = nn.ModuleList(self.layers)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        l_sample = x\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        noised_sample = x\n",
    "        noised_sample = 1.2 * l_sample + 4.9e-7 * noised_sample\n",
    "        return x\n",
    "\n",
    "translator = Translator().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from skimage.metrics import structural_similarity as ssim\n",
    "\n",
    "def create_logits(target_label, pred, confidence=0.5, device=device):\n",
    "    logits = torch.zeros(pred.shape, dtype=torch.float64)\n",
    "    pred_labels = [int(element.item()) for element in torch.argmax(pred, dim=1)]\n",
    "#     print(\"length\", len(pred_labels))\n",
    "#     print(\"pred_lables:\",pred_labels)\n",
    "#     print(logits.shape)\n",
    "    logits[range(logits.shape[0]), pred_labels] = torch.DoubleTensor([1-confidence]*pred.shape[0])\n",
    "    logits[range(logits.shape[0]), [target_label]*pred.shape[0]] += torch.DoubleTensor([confidence]*pred.shape[0])\n",
    "    return logits.to(device)\n",
    "\n",
    "def structural(org_image, noised_image):\n",
    "    batch_size, channels, width, height = org_image.shape\n",
    "    loss1 = 0\n",
    "    for b_ in range(batch_size):\n",
    "        ch_loss = 0\n",
    "        for ch_ in range(channels):\n",
    "            ch_loss += 1-ssim(org_image[b_][ch_].detach().cpu().numpy(), noised_image[b_][ch_].detach().cpu().numpy())\n",
    "        loss1 += ch_loss/channels\n",
    "    return loss1\n",
    "            \n",
    "class T_Loss(nn.Module):\n",
    "    def __init__(self, decoder=model.decode, classifier=classifier):\n",
    "        super(T_Loss, self).__init__()\n",
    "        self.decoder = decoder\n",
    "        self.classifier = classifier\n",
    "    def forward(self, x, org_x, target_label):\n",
    "        \n",
    "        org_image = self.decoder(org_x)\n",
    "        noised_image = self.decoder(x)\n",
    "        batch_size, channels, width, height = org_image.shape\n",
    "        loss1 = ssim_loss(org_image, noised_image)\n",
    "#         print(type(loss1))\n",
    "#         print(loss1)\n",
    "#         loss1 = torch.Tensor(loss1 / batch_size).to(device)\n",
    "#         print(\"loss1:\", loss1)\n",
    "        preds = self.classifier(F.upsample(noised_image, (28,28), mode='bilinear', align_corners=True))\n",
    "#         print(\"preds:\",preds)\n",
    "        target = create_logits(target_label, preds)\n",
    "#         print(target)\n",
    "        loss2 = nn.BCELoss(reduction='sum')(preds, target.float())\n",
    "#         print(type(loss2))\n",
    "#         print(\"loss1:\",100*(1-loss1))\n",
    "#         print(\"loss2:\",loss2)\n",
    "        loss = 100*(1-loss1) + torch.sqrt(loss2)\n",
    "        \n",
    "        \n",
    "        out_labels = preds.argmax(dim=1, keepdim=True)\n",
    "#         print(out_labels)\n",
    "#         print(torch.empty(out_labels.shape).fill_(target_label))\n",
    "#         print(preds)\n",
    "#         correct = out_labels.eq(torch.Tensor([target_label]*out_labels.shape[0]).to(device)).sum()\n",
    "        correct = out_labels.eq(torch.empty(out_labels.shape).fill_(target_label).to(device)).sum()\n",
    "#         print(out_labels.shape)\n",
    "#         print(correct)\n",
    "#         print(torch.Tensor([target_label]*out_labels.shape[0]))\n",
    "#         print(out_labels)\n",
    "        return loss, correct.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "T_Loss(\n",
       "  (classifier): Classifier(\n",
       "    (conv1): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (dropout1): Dropout2d(p=0.25, inplace=False)\n",
       "    (dropout2): Dropout2d(p=0.5, inplace=False)\n",
       "    (fc1): Linear(in_features=9216, out_features=128, bias=True)\n",
       "    (fc2): Linear(in_features=128, out_features=10, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tloss = T_Loss().to(device)\n",
    "tloss.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VAE(\n",
       "  (img_to_features): Sequential(\n",
       "    (0): Conv2d(1, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (1): ReLU()\n",
       "    (2): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (3): ReLU()\n",
       "    (4): Conv2d(64, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (5): ReLU()\n",
       "  )\n",
       "  (features_to_hidden): Sequential(\n",
       "    (0): Linear(in_features=1024, out_features=256, bias=True)\n",
       "    (1): ReLU()\n",
       "  )\n",
       "  (fc_mean): Linear(in_features=256, out_features=10, bias=True)\n",
       "  (fc_log_var): Linear(in_features=256, out_features=10, bias=True)\n",
       "  (fc_alphas): ModuleList(\n",
       "    (0): Linear(in_features=256, out_features=10, bias=True)\n",
       "  )\n",
       "  (latent_to_features): Sequential(\n",
       "    (0): Linear(in_features=20, out_features=256, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=256, out_features=1024, bias=True)\n",
       "    (3): ReLU()\n",
       "  )\n",
       "  (features_to_img): Sequential(\n",
       "    (0): ConvTranspose2d(64, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (1): ReLU()\n",
       "    (2): ConvTranspose2d(32, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (3): ReLU()\n",
       "    (4): ConvTranspose2d(32, 1, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (5): Sigmoid()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "translator = Translator().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6af324cd9fb4f40868276f532a70080",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=20.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1\tLoss: 62.558034\tCorrect: 32708\n",
      "Train Epoch: 2\tLoss: 53.672535\tCorrect: 60000\n",
      "Train Epoch: 3\tLoss: 52.127518\tCorrect: 60000\n",
      "Train Epoch: 4\tLoss: 51.324536\tCorrect: 60000\n",
      "Train Epoch: 5\tLoss: 50.692440\tCorrect: 60000\n",
      "Train Epoch: 6\tLoss: 50.028312\tCorrect: 60000\n",
      "Train Epoch: 7\tLoss: 48.837131\tCorrect: 59850\n",
      "Train Epoch: 8\tLoss: 46.888863\tCorrect: 59998\n",
      "Train Epoch: 9\tLoss: 45.570488\tCorrect: 59976\n",
      "Train Epoch: 10\tLoss: 45.291929\tCorrect: 60000\n",
      "Train Epoch: 11\tLoss: 44.939372\tCorrect: 60000\n",
      "Train Epoch: 12\tLoss: 44.537722\tCorrect: 60000\n",
      "Train Epoch: 13\tLoss: 44.261224\tCorrect: 59998\n",
      "Train Epoch: 14\tLoss: 43.928076\tCorrect: 60000\n",
      "Train Epoch: 15\tLoss: 43.758833\tCorrect: 60000\n",
      "Train Epoch: 16\tLoss: 43.667817\tCorrect: 60000\n",
      "Train Epoch: 17\tLoss: 43.577310\tCorrect: 60000\n",
      "Train Epoch: 18\tLoss: 43.510614\tCorrect: 60000\n",
      "Train Epoch: 19\tLoss: 43.500440\tCorrect: 60000\n",
      "Train Epoch: 20\tLoss: 43.445530\tCorrect: 60000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "attack_log_interval = 1\n",
    "alt_target = 0\n",
    "translator.train()\n",
    "optimizer = optim.Adam(translator.parameters(), lr=1e-5)\n",
    "for epoch in tqdm(range(20)):\n",
    "    epoch_loss = 0\n",
    "    epoch_correct = 0\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data = torch.FloatTensor(data).to(device)\n",
    "        \n",
    "        _, l_dist = model(data)\n",
    "        l_sample = model.reparameterize(l_dist)\n",
    "        \n",
    "        noised_sample = translator(l_sample)\n",
    "        loss, correct = tloss(noised_sample, l_sample, alt_target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "#         print(correct)\n",
    "        epoch_loss += loss.item()\n",
    "        epoch_correct += correct\n",
    "        \n",
    "    if (epoch+1) % attack_log_interval == 0:\n",
    "        print('Train Epoch: {}\\tLoss: {:.6f}\\tCorrect: {}'.format(\n",
    "            epoch+1, epoch_loss/batch_idx, epoch_correct))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7696\n",
      "Accuracy:  76.96\n"
     ]
    }
   ],
   "source": [
    "total_correct = 0\n",
    "total_test = 0\n",
    "for batch_idx, (data, target) in enumerate(test_loader):\n",
    "    total_test += data.shape[0]\n",
    "    data = torch.FloatTensor(data).to(device)\n",
    "\n",
    "    _, l_dist = model(data)\n",
    "    l_sample = model.reparameterize(l_dist)\n",
    "\n",
    "    noised_sample = translator(l_sample)\n",
    "#     noised_sample = 1 * l_sample + 6e-7 * noised_sample #2\n",
    "#     noised_sample = 1 * l_sample + 4e-7 * noised_sample #0\n",
    "    noised_sample = 1.2 * l_sample + 0.5 * noised_sample\n",
    "    loss, correct = tloss(noised_sample, l_sample, alt_target)\n",
    "    total_correct += correct\n",
    "#     print(correct)\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "#         print(correct)\n",
    "#         epoch_loss += loss.item()\n",
    "    \n",
    "\n",
    "#     if (epoch+1) % attack_log_interval == 0:\n",
    "#         print('Train Epoch: \\tCorrect: {}'.format(\n",
    "#             epoch, epoch_correct))\n",
    "print(total_correct)\n",
    "print(\"Accuracy: \", 100*(total_correct/total_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(i):\n",
    "    _, l_dist = model(example_data[i].unsqueeze_(0).to(device))\n",
    "    l_sample = model.reparameterize(l_dist)\n",
    "    noised_sample = translator(l_sample)\n",
    "    print(noised_sample)\n",
    "    print(l_sample)\n",
    "#     noised_sample = 1 * ((l_sample - l_sample.min())/(l_sample.max() - l_sample.min())) + 1e-2 * ((noised_sample - noised_sample.min())/(noised_sample.max() - noised_sample.min()))\n",
    "    noised_sample = 0.1 * l_sample + 1e-9 * noised_sample\n",
    "#     noised_sample = l_sample + 1e-7 * noised_sample\n",
    "    final = model.decode(noised_sample)\n",
    "    pred_org = torch.argmax(classifier(F.upsample(example_data[i,:,:,:].unsqueeze(0).cuda(), (28,28), mode='bilinear', align_corners=True)))\n",
    "    pred = torch.argmax(classifier(F.upsample(final, (28,28), mode='bilinear', align_corners=True)))\n",
    "    print(\"Prediction: {}, {}\".format(pred_org.item(), pred.item()))\n",
    "    plt.figure(figsize=(10,10))\n",
    "    plt.subplot(1,2,1)\n",
    "    plt.imshow(example_data[i][0].detach().cpu().numpy(), cmap='gray', interpolation='none')\n",
    "    plt.subplot(1,2,2)\n",
    "    plt.imshow(final[0][0].detach().cpu().numpy(), cmap='gray', interpolation='none')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 3.1379, -2.4972,  0.4863,  0.0994,  2.1294, -1.7448, -1.0354, -1.6855,\n",
      "          1.6315, -2.0342, -0.0072,  1.6776,  1.0376,  2.2014,  1.8566,  1.1917,\n",
      "          1.0318,  1.3728, -0.0158, -0.1569]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward>)\n",
      "tensor([[ 0.0328,  0.0795, -0.0060, -0.0242,  0.3926, -1.2722,  0.2734,  0.0592,\n",
      "         -0.2005,  0.7012,  0.0000,  0.0000,  1.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000]], device='cuda:0',\n",
      "       grad_fn=<CatBackward>)\n",
      "Prediction: 3, 0\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "test(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def visualize_latent(i):\n",
    "    _, l_dist = model(example_data[i].unsqueeze_(0).to(device))\n",
    "    l_sample = model.reparameterize(l_dist)\n",
    "    pred = classifier(F.upsample(example_data[i,:,:,:].unsqueeze(0).cuda(), (28,28), mode='bilinear', align_corners=True))\n",
    "    bars = ('0', '1', '2', '3', '4', '5', '6', '7', '8', '9')\n",
    "    y_pos = np.arange(len(bars))\n",
    "    height = pred.detach().cpu().numpy().tolist()[0]\n",
    "    plt.figure(figsize=(10,5))    \n",
    "    plt.subplot(1,2,1)\n",
    "    plt.imshow(example_data[i][0].detach().cpu().numpy(), cmap='gray', interpolation='none')\n",
    "    plt.subplot(1,2,2)\n",
    "    plt.bar(y_pos, height)\n",
    "    plt.xticks(y_pos, bars)\n",
    "\n",
    "    print(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2.2024e-08, 4.9614e-08, 1.3808e-07, 8.1867e-06, 6.7062e-05, 1.5721e-06,\n",
      "         3.8224e-10, 2.9513e-04, 1.4086e-05, 9.9961e-01]], device='cuda:0',\n",
      "       grad_fn=<SoftmaxBackward>)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "visualize_latent(11)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
