{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train a JointVAE model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get MNIST data\n",
    "Build a simple JointVAE model on MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "VAE_TRAIN = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataloaders import get_mnist_dataloaders_attack, get_mnist_dataloaders\n",
    "# train_loader, test_loader = get_mnist_dataloaders_attack(2, 5, train_batch_size=64, test_batch_size=64, path_to_data='/home/data/bvaa')\n",
    "train_loader, test_loader = get_mnist_dataloaders(batch_size=64, path_to_data='/home/data/bvaa')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets, transforms\n",
    "all_transforms = transforms.Compose([\n",
    "        transforms.Resize(32),\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "train_data = datasets.MNIST('/home/data/bvaa/', train=True, download=True,\n",
    "                                transform=all_transforms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define latent distribution of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Latent distribution will be joint distribution of 10 gaussian normal distributions\n",
    "# and one 10 dimensional Gumbel Softmax distribution\n",
    "latent_spec = {'cont': 10,\n",
    "               'disc': [10]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import VAE\n",
    "\n",
    "model = VAE(latent_spec=latent_spec, img_size=(1, 32, 32), use_cuda=True).cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import optim\n",
    "\n",
    "# Build optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=5e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Classifier, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, 3, 1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, 1)\n",
    "        self.dropout1 = nn.Dropout2d(0.25)\n",
    "        self.dropout2 = nn.Dropout2d(0.5)\n",
    "        self.fc1 = nn.Linear(9216, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = self.dropout1(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.fc2(x)\n",
    "        output = F.softmax(x, dim=1)\n",
    "        return output\n",
    "\n",
    "classifier = Classifier().cuda()\n",
    "classifier.load_state_dict(torch.load('../VAE/models/mnist_cnn_non_log.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from training import Trainer\n",
    "\n",
    "# Define the capacities\n",
    "# Continuous channels\n",
    "cont_capacity = [0.0, 5.0, 25000, 30.0]  # Starting at a capacity of 0.0, increase this to 5.0\n",
    "                                         # over 25000 iterations with a gamma of 30.0\n",
    "# Discrete channels\n",
    "disc_capacity = [0.0, 5.0, 25000, 30.0]  # Starting at a capacity of 0.0, increase this to 5.0\n",
    "                                         # over 25000 iterations with a gamma of 30.0\n",
    "\n",
    "# Build a trainer\n",
    "trainer = Trainer(model, optimizer,\n",
    "                  cont_capacity=cont_capacity,\n",
    "                  disc_capacity=disc_capacity, use_cuda=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Build a visualizer which will be passed to trainer to visualize progress during training\n",
    "# from visualize import Visualizer\n",
    "\n",
    "# viz = Visualizer(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model for 10 epochs\n",
    "# Note this should really be a 100 epochs and trained on a GPU, but this is just to demo\n",
    "\n",
    "# trainer.train(train_loader, epochs=100, save_training_gif=('./training.gif', viz))\n",
    "if VAE_TRAIN:\n",
    "    trainer.train(train_loader, epochs=100)\n",
    "    torch.save(model.state_dict(), 'models/vae.pth')\n",
    "else:\n",
    "    model.load_state_dict(torch.load('models/vae.pth'))\n",
    "    model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "attack_digit = 2\n",
    "target_digit = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.where(example_targets==1)[0].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = enumerate(test_loader)\n",
    "batch_idx, (example_data, example_targets) = next(examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 1, 32, 32])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_64(data):\n",
    "    l_sample = None\n",
    "    for i in range(data.shape[0]):\n",
    "        output, l_dist_x = model(data[i,:,:,:].unsqueeze(0).cuda())\n",
    "        l_sample_x = model.reparameterize(l_dist_x)\n",
    "        if l_sample is None:\n",
    "            l_sample = l_sample_x\n",
    "        else:\n",
    "            l_sample += l_sample_x\n",
    "    l_sample = l_sample / data.shape[0]\n",
    "    new_output = model.decode(l_sample)\n",
    "    plt.imshow(new_output[0][0].detach().cpu().numpy(), cmap='gray', interpolation='none')\n",
    "    pred = classifier(F.upsample(new_output, (28,28), mode='bilinear', align_corners=True))\n",
    "    print(pred)\n",
    "    print(torch.argmax(pred))\n",
    "    return l_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch_avg(data):\n",
    "    l_sample = None\n",
    "    for i in range(data.shape[0]):\n",
    "        output, l_dist_x = model(data[i,:,:,:].unsqueeze(0).cuda())\n",
    "        l_sample_x = model.reparameterize(l_dist_x)\n",
    "        if l_sample is None:\n",
    "            l_sample = l_sample_x\n",
    "        else:\n",
    "            l_sample += l_sample_x\n",
    "    l_sample = l_sample / data.shape[0]\n",
    "#     new_output = model.decode(l_sample)\n",
    "#     plt.imshow(new_output[0][0].detach().cpu().numpy(), cmap='gray', interpolation='none')\n",
    "#     pred = classifier(F.upsample(new_output, (28,28), mode='bilinear', align_corners=True))\n",
    "#     print(pred)\n",
    "#     print(torch.argmax(pred))\n",
    "    return l_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_avg_mnist(img1, data):\n",
    "#     new_l_sample = None\n",
    "#     count = len(list_to_process)\n",
    "    output, l_dist_x = model(img1.cuda())\n",
    "    l_sample_x = model.reparameterize(l_dist_x)\n",
    "    l_sample_y = get_batch_avg(data)\n",
    "#     output, l_dist_y = model(img2.cuda())\n",
    "#     l_sample_y = model.reparameterize(l_dist_y)\n",
    "    \n",
    "    l_sample = 1*l_sample_x + 0.4*l_sample_y\n",
    "    \n",
    "    new_output = model.decode(l_sample)\n",
    "#     for i in list_to_process:\n",
    "#         example_img = example_data[i,:,:,:].unsqueeze(0).cuda()\n",
    "#         output, l_dist = model(example_img)\n",
    "#         l_sample = model.reparameterize(l_dist)\n",
    "#         if new_l_sample is None:\n",
    "#             new_l_sample = l_sample\n",
    "#         else:\n",
    "#             new_l_sample += l_sample\n",
    "#     new_l_sample = new_l_sample / count\n",
    "#     new_output = model.decode(new_l_sample)\n",
    "    plt.imshow(new_output[0][0].detach().cpu().numpy(), cmap='gray', interpolation='none')\n",
    "    pred = classifier(F.upsample(new_output, (28,28), mode='bilinear', align_corners=True))\n",
    "    print(pred)\n",
    "    print(torch.argmax(pred))\n",
    "    print(pred[0,2].item(), pred[0,5].item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "too many indices for tensor of dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-13a9ccb0a810>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mplot_avg_mnist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexample_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexample_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-15-3ae70194e763>\u001b[0m in \u001b[0;36mplot_avg_mnist\u001b[0;34m(img1, data)\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml_dist_x\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0ml_sample_x\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreparameterize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml_dist_x\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0ml_sample_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_batch_avg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;31m#     output, l_dist_y = model(img2.cuda())\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m#     l_sample_y = model.reparameterize(l_dist_y)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-14-5fe7e380e594>\u001b[0m in \u001b[0;36mget_batch_avg\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0ml_sample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m         \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml_dist_x\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m         \u001b[0ml_sample_x\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreparameterize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml_dist_x\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0ml_sample\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: too many indices for tensor of dimension 1"
     ]
    }
   ],
   "source": [
    "plot_avg_mnist(example_data[0][3].unsqueeze(0), example_data[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[9.7418e-04, 5.3215e-05, 1.9554e-06, 1.1177e-03, 1.6990e-04, 9.8696e-01,\n",
      "         1.1214e-04, 3.0288e-05, 1.3978e-03, 9.1855e-03]], device='cuda:0',\n",
      "       grad_fn=<SoftmaxBackward>)\n",
      "tensor(5, device='cuda:0')\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_64(example_data[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_img = example_data[12,:,:,:].unsqueeze(0).cuda()\n",
    "output, l_dist = model(example_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "l_sample = model.reparameterize(l_dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_l_sample = l_sample\n",
    "new_l_sample = 0.9 * l_sample\n",
    "new_output = model.decode(new_l_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.], device='cuda:0', grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "cos = nn.CosineSimilarity(dim=1, eps=1e-6)\n",
    "output_sim = cos(l_sample, new_l_sample)\n",
    "print(output_sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_mnist(list_to_process):\n",
    "    new_l_sample = None\n",
    "    count = len(list_to_process)\n",
    "    for i in list_to_process:\n",
    "        example_img = example_data[i,:,:,:].unsqueeze(0).cuda()\n",
    "        output, l_dist = model(example_img)\n",
    "        l_sample = model.reparameterize(l_dist)\n",
    "        if new_l_sample is None:\n",
    "            new_l_sample = l_sample\n",
    "        else:\n",
    "            new_l_sample += l_sample\n",
    "    new_l_sample = new_l_sample / count\n",
    "    new_output = model.decode(new_l_sample)\n",
    "    plt.imshow(new_output[0][0].detach().cpu().numpy(), cmap='gray', interpolation='none')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check(i, j, alpha, beta):\n",
    "    im1 = example_data[i,:,:,:].unsqueeze(0).cuda()\n",
    "    im2 = example_data[j,:,:,:].unsqueeze(0).cuda()\n",
    "    out1, l_dist1 = model(im1)\n",
    "    out2, l_dist2 = model(im2)\n",
    "    l_sample1 = model.reparameterize(l_dist1)\n",
    "    l_sample2 = model.reparameterize(l_dist2)\n",
    "    l_sample = alpha*l_sample1 + beta*l_sample2\n",
    "    new_out = model.decode(l_sample)\n",
    "#     new_out1 = model.decode(l_sample1)\n",
    "#     new_out2 = model.decode(l_sample2)\n",
    "    plt.figure(figsize=(10,15))\n",
    "    plt.subplot(1,3,1)\n",
    "    plt.imshow(example_data[i][0].detach().cpu().numpy(), cmap='gray', interpolation='none')\n",
    "    plt.subplot(1,3,2)\n",
    "    plt.imshow(example_data[j][0].detach().cpu().numpy(), cmap='gray', interpolation='none')\n",
    "    plt.subplot(1,3,3)\n",
    "    plt.imshow(new_out[0][0].detach().cpu().numpy(), cmap='gray', interpolation='none')\n",
    "    print(torch.argmax(classifier(F.upsample(new_out, (28,28), mode='bilinear', align_corners=True))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2, device='cuda:0')\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x1080 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "alpha=1\n",
    "beta = 1.2\n",
    "check(19,11, alpha, beta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Attack(nn.Module):\n",
    "#     def __init__(self, attack_digit=attack_digit, target_digit=target_digit, vae=model, classifier=classifier, avg_latent=l_sample_list):\n",
    "#         super(self, Attack).__init__()\n",
    "#         self.classifier = classifier\n",
    "#         self.classifier.eval()\n",
    "#         self.vae = vae\n",
    "#         self.vae.eval()\n",
    "#         self.avg_latent = avg_latent\n",
    "#         self.attack_digit = attack_digit\n",
    "#         self.target_digit = target_digit\n",
    "#         self.hidden_layers = hidden_layers\n",
    "#         self.hidden_layers.insert(0, latent_dim)\n",
    "#         self.hidden_layers.append(latent_dim)\n",
    "#         self.layers = []\n",
    "        \n",
    "#         for i in range(len(self.hidden_layers)-1):\n",
    "#             self.layers.append(nn.Linear(self.hidden_layers[i], self.hidden_layers[i+1]))\n",
    "        \n",
    "#         self.layers = nn.ModuleList(self.layers)\n",
    "        \n",
    "#     def forward(self, x, y):\n",
    "#         _, l_dist_x = self.vae(x)\n",
    "#         _, l_dist_y = self.vae(y)\n",
    "#         l_sample_x = self.vae.reparameterize(l_dist_x)\n",
    "#         l_sample_y = self.vae.reparameterize(l_dist_y)\n",
    "#         noised_sample = l_sample\n",
    "#         for layer in self.layers:\n",
    "#             noised_sample = layer(noised_sample)\n",
    "#         noised_images = self.vae.decoder(noised_sample)\n",
    "#         preds = self.classifier(F.upsample(noised_image, (28,28), mode='bilinear', align_corners=True))\n",
    "#         return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import exp\n",
    "\n",
    "def gaussian(window_size, sigma):\n",
    "    gauss = torch.Tensor([exp(-(x - window_size//2)**2/float(2*sigma**2)) for x in range(window_size)])\n",
    "    return gauss/gauss.sum()\n",
    "\n",
    "def create_window(window_size, channel):\n",
    "    _1D_window = gaussian(window_size, 1.5).unsqueeze(1)\n",
    "    _2D_window = _1D_window.mm(_1D_window.t()).float().unsqueeze(0).unsqueeze(0)\n",
    "    window = torch.Tensor(_2D_window.expand(channel, 1, window_size, window_size).contiguous())\n",
    "    return window\n",
    "\n",
    "def _ssim(img1, img2, window, window_size, channel, size_average = True):\n",
    "    mu1 = F.conv2d(img1, window, padding = window_size//2, groups = channel)\n",
    "    mu2 = F.conv2d(img2, window, padding = window_size//2, groups = channel)\n",
    "\n",
    "    mu1_sq = mu1.pow(2)\n",
    "    mu2_sq = mu2.pow(2)\n",
    "    mu1_mu2 = mu1*mu2\n",
    "\n",
    "    sigma1_sq = F.conv2d(img1*img1, window, padding = window_size//2, groups = channel) - mu1_sq\n",
    "    sigma2_sq = F.conv2d(img2*img2, window, padding = window_size//2, groups = channel) - mu2_sq\n",
    "    sigma12 = F.conv2d(img1*img2, window, padding = window_size//2, groups = channel) - mu1_mu2\n",
    "\n",
    "    C1 = 0.01**2\n",
    "    C2 = 0.03**2\n",
    "\n",
    "    ssim_map = ((2*mu1_mu2 + C1)*(2*sigma12 + C2))/((mu1_sq + mu2_sq + C1)*(sigma1_sq + sigma2_sq + C2))\n",
    "\n",
    "    if size_average:\n",
    "        return ssim_map.mean()\n",
    "    else:\n",
    "        return ssim_map.mean(1).mean(1).mean(1)\n",
    "\n",
    "class SSIM(nn.Module):\n",
    "    def __init__(self, window_size = 11, size_average = True):\n",
    "        super(SSIM, self).__init__()\n",
    "        self.window_size = window_size\n",
    "        self.size_average = size_average\n",
    "        self.channel = 1\n",
    "        self.window = create_window(window_size, self.channel)\n",
    "\n",
    "    def forward(self, img1, img2):\n",
    "        (_, channel, _, _) = img1.size()\n",
    "\n",
    "        if channel == self.channel and self.window.data.type() == img1.data.type():\n",
    "            window = self.window\n",
    "        else:\n",
    "            window = create_window(self.window_size, channel)\n",
    "            \n",
    "            if img1.is_cuda:\n",
    "                window = window.cuda(img1.get_device())\n",
    "            window = window.type_as(img1)\n",
    "            \n",
    "            self.window = window\n",
    "            self.channel = channel\n",
    "\n",
    "\n",
    "        return _ssim(img1, img2, window, self.window_size, channel, self.size_average)\n",
    "\n",
    "def ssim(img1, img2, window_size = 11, size_average = True):\n",
    "    (_, channel, _, _) = img1.size()\n",
    "    window = create_window(window_size, channel)\n",
    "    \n",
    "    if img1.is_cuda:\n",
    "        window = window.cuda(img1.get_device())\n",
    "    window = window.type_as(img1)\n",
    "    \n",
    "    return _ssim(img1, img2, window, window_size, channel, size_average)\n",
    "\n",
    "ssim_loss = SSIM(window_size = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from skimage.metrics import structural_similarity as ssim\n",
    "\n",
    "use_cuda = True\n",
    "device = torch.device('cuda' if use_cuda else 'cpu')\n",
    "\n",
    "avg_latent = torch.load('tensor/latent.pt')\n",
    "\n",
    "def create_logits(target_label, pred, confidence=0.4, device=device):\n",
    "    logits = torch.zeros(pred.shape, dtype=torch.float64)\n",
    "    pred_labels = [int(element.item()) for element in torch.argmax(pred, dim=1)]\n",
    "#     print(\"length\", len(pred_labels))\n",
    "#     print(\"pred_lables:\",pred_labels)\n",
    "#     print(logits.shape)\n",
    "    logits[range(logits.shape[0]), pred_labels] = torch.DoubleTensor([1-confidence]*pred.shape[0])\n",
    "    logits[range(logits.shape[0]), [target_label]*pred.shape[0]] += torch.DoubleTensor([confidence]*pred.shape[0])\n",
    "    return logits.to(device)\n",
    "\n",
    "def structural(org_image, noised_image):\n",
    "    batch_size, channels, width, height = org_image.shape\n",
    "    loss1 = 0\n",
    "    for b_ in range(batch_size):\n",
    "        ch_loss = 0\n",
    "        for ch_ in range(channels):\n",
    "            ch_loss += 1-ssim(org_image[b_][ch_].detach().cpu().numpy(), noised_image[b_][ch_].detach().cpu().numpy())\n",
    "        loss1 += ch_loss/channels\n",
    "    return loss1\n",
    "            \n",
    "class T_Loss(nn.Module):\n",
    "    def __init__(self, decoder=model.decode, classifier=classifier,latent_dim=20, avg_latent=avg_latent, classes=len(train_data.classes)):\n",
    "        super(T_Loss, self).__init__()\n",
    "        self.decoder = decoder\n",
    "        self.classifier = classifier\n",
    "        self.avg_latent = avg_latent\n",
    "        self.classes = classes\n",
    "        self.latent_dim = latent_dim\n",
    "    def forward(self, coff, org_x, target_label):\n",
    "        \n",
    "        org_image = self.decoder(org_x)\n",
    "        noise = 0.2*torch.transpose(torch.clamp(coff[:,None].cuda(), min=-0.5, max=0.2)*self.avg_latent.T, 1, 2).sum(1)\n",
    "#         noise = torch.clamp(noise, min=-0.2, max=0.2)\n",
    "#         final = torch.zeros(org_x.shape)\n",
    "#         for i in range(final.shape[0]):\n",
    "#             z = torch.zeros((1, self.latent_dim))\n",
    "#             count = 0\n",
    "#             for j in coff[i,:].tolist():\n",
    "#                 z += j * self.avg_latent[count].cpu()\n",
    "#                 count += 1\n",
    "#             final[i] = z\n",
    "        noised_sample = org_x +  noise.cuda()\n",
    "        noised_image = self.decoder(noised_sample)\n",
    "        batch_size, channels, width, height = org_image.shape\n",
    "        loss1 = ssim_loss(org_image, noised_image)\n",
    "#         print(type(loss1))\n",
    "#         print(loss1)\n",
    "#         loss1 = torch.Tensor(loss1 / batch_size).to(device)\n",
    "#         print(\"loss1:\", loss1)\n",
    "        preds = self.classifier(F.upsample(noised_image, (28,28), mode='bilinear', align_corners=True))\n",
    "\n",
    "        target = create_logits(target_label, preds)\n",
    "\n",
    "        loss2 = nn.BCELoss(reduction='sum')(preds, target.float())\n",
    "#         print(400*(1-loss1))\n",
    "#         print(loss2)\n",
    "        loss = 500*(1-loss1) + 0.8*loss2\n",
    "        \n",
    "        \n",
    "        out_labels = preds.argmax(dim=1, keepdim=True)\n",
    "#         print(out_labels)\n",
    "#         print(torch.empty(out_labels.shape).fill_(target_label))\n",
    "#         print(preds)\n",
    "#         correct = out_labels.eq(torch.Tensor([target_label]*out_labels.shape[0]).to(device)).sum()\n",
    "        correct = out_labels.eq(torch.empty(out_labels.shape).fill_(target_label).to(device)).sum()\n",
    "#         print(out_labels.shape)\n",
    "#         print(correct)\n",
    "#         print(torch.Tensor([target_label]*out_labels.shape[0]))\n",
    "#         print(out_labels)\n",
    "        return loss, correct.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cuda = True\n",
    "device = torch.device('cuda' if use_cuda else 'cpu')\n",
    "\n",
    "# Constrained Translator\n",
    "class Translator(nn.Module):\n",
    "    def __init__(self, hidden_layers=[5, 10, 10, len(train_data.classes)], latent_dim=20):\n",
    "        super(Translator, self).__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "        self.hidden_layers = hidden_layers\n",
    "        self.hidden_layers.insert(0, latent_dim)\n",
    "        self.layers = []\n",
    "        \n",
    "        for i in range(len(self.hidden_layers)-1):\n",
    "            self.layers.append(nn.Linear(self.hidden_layers[i], self.hidden_layers[i+1]))\n",
    "        \n",
    "        self.layers = nn.ModuleList(self.layers)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        l_sample = x\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        coff = x\n",
    "#         final = torch.zeros((x.shape[0], self.latent_dim))\n",
    "#         for i in range(final.shape[0]):\n",
    "#             z = torch.zeros((1, self.latent_dim))\n",
    "#             count = 0\n",
    "#             for j in coff[i,:].tolist():\n",
    "#                 z += j * self.avg_latent[count].cpu()\n",
    "#                 count += 1\n",
    "#             final[i] = z\n",
    "#         noised_sample = l_sample +  final.cuda()\n",
    "#         print(coff)\n",
    "        return coff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "T_Loss(\n",
       "  (classifier): Classifier(\n",
       "    (conv1): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (dropout1): Dropout2d(p=0.25, inplace=False)\n",
       "    (dropout2): Dropout2d(p=0.5, inplace=False)\n",
       "    (fc1): Linear(in_features=9216, out_features=128, bias=True)\n",
       "    (fc2): Linear(in_features=128, out_features=10, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translator = Translator().to(device)\n",
    "tloss = T_Loss().to(device)\n",
    "tloss.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78ebd3066d11407ca10b305da8826900",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1\tLoss: 394.212552\tCorrect: 5680\n",
      "Train Epoch: 2\tLoss: 340.687516\tCorrect: 8344\n",
      "Train Epoch: 3\tLoss: 320.950686\tCorrect: 10579\n",
      "Train Epoch: 4\tLoss: 313.951212\tCorrect: 10871\n",
      "Train Epoch: 5\tLoss: 309.471028\tCorrect: 10350\n",
      "Train Epoch: 6\tLoss: 305.163565\tCorrect: 10030\n",
      "Train Epoch: 7\tLoss: 301.066072\tCorrect: 10256\n",
      "Train Epoch: 8\tLoss: 297.227305\tCorrect: 10411\n",
      "Train Epoch: 9\tLoss: 294.291063\tCorrect: 10583\n",
      "Train Epoch: 10\tLoss: 292.825212\tCorrect: 11342\n",
      "Train Epoch: 11\tLoss: 291.401182\tCorrect: 12966\n",
      "Train Epoch: 12\tLoss: 289.819191\tCorrect: 13854\n",
      "Train Epoch: 13\tLoss: 288.778424\tCorrect: 13861\n",
      "Train Epoch: 14\tLoss: 288.865326\tCorrect: 13712\n",
      "Train Epoch: 15\tLoss: 288.220636\tCorrect: 14254\n",
      "Train Epoch: 16\tLoss: 286.777264\tCorrect: 15132\n",
      "Train Epoch: 17\tLoss: 285.690108\tCorrect: 15551\n",
      "Train Epoch: 18\tLoss: 284.411480\tCorrect: 15514\n",
      "Train Epoch: 19\tLoss: 282.668289\tCorrect: 15542\n",
      "Train Epoch: 20\tLoss: 280.740747\tCorrect: 15685\n",
      "Train Epoch: 21\tLoss: 278.706446\tCorrect: 15861\n",
      "Train Epoch: 22\tLoss: 277.319864\tCorrect: 15964\n",
      "Train Epoch: 23\tLoss: 276.693856\tCorrect: 16089\n",
      "Train Epoch: 24\tLoss: 276.553414\tCorrect: 16359\n",
      "Train Epoch: 25\tLoss: 276.619914\tCorrect: 16401\n",
      "Train Epoch: 26\tLoss: 276.426149\tCorrect: 16425\n",
      "Train Epoch: 27\tLoss: 275.690180\tCorrect: 16494\n",
      "Train Epoch: 28\tLoss: 274.784218\tCorrect: 16559\n",
      "Train Epoch: 29\tLoss: 274.035517\tCorrect: 16710\n",
      "Train Epoch: 30\tLoss: 273.430991\tCorrect: 16954\n",
      "Train Epoch: 31\tLoss: 272.638743\tCorrect: 17380\n",
      "Train Epoch: 32\tLoss: 271.707521\tCorrect: 18009\n",
      "Train Epoch: 33\tLoss: 270.900037\tCorrect: 18693\n",
      "Train Epoch: 34\tLoss: 270.424994\tCorrect: 19138\n",
      "Train Epoch: 35\tLoss: 270.174667\tCorrect: 19217\n",
      "Train Epoch: 36\tLoss: 270.160324\tCorrect: 19187\n",
      "Train Epoch: 37\tLoss: 270.079535\tCorrect: 19261\n",
      "Train Epoch: 38\tLoss: 269.718657\tCorrect: 19354\n",
      "Train Epoch: 39\tLoss: 269.819186\tCorrect: 19428\n",
      "Train Epoch: 40\tLoss: 269.671691\tCorrect: 19471\n",
      "Train Epoch: 41\tLoss: 269.171214\tCorrect: 19556\n",
      "Train Epoch: 42\tLoss: 268.357719\tCorrect: 19630\n",
      "Train Epoch: 43\tLoss: 267.377237\tCorrect: 19782\n",
      "Train Epoch: 44\tLoss: 266.608784\tCorrect: 19910\n",
      "Train Epoch: 45\tLoss: 266.148541\tCorrect: 19987\n",
      "Train Epoch: 46\tLoss: 265.898747\tCorrect: 19972\n",
      "Train Epoch: 47\tLoss: 265.662471\tCorrect: 19857\n",
      "Train Epoch: 48\tLoss: 265.503712\tCorrect: 19794\n",
      "Train Epoch: 49\tLoss: 265.372435\tCorrect: 19725\n",
      "Train Epoch: 50\tLoss: 265.230242\tCorrect: 19704\n",
      "Train Epoch: 51\tLoss: 265.011079\tCorrect: 19754\n",
      "Train Epoch: 52\tLoss: 264.804655\tCorrect: 19777\n",
      "Train Epoch: 53\tLoss: 264.563501\tCorrect: 19816\n",
      "Train Epoch: 54\tLoss: 264.321358\tCorrect: 19892\n",
      "Train Epoch: 55\tLoss: 264.067073\tCorrect: 19903\n",
      "Train Epoch: 56\tLoss: 263.824764\tCorrect: 19831\n",
      "Train Epoch: 57\tLoss: 263.614056\tCorrect: 19786\n",
      "Train Epoch: 58\tLoss: 263.426672\tCorrect: 19739\n",
      "Train Epoch: 59\tLoss: 263.249052\tCorrect: 19743\n",
      "Train Epoch: 60\tLoss: 263.065574\tCorrect: 19690\n",
      "Train Epoch: 61\tLoss: 262.889680\tCorrect: 19638\n",
      "Train Epoch: 62\tLoss: 262.646237\tCorrect: 19571\n",
      "Train Epoch: 63\tLoss: 262.493012\tCorrect: 19466\n",
      "Train Epoch: 64\tLoss: 262.466346\tCorrect: 19377\n",
      "Train Epoch: 65\tLoss: 262.425492\tCorrect: 19311\n",
      "Train Epoch: 66\tLoss: 262.382407\tCorrect: 19313\n",
      "Train Epoch: 67\tLoss: 262.345911\tCorrect: 19239\n",
      "Train Epoch: 68\tLoss: 262.240521\tCorrect: 19307\n",
      "Train Epoch: 69\tLoss: 262.063869\tCorrect: 19292\n",
      "Train Epoch: 70\tLoss: 262.016566\tCorrect: 19337\n",
      "Train Epoch: 71\tLoss: 262.078520\tCorrect: 19350\n",
      "Train Epoch: 72\tLoss: 262.164573\tCorrect: 19455\n",
      "Train Epoch: 73\tLoss: 262.275453\tCorrect: 19383\n",
      "Train Epoch: 74\tLoss: 262.045946\tCorrect: 19609\n",
      "Train Epoch: 75\tLoss: 262.323943\tCorrect: 19460\n",
      "Train Epoch: 76\tLoss: 262.793425\tCorrect: 19579\n",
      "Train Epoch: 77\tLoss: 262.939297\tCorrect: 19305\n",
      "Train Epoch: 78\tLoss: 263.605666\tCorrect: 19658\n",
      "Train Epoch: 79\tLoss: 264.284125\tCorrect: 19523\n",
      "Train Epoch: 80\tLoss: 264.539112\tCorrect: 19644\n",
      "Train Epoch: 81\tLoss: 263.713557\tCorrect: 19345\n",
      "Train Epoch: 82\tLoss: 263.528033\tCorrect: 19813\n",
      "Train Epoch: 83\tLoss: 263.758734\tCorrect: 19702\n",
      "Train Epoch: 84\tLoss: 264.139584\tCorrect: 19915\n",
      "Train Epoch: 85\tLoss: 262.668735\tCorrect: 19832\n",
      "Train Epoch: 86\tLoss: 262.780692\tCorrect: 20124\n",
      "Train Epoch: 87\tLoss: 261.906362\tCorrect: 20182\n",
      "Train Epoch: 88\tLoss: 261.143012\tCorrect: 20056\n",
      "Train Epoch: 89\tLoss: 261.108898\tCorrect: 20358\n",
      "Train Epoch: 90\tLoss: 261.381011\tCorrect: 20236\n",
      "Train Epoch: 91\tLoss: 261.419184\tCorrect: 20201\n",
      "Train Epoch: 92\tLoss: 261.856944\tCorrect: 20390\n",
      "Train Epoch: 93\tLoss: 262.437222\tCorrect: 20319\n",
      "Train Epoch: 94\tLoss: 262.544182\tCorrect: 20784\n",
      "Train Epoch: 95\tLoss: 262.466562\tCorrect: 20650\n",
      "Train Epoch: 96\tLoss: 261.694691\tCorrect: 20844\n",
      "Train Epoch: 97\tLoss: 262.335118\tCorrect: 20985\n",
      "Train Epoch: 98\tLoss: 262.769589\tCorrect: 21044\n",
      "Train Epoch: 99\tLoss: 262.860720\tCorrect: 21052\n",
      "Train Epoch: 100\tLoss: 263.691111\tCorrect: 20907\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "attack_log_interval = 1\n",
    "alt_target = 5\n",
    "translator.train()\n",
    "optimizer = optim.Adam(translator.parameters(), lr=1e-5)\n",
    "for epoch in tqdm(range(100)):\n",
    "    epoch_loss = 0\n",
    "    epoch_correct = 0\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "#         print(target)\n",
    "        data = torch.FloatTensor(data).to(device)\n",
    "        \n",
    "        _, l_dist = model(data)\n",
    "        l_sample = model.reparameterize(l_dist)\n",
    "        \n",
    "        coff = translator(l_sample)\n",
    "        loss, correct = tloss(coff, l_sample, alt_target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "#         print(correct)\n",
    "        epoch_loss += loss.item()\n",
    "    \n",
    "        epoch_correct += correct\n",
    "        \n",
    "    if (epoch+1) % attack_log_interval == 0:\n",
    "        print('Train Epoch: {}\\tLoss: {:.6f}\\tCorrect: {}'.format(\n",
    "            epoch+1, epoch_loss/batch_idx, epoch_correct))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3477\n",
      "Accuracy:  34.77\n"
     ]
    }
   ],
   "source": [
    "translator.eval()\n",
    "total_correct = 0\n",
    "total_test = 0\n",
    "for batch_idx, (data, target) in enumerate(test_loader):\n",
    "    total_test += data.shape[0]\n",
    "    data = torch.FloatTensor(data).to(device)\n",
    "\n",
    "    _, l_dist = model(data)\n",
    "    l_sample = model.reparameterize(l_dist)\n",
    "\n",
    "    coff = translator(l_sample)\n",
    "    loss, correct = tloss(coff, l_sample, alt_target)\n",
    "    total_correct += correct\n",
    "#     print(correct)\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "#         print(correct)\n",
    "#         epoch_loss += loss.item()\n",
    "    \n",
    "\n",
    "#     if (epoch+1) % attack_log_interval == 0:\n",
    "#         print('Train Epoch: \\tCorrect: {}'.format(\n",
    "#             epoch, epoch_correct))\n",
    "print(total_correct)\n",
    "print(\"Accuracy: \", 100*(total_correct/total_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(i):\n",
    "    _, l_dist = model(example_data[i].unsqueeze_(0).to(device))\n",
    "    l_sample = model.reparameterize(l_dist)\n",
    "    coff = translator(l_sample)\n",
    "    print(l_sample.shape)\n",
    "    print(torch.transpose(coff[:,None].cuda()*avg_latent.T, 1, 2).sum(1).shape)\n",
    "    noised_latent = torch.transpose(coff[:,None].cuda()*avg_latent.T, 1, 2).sum(1)\n",
    "    print(noised_latent.shape)\n",
    "#     print(noised_sample)\n",
    "#     print(l_sample)\n",
    "#     noised_sample = 1 * ((l_sample - l_sample.min())/(l_sample.max() - l_sample.min())) + 1e-2 * ((noised_sample - noised_sample.min())/(noised_sample.max() - noised_sample.min()))\n",
    "#     noised_sample = 1 * l_sample + 2e-2 * noised_sample\n",
    "#     noised_sample = l_sample + 1e-7 * noised_sample\n",
    "    final = model.decode(noised_latent)\n",
    "    pred_org = torch.argmax(classifier(F.upsample(example_data[i,:,:,:].unsqueeze(0).cuda(), (28,28), mode='bilinear', align_corners=True)))\n",
    "    pred = torch.argmax(classifier(F.upsample(final, (28,28), mode='bilinear', align_corners=True)))\n",
    "    print(\"Prediction: {}, {}\".format(pred_org.item(), pred.item()))\n",
    "    plt.figure(figsize=(10,10))\n",
    "    plt.subplot(1,2,1)\n",
    "    plt.imshow(example_data[i][0].detach().cpu().numpy(), cmap='gray', interpolation='none')\n",
    "    plt.subplot(1,2,2)\n",
    "    plt.imshow(final[0][0].detach().cpu().numpy(), cmap='gray', interpolation='none')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 20])\n",
      "torch.Size([1, 20])\n",
      "torch.Size([1, 20])\n",
      "Prediction: 6, 5\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "test(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets, transforms\n",
    "all_transforms = transforms.Compose([\n",
    "        transforms.Resize(32),\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "train_data = datasets.MNIST('/home/data/bvaa/', train=True, download=True,\n",
    "                                transform=all_transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_index = {}\n",
    "for i in range(len(train_data)):\n",
    "    index = train_data[i][1]\n",
    "    if index not in list_index.keys():\n",
    "        list_index[index] = [i]\n",
    "    else:\n",
    "        list_index[index].append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7fa8d07ca5a545aa83bd45d12fd2ab18",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=10.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "l_sample_list = {}\n",
    "def get_average_latent_space(list_index):\n",
    "    for i in tqdm(list_index.keys()):\n",
    "        for j in list_index[i]:\n",
    "            output, l_dist = model(train_data[j][0].unsqueeze(0).cuda())\n",
    "            l_sample_x = model.reparameterize(l_dist)\n",
    "            if i not in l_sample_list.keys():\n",
    "                l_sample_list[i] = [l_sample_x]\n",
    "            else:\n",
    "                l_sample_list[i].append(l_sample_x)\n",
    "        l_sample_list[i] = torch.mean(torch.stack(l_sample_list[i]), dim=0)\n",
    "        \n",
    "get_average_latent_space(list_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = []\n",
    "for i in l_sample_list.keys():\n",
    "    l.append(l_sample_list[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "l_tensor = torch.stack(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = l_tensor.squeeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 20])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(l, 'tensor/latent.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "e = torch.load('tensor/latent.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "coff = torch.randn((2,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 10])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coff.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = coff[:,None].cuda()*l.T\n",
    "r = torch.transpose(m,1,2) \n",
    "r.sum(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = torch.transpose(m,1,2)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 10, 20])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 20])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r.sum(1).shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
