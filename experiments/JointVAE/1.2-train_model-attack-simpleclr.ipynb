{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train a JointVAE model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get MNIST data\n",
    "Build a simple JointVAE model on MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "VAE_TRAIN = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataloaders import get_mnist_dataloaders_attack, get_mnist_dataloaders\n",
    "# train_loader, test_loader = get_mnist_dataloaders_attack(2, 5, train_batch_size=64, test_batch_size=64, path_to_data='/home/data/bvaa')\n",
    "train_loader, test_loader = get_mnist_dataloaders(batch_size=64, path_to_data='/home/data/bvaa')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets, transforms\n",
    "all_transforms = transforms.Compose([\n",
    "        transforms.Resize(32),\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "train_data = datasets.MNIST('/home/data/bvaa/', train=True, download=True,\n",
    "                                transform=all_transforms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define latent distribution of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Latent distribution will be joint distribution of 10 gaussian normal distributions\n",
    "# and one 10 dimensional Gumbel Softmax distribution\n",
    "latent_spec = {'cont': 10,\n",
    "               'disc': [10]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import VAE\n",
    "\n",
    "model = VAE(latent_spec=latent_spec, img_size=(1, 32, 32), use_cuda=True).cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import optim\n",
    "\n",
    "# Build optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=5e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Classifier, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, 3, 1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, 1)\n",
    "        self.dropout1 = nn.Dropout2d(0.25)\n",
    "        self.dropout2 = nn.Dropout2d(0.5)\n",
    "        self.fc1 = nn.Linear(9216, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = self.dropout1(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.fc2(x)\n",
    "        output = F.softmax(x, dim=1)\n",
    "        return output\n",
    "\n",
    "classifier = Classifier().cuda()\n",
    "classifier.load_state_dict(torch.load('../VAE/models/mnist_cnn_non_log.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from training import Trainer\n",
    "\n",
    "# Define the capacities\n",
    "# Continuous channels\n",
    "cont_capacity = [0.0, 5.0, 25000, 30.0]  # Starting at a capacity of 0.0, increase this to 5.0\n",
    "                                         # over 25000 iterations with a gamma of 30.0\n",
    "# Discrete channels\n",
    "disc_capacity = [0.0, 5.0, 25000, 30.0]  # Starting at a capacity of 0.0, increase this to 5.0\n",
    "                                         # over 25000 iterations with a gamma of 30.0\n",
    "\n",
    "# Build a trainer\n",
    "trainer = Trainer(model, optimizer,\n",
    "                  cont_capacity=cont_capacity,\n",
    "                  disc_capacity=disc_capacity, use_cuda=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Build a visualizer which will be passed to trainer to visualize progress during training\n",
    "# from visualize import Visualizer\n",
    "\n",
    "# viz = Visualizer(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model for 10 epochs\n",
    "# Note this should really be a 100 epochs and trained on a GPU, but this is just to demo\n",
    "\n",
    "# trainer.train(train_loader, epochs=100, save_training_gif=('./training.gif', viz))\n",
    "if VAE_TRAIN:\n",
    "    trainer.train(train_loader, epochs=100)\n",
    "    torch.save(model.state_dict(), 'models/vae.pth')\n",
    "else:\n",
    "    model.load_state_dict(torch.load('models/vae.pth'))\n",
    "    model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "attack_digit = 2\n",
    "target_digit = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.where(example_targets==1)[0].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = enumerate(test_loader)\n",
    "batch_idx, (example_data, example_targets) = next(examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 1, 32, 32])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_64(data):\n",
    "    l_sample = None\n",
    "    for i in range(data.shape[0]):\n",
    "        output, l_dist_x = model(data[i,:,:,:].unsqueeze(0).cuda())\n",
    "        l_sample_x = model.reparameterize(l_dist_x)\n",
    "        if l_sample is None:\n",
    "            l_sample = l_sample_x\n",
    "        else:\n",
    "            l_sample += l_sample_x\n",
    "    l_sample = l_sample / data.shape[0]\n",
    "    new_output = model.decode(l_sample)\n",
    "    plt.imshow(new_output[0][0].detach().cpu().numpy(), cmap='gray', interpolation='none')\n",
    "    pred = classifier(F.upsample(new_output, (28,28), mode='bilinear', align_corners=True))\n",
    "    print(pred)\n",
    "    print(torch.argmax(pred))\n",
    "    return l_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch_avg(data):\n",
    "    l_sample = None\n",
    "    for i in range(data.shape[0]):\n",
    "        output, l_dist_x = model(data[i,:,:,:].unsqueeze(0).cuda())\n",
    "        l_sample_x = model.reparameterize(l_dist_x)\n",
    "        if l_sample is None:\n",
    "            l_sample = l_sample_x\n",
    "        else:\n",
    "            l_sample += l_sample_x\n",
    "    l_sample = l_sample / data.shape[0]\n",
    "#     new_output = model.decode(l_sample)\n",
    "#     plt.imshow(new_output[0][0].detach().cpu().numpy(), cmap='gray', interpolation='none')\n",
    "#     pred = classifier(F.upsample(new_output, (28,28), mode='bilinear', align_corners=True))\n",
    "#     print(pred)\n",
    "#     print(torch.argmax(pred))\n",
    "    return l_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_avg_mnist(img1, data):\n",
    "#     new_l_sample = None\n",
    "#     count = len(list_to_process)\n",
    "    output, l_dist_x = model(img1.cuda())\n",
    "    l_sample_x = model.reparameterize(l_dist_x)\n",
    "    l_sample_y = get_batch_avg(data)\n",
    "#     output, l_dist_y = model(img2.cuda())\n",
    "#     l_sample_y = model.reparameterize(l_dist_y)\n",
    "    \n",
    "    l_sample = 1*l_sample_x + 0.4*l_sample_y\n",
    "    \n",
    "    new_output = model.decode(l_sample)\n",
    "#     for i in list_to_process:\n",
    "#         example_img = example_data[i,:,:,:].unsqueeze(0).cuda()\n",
    "#         output, l_dist = model(example_img)\n",
    "#         l_sample = model.reparameterize(l_dist)\n",
    "#         if new_l_sample is None:\n",
    "#             new_l_sample = l_sample\n",
    "#         else:\n",
    "#             new_l_sample += l_sample\n",
    "#     new_l_sample = new_l_sample / count\n",
    "#     new_output = model.decode(new_l_sample)\n",
    "    plt.imshow(new_output[0][0].detach().cpu().numpy(), cmap='gray', interpolation='none')\n",
    "    pred = classifier(F.upsample(new_output, (28,28), mode='bilinear', align_corners=True))\n",
    "    print(pred)\n",
    "    print(torch.argmax(pred))\n",
    "    print(pred[0,2].item(), pred[0,5].item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "too many indices for tensor of dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-13a9ccb0a810>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mplot_avg_mnist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexample_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexample_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-15-3ae70194e763>\u001b[0m in \u001b[0;36mplot_avg_mnist\u001b[0;34m(img1, data)\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml_dist_x\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0ml_sample_x\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreparameterize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml_dist_x\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0ml_sample_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_batch_avg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;31m#     output, l_dist_y = model(img2.cuda())\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m#     l_sample_y = model.reparameterize(l_dist_y)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-14-5fe7e380e594>\u001b[0m in \u001b[0;36mget_batch_avg\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0ml_sample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m         \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml_dist_x\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m         \u001b[0ml_sample_x\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreparameterize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml_dist_x\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0ml_sample\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: too many indices for tensor of dimension 1"
     ]
    }
   ],
   "source": [
    "plot_avg_mnist(example_data[0][3].unsqueeze(0), example_data[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[9.7418e-04, 5.3215e-05, 1.9554e-06, 1.1177e-03, 1.6990e-04, 9.8696e-01,\n",
      "         1.1214e-04, 3.0288e-05, 1.3978e-03, 9.1855e-03]], device='cuda:0',\n",
      "       grad_fn=<SoftmaxBackward>)\n",
      "tensor(5, device='cuda:0')\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_64(example_data[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_img = example_data[12,:,:,:].unsqueeze(0).cuda()\n",
    "output, l_dist = model(example_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "l_sample = model.reparameterize(l_dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_l_sample = l_sample\n",
    "new_l_sample = 0.9 * l_sample\n",
    "new_output = model.decode(new_l_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.], device='cuda:0', grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "cos = nn.CosineSimilarity(dim=1, eps=1e-6)\n",
    "output_sim = cos(l_sample, new_l_sample)\n",
    "print(output_sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_mnist(list_to_process):\n",
    "    new_l_sample = None\n",
    "    count = len(list_to_process)\n",
    "    for i in list_to_process:\n",
    "        example_img = example_data[i,:,:,:].unsqueeze(0).cuda()\n",
    "        output, l_dist = model(example_img)\n",
    "        l_sample = model.reparameterize(l_dist)\n",
    "        if new_l_sample is None:\n",
    "            new_l_sample = l_sample\n",
    "        else:\n",
    "            new_l_sample += l_sample\n",
    "    new_l_sample = new_l_sample / count\n",
    "    new_output = model.decode(new_l_sample)\n",
    "    plt.imshow(new_output[0][0].detach().cpu().numpy(), cmap='gray', interpolation='none')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check(i, j, alpha, beta):\n",
    "    im1 = example_data[i,:,:,:].unsqueeze(0).cuda()\n",
    "    im2 = example_data[j,:,:,:].unsqueeze(0).cuda()\n",
    "    out1, l_dist1 = model(im1)\n",
    "    out2, l_dist2 = model(im2)\n",
    "    l_sample1 = model.reparameterize(l_dist1)\n",
    "    l_sample2 = model.reparameterize(l_dist2)\n",
    "    l_sample = alpha*l_sample1 + beta*l_sample2\n",
    "    new_out = model.decode(l_sample)\n",
    "#     new_out1 = model.decode(l_sample1)\n",
    "#     new_out2 = model.decode(l_sample2)\n",
    "    plt.figure(figsize=(10,15))\n",
    "    plt.subplot(1,3,1)\n",
    "    plt.imshow(example_data[i][0].detach().cpu().numpy(), cmap='gray', interpolation='none')\n",
    "    plt.subplot(1,3,2)\n",
    "    plt.imshow(example_data[j][0].detach().cpu().numpy(), cmap='gray', interpolation='none')\n",
    "    plt.subplot(1,3,3)\n",
    "    plt.imshow(new_out[0][0].detach().cpu().numpy(), cmap='gray', interpolation='none')\n",
    "    print(torch.argmax(classifier(F.upsample(new_out, (28,28), mode='bilinear', align_corners=True))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2, device='cuda:0')\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAlAAAADECAYAAABQih85AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAAfC0lEQVR4nO3de4zV9bnv8c8j94tcFRgQuchIUVSIiAiNt1oFe6i2tdYLxrT0sJtuW9vYpqanPd27Jk1N2m1sWnfCVoMmPW5vNSptKlSxFLVjUVBAUC4iUhBUhAFFLsP3/DHLbBbPs5z1m1kzsy7vV0Jm5sNa8/v+hvWs9WXN8/3+LKUkAAAAFO+4zh4AAABApWECBQAAkBETKAAAgIyYQAEAAGTEBAoAACAjJlAAAAAZtWkCZWYzzex1M9tgZreWalBApaImgHzUBKqVtXYfKDPrIukNSZ+XtFXSPyRdm1J67VPuw6ZTKCspJSvV96ImUA2oCSBfoZpoyztQUyVtSCltSikdlPTfkq5ow/cDKh01AeSjJlC12jKBGiHp7aO+3prL8pjZPDNbbmbL23AsoBJQE0A+agJVq2sb7hu9peXeek0pzZc0X+KtWVQ9agLIR02garXlHaitkkYe9fVJkra1bThARaMmgHzUBKpWWyZQ/5BUb2ZjzKy7pGskPVGaYQEViZoA8lETqFqt/hVeSumwmd0k6SlJXSTdm1JaU7KRARWGmgDyUROoZq3exqBVB+N32ygzpVyy3RrUBMoNNQHka49tDAAAAGpSW1bhAQCATmbm3yA57rj4/ZHot07R/aPbFfsbq478zVZn4h0oAACAjJhAAQAAZMQECgAAICMmUAAAABnRRF6Enj17hvm0adOKyoYNG1bS8RRq0GtoaHDZqlWrXLZlyxaX7d27t+0DAwC0StTI3b17d5edcMIJLhs5cqTL6urqwuNMnTrVZd26dXNZY2Ojyw4dOuSy9957z2WvvvpqeOzVq1e7bP/+/S47cuRIeP9ywztQAAAAGTGBAgAAyIgJFAAAQEZMoAAAADJiAgUAAJARq/CKMGLEiDD/2te+5rLLLrvMZfv27XNZtHJhzJgxLotWPUS3k+IVgNGqh5UrV7rsueeec9lrr73msoMHD4bHBgAUJ1pxF632jl57LrroIpdFz/1nnXVWeOxRo0a5LLrsS1NTk8ui16Pdu3e7bOPGjeGx7777bpdFrz27du1yWTleHoZ3oAAAADJiAgUAAJAREygAAICMmEABAABk1KYmcjPbLGmvpCZJh1NKU0oxqHJTaEv8iRMnumz79u0ue+CBB1y2fv16l51zzjkuO3DggMvOP//8cDxRw+FVV13lsqjRfdGiRS6Lxv3iiy+Gx+ZSMM1qpSaAYtV6TRR7iZaoufvSSy91WfT8HV3KpdAlxKJjR03kXbv66UF0Ln369HHZkCFDwmP36NHDZVET+gsvvOCyqIG9s5ViFd5FKSW/pAyoXdQEkI+aQNXhV3gAAAAZtXUClSQtMrOXzGxedAMzm2dmy81seRuPBVQCagLIR02gKrX1V3gzUkrbzGyIpMVmti6ltPToG6SU5kuaL0lmVn47YQGlRU0A+agJVKU2TaBSSttyH3ea2WOSpkpa+un3qjz79+8P83Xr1rns+eefd9lDDz3ksg8//NBlTz31lMui5r6ouVuSpkzxvZnXX3+9y6ZPn+6yOXPmuGz8+PEu+8lPfhIee9myZS6LdrKtdrVSE+0heqz37t3bZcOHDy/qvp3tyJEjLouuQLBnzx6XVVPt1HpNRI3XUTP1oEGDXHbiiSe6LGqmjh5XUXO2JL399ttFjXHw4MEuO/nkk10W1ejAgQPDY0+aNMllM2fOdNmKFStcVo5N5K1+1jGzPmZ2/CefS7pUkr9uCFAjqAkgHzWBataWd6CGSnosN3PtKun/pZT+XJJRAZWJmgDyUROoWq2eQKWUNkmKr1YI1CBqAshHTaCalV/jAAAAQJkrxUaaVW/NmjVhHjVUR83hUVasqBF169at4W2jPNrRdd48v5L461//ustOP/10l337298Oj7127VqXvfvuuy5LiQU2iPXt29dlM2bMcNltt91W1H2luDm2o3z88ccu++1vf+uyRx55xGUffPBBu4wJHS96DHbr1s1l0cKB999/32ULFy502c6dO1321ltvheOJvmf0OtOvXz+XTZ482WU333yzy6Lmdyne3TzayTy6XTniHSgAAICMmEABAABkxAQKAAAgIyZQAAAAGVVGp1YnO3DgQJhXQpN0NMa77rrLZVET4Y9+9COXnXfeeeFxrr76apfdfffdLosaawEpblD91a9+5bJTTz216O/ZmU3k0c7J1157rcueeeYZl9FEXj2i59ZoYdGGDRtctmXLFpdFV8aIHmuFXrei8USiZvPoOKtX+31RowVIktSzZ0+X9e/f32WdWbdZ8A4UAABARkygAAAAMmICBQAAkBETKAAAgIxoIi9CocbwcmsYj0QNg1GD6rp161y2adMml40cOTI8TtQ0eNxxzM9RvD179rgselxGTeT79u0Lv+eiRYtcFjW9fvTRRy6rr6932QUXXOCy8ePHh8eOai9qGG9sbAzvj+oQvU5Ei2kKNX239hhtVWyz+cCBA13Wo0eP8LbReb/yyisui+qxHPEKBwAAkBETKAAAgIyYQAEAAGTEBAoAACCjFidQZnavme00s9VHZYPMbLGZrc999F1kQJWiJoB81ARqUTGr8BZI+q2k+4/KbpX0dErpl2Z2a+5rf90PlKVodcWaNWtctmTJEpfNmzcv/J4TJkxwWZcuXVoxuoqwQNREyW3cuNFld955p8vefPNNl23evDn8ntEqvui20SUqrrvuOpcNGzbMZYVWQEXfM1pxVCWXN1ogaqJNym1Vd/T8Ha1MPeOMM1xWaAV2tLJ78eLFLotqpxy1+A5USmmppF3HxFdIui/3+X2SrizxuICyRU0A+agJ1KLW9kANTSltl6TcxyGlGxJQkagJIB81garW7htpmtk8SfHvfYAaRE0A+agJVKLWvgO1w8zqJCn3cWehG6aU5qeUpqSUprTyWEAloCaAfNQEqlpr34F6QtKNkn6Z+/h4yUaETvHOO++47I033nBZocbwfv36uczM2j6wykFNtNHevXtd1tDQ4LIdO3a4LLo8kRQ3aA8Z4n+TdN5557ls+vTpLuvfv3/Rx378cf8QePXVV122f//+8P5VgJqoENFz9dChQ132zW9+02UjRoxw2cGDB8PjPPnkky6LFnUUexmZzlbMNgYPSHpB0ngz22pmc9VcEJ83s/WSPp/7GqgJ1ASQj5pALWrxHaiU0rUF/upzJR4LUBGoCSAfNYFaxE7kAAAAGTGBAgAAyKjdtzFAZYh2ju3alYcHOteBAwdctmHDBpeNGTMmvP+MGTNcFjWHT5s2zWWnnXaayxobG13297//PTz2XXfd5bKtW7e6rKmpKbw/0FF69+7tsq9+9asuO//8813WrVs3l0U7jkvSww8/7LJoEUW57cpeCO9AAQAAZMQECgAAICMmUAAAABkxgQIAAMiILuEOEO3y2r17d5f16tXLZVEz3YcffhgeJ2pGLbYZb/jw4S77zGc+47JDhw6F94+aYytlN1mUh+jxHz0ux40b57KLLroo/J6zZs1y2dixY10WNdG+++67Llu6dKnLHnroofDYL730kssqpTkW1atHjx4uixZRXHfddS6LrjgRLay47777wmNv2bLFZZW8iIJ3oAAAADJiAgUAAJAREygAAICMmEABAABkRBN5iUUN44MHD3bZqaee6rJRo0a5LGrEXrNmTXjsqEFv3759RX3PqGH8wgsvdNnevXvDYy9ZssRlhw8fDm+L6hU9/qOm1T59+risvr7eZV/+8pddFu2QXFdXF46nS5cuLtu1a5fLtm/f7rK//vWvLluwYIHLVq5cGR6bhnG0h6jGoqxnz57h/aPd+X/4wx+6LNrdP1pE1NDQ4LIHHnggPHbUcF7JeAcKAAAgIyZQAAAAGTGBAgAAyIgJFAAAQEYtNpGb2b2S/peknSmlibns3yT9b0mfbNX745TSn9prkJ0tatCT4t2Lox3GZ8+e7bIf/OAHLosay6OG72effTYcz29+8xuXvfDCCy7bvXu3ywYMGOCyIUOGuOzjjz8Ojx3l1dpES01Ixx0X/9+rb9++Lose19OnT3fZZZdd5rKZM2e6LKrHQgsWoobxu+++22WLFi1y2euvv+6yd955JzxOraMmCiu26bvYK1Ycf/zxLhs0aJDLot3FJem73/2uy6Ld+SPRAqaf/vSnLosWNEnVd3WKYt6BWiDJP4tJd6SUJuX+1FxRoKYtEDUBHG2BqAnUmBYnUCmlpZL8f+OAGkVNAPmoCdSitvRA3WRmr5rZvWY2sNCNzGyemS03s+VtOBZQCagJIB81garV2gnUf0o6RdIkSdsl/brQDVNK81NKU1JKU1p5LKASUBNAPmoCVa1VO5GnlHZ88rmZ/ZekhSUbURmKmvYk6dZbb3XZrFmzXLZx40aXPfLIIy6Ldog988wzXRbtEC5Jw4YNc9nPf/5zly1btsxl0c7QURP422+/HR774YcfdtnBgwfD21ajWquJ4cOHh/mVV17psuuvv95lEydOdFnUMBs11ka7IS9dujQczz333OOyaIfxd99912XV1vDa0SqxJqLHW7Rgolu3bi7r2jV+OY1uGy1AKvaKFePGjXPZ+PHjXXbuueeG44mueBHV1Nq1a132i1/8wmVRY3mt1E6r3oEys6Ovm/AlSatLMxygMlETQD5qAtWumG0MHpB0oaQTzGyrpJ9JutDMJklKkjZL+pd2HCNQVqgJIB81gVrU4gQqpXRtEPv3xYEaQU0A+agJ1CJ2IgcAAMiICRQAAEBG1pGX2zCzsrq2R7Q6IlpFd80114T3j1aZPf/880Vl0aqf/v37u2z06NEuu+mmm8LxnHfeeS6LLj0RrQqsq6tzWa9evVx2xx13hMe+//77XdbU1BTetpyklOLr9HSQzqyJ6LIr0cqdaNXn1KlTw+9ZX1/vsmh1aM+ePV0W1VP0WH3wwQdd9qc/xZtcR/dvbGx0WSU8VjtKrdREly5dXBY9551wwgkuGzp0aFGZFK+Qi1axRs//J554ostGjBhR1LGjy3NJ8UrDzZs3u+xnP/uZy/7yl7+4LLo0WLVdxqtQTfAOFAAAQEZMoAAAADJiAgUAAJAREygAAICMWnUpl2pRbBP5kCFDwvtHl4l45plnXLZrl79IebR1ftRYvm3bNpdFzY+SdPLJJ7ssauqNbhc1Fq5cudJlq1atCo9NE255iy7fcPXVV7vsi1/8osuix8ugQYPC40TN4dFlHVasWOGyP/7xjy5bsmSJyzZt2uSyaLGEFDemV1uDK1oWXSYoatqOni8vuOACl5122mkui+pEipu+o2b1qE6iLLoMTLQgJLoEjRQ3fUeX94ou5VJLl+cqBu9AAQAAZMQECgAAICMmUAAAABkxgQIAAMioZprI+/Tp47LPfvazLosaBqOGV0l67bXXXLZjx45WjK5Z1DD44Ycfuiza2VySXn75ZZdFDYzRzrpRw+DChQtd9uabb4bHRnkbM2aMy+bOneuyU045xWWFFi1EDhw44LLosbVo0SKXRbuJNzQ0uIwmcBQSLYaRpH79+rksagSfOXOmy84++2yXRQ3j0WuMFDerR03txS54iJrDo/OOalGStm7d6rLVq1eHtz1W165+yhCNJ3otk6qvdnkHCgAAICMmUAAAABkxgQIAAMiICRQAAEBGLTaRm9lISfdLGibpiKT5KaU7zWyQpAcljZa0WdLVKaUP2m+obRM1EV566aUuGz16tMueeOKJ8Hvu2bOnzeNqSdQcOGDAgPC2WZp9j7Vz506XrVmzxmXvv/9+q49RLSqxJqJG1vHjx7usPZo8o8bTqAn3C1/4gsuiMUYNr9HO/lLcMNvY2Oiyw4cPh/dHccqlJgrtvh0tnLn44otdFjWRDxs2zGXRVSwKPYaihuro8Ro910dZdJyoAT26soUUP4ePGzfOZdE5Ll++3GXRYqpo8ZMUP78Um0U/s0LN6h2lmHegDku6JaU0QdI0Sf9qZqdJulXS0ymleklP574GagE1AeSjJlBzWpxApZS2p5Rezn2+V9JaSSMkXSHpvtzN7pN0ZXsNEign1ASQj5pALcq0D5SZjZY0WVKDpKEppe1Sc/GYWXjFXTObJ2le24YJlCdqAshHTaBWFD2BMrO+kh6V9L2UUmOhDcuOlVKaL2l+7ntU1y5aqGnUBJCPmkAtKWoCZWbd1FwUv08p/SEX7zCzutz/Kuok+S7kMhLt/Dpq1Kii7rtp06YwL9Qo11rRGEeOHOmyL33pS+H9zz33XJf17NnTZVETYl1dncvOOOMMl/3tb38Ljx015lazSquJqKE0+reMaiJqzC20kKFXr14umzhxosuiXaCbmppcFjW8Ll682GX79+8PxxPtzr9t2zaXFdq1uS2iBte33nrLZR9//HFR940Ueg7qiAUuxyqHmii0kCZ6fps6darLxo4d67LoMR09h0aPX6lwY/uxon/z6HG9a9cul0X1HV0BQIob06MrFZx++ukuq6+vd9myZctc9s9//jM8djT26DEcvZ5Ej+lid29vLy3+y1rzT/seSWtTSv9x1F89IenG3Oc3Snq89MMDyg81AeSjJlCLinkHaoakGyStMrOVuezHkn4p6SEzmytpi6Svts8QgbJDTQD5qAnUnBYnUCmlZZIK/SL7c6UdDlD+qAkgHzWBWsRO5AAAABll2sag2hS722m0S7cUN39Goqa9Pn36uCzaDfYrX/mKy2644YbwONFu02+88YbLevfu7bLhw4e7bPbs2S575ZVXwmM/++yzLot+Ph3Z4If/sWLFCpd95zvfcdlVV13lsh49erhs8uTJ4XGiXZujxRFRFtXE8ccf77I5c+a4rNBqrw8+8Jte796922VRA2+0E3N0nELNw5Enn3zSZVFTe7E7o0dN8pK0ZMmSosdUTaJ/MyleHBE1lkeLbqLvGTWGF3puix5b+/btc9lHH33ksmjn7+eee85lW7ZscVmhBQbR60y0U/vgwYNd1rdvX5dFzeaFdkGPFlFs3LjRZdHrVnQFgvfee89lha5K0B6vPbwDBQAAkBETKAAAgIyYQAEAAGTEBAoAACAjJlAAAAAZ1cwqvKgDP1ol1rWr/5FEqzWkeJVCtDojWsV09tlnu+yWW25x2YUXXuiyQpedeOaZZ1x2++23u+ykk05y2be+9S2XnXPOOS677bbbwmPffPPNLlu5cqXLopUmrMxrf9ElIaIVlYVWWR6r0GWQoku8RDUVXaJo+vTpLoseg2eeeabLolV9UryaLVr1Fq20GjFiRFG3i1b6SXGdfeMb3whv21p33XVXmNfqKrxCzyXFXhIlul302IqOE11WRIova7Jq1SqXNTQ0uOzpp592WbRqLXpNiF53JGnHjh0uGzRokMui18cJEya4LFrBPXr06PDY0WWdXn/9dZdFqyGjn2NUe4VW4bUH3oECAADIiAkUAABARkygAAAAMmICBQAAkJF1ZAOvmXVat3C0BX3UoH3//fe7LGoMl+LGvWJ/nlGDX9Q4t337dpf9+te/Dr/nwoULXRY1zHbp0sVll1xyicu+//3vu+z8888Pjx1tqX/nnXe67J577nFZoW3/O0JKqdAFUDtEZ9ZEWxSqiUKXVCnm/lGzedSUPnXqVJcVapiNLmcRXRIiag6PGmGj2xV6/I4dOzbMSym65IUUL+AoViXXRK9evcJ8xowZLps1a5bLPvc5f93jaMFD1Ly8dOnS8NiPPvqoy6J/n2Iv71LsZX4K1Wi/fv1cVl9f77LovK+55hqXZVnUEZ3j+vXrXfbnP//ZZU899ZTLotfHQs38bVGoJngHCgAAICMmUAAAABkxgQIAAMioxQmUmY00syVmttbM1pjZzbn838zsn2a2Mvfn8vYfLtD5qAkgHzWBWtRiE7mZ1UmqSym9bGbHS3pJ0pWSrpa0L6X0q6IP1okNs1FDXdSgOnfuXJfNmTMn/J7jxo1zWdTEGDW1RY1zUbPh4sWLXbZu3bpwPHv27HFZsQ2H/fv3d9kZZ5zhstmzZ4f3v/xy/7z42GOPuex3v/udy6KdcTtKaxpmq6UmKkFUt3369HFZoeb1I0eOuCzaYTm6f9QIG92uUI0VaqQtpUINs9E5FquSayJaiCDFO21HWbTTdu/evV0W7QZeqKE/apyOHjPRa3F7LPIq9moZUbP55MmTXRb9zKL7SvFzfdQIvmLFCpdFC5WinePb42dWqCZavJRLSmm7pO25z/ea2VpJ/hoHQI2gJoB81ARqUaYeKDMbLWmypE8u2nOTmb1qZvea2cAC95lnZsvNbHmbRgqUIWoCyEdNoFYUPYEys76SHpX0vZRSo6T/lHSKpElq/p9HuDlRSml+SmlKSmlKCcYLlA1qAshHTaCWFDWBMrNuai6K36eU/iBJKaUdKaWmlNIRSf8lye9uB1QpagLIR02g1hTTRG6S7pO0K6X0vaPyutzvvWVm35d0bkrJb1Oa/73KqmE2aqYbPny4ywrtKBw1ykW7fEeNrI2NjS7bvHmzy9555x2XRTugt1XUHBs1xA8bNiy8f7RrbdQw+Oabb7qsPc6nWK1smK3amgCqsSai5+Xo+b/Y5+8oa2pqCo/dkVf7KKXo5xMtjIiyQs38UfP8oUOHXBYtjoh+5h31s211E7mkGZJukLTKzD7Zf/7Hkq41s0mSkqTNkv6lBOMEKgE1AeSjJlBzilmFt0xSNPv6U+mHA5Q/agLIR02gFrETOQAAQEZMoAAAADJqsYm8pAejYRZlpjUNs6VETaDcUBNAvkI1wTtQAAAAGTGBAgAAyIgJFAAAQEZMoAAAADJiAgUAAJAREygAAICMmEABAABkxAQKAAAgIyZQAAAAGTGBAgAAyIgJFAAAQEZMoAAAADJiAgUAAJAREygAAICMWpxAmVlPM3vRzF4xszVm9u+5fIyZNZjZejN70My6t/9wgc5HTQD5qAnUomLegTog6eKU0lmSJkmaaWbTJN0u6Y6UUr2kDyTNbb9hAmWFmgDyUROoOS1OoFKzfbkvu+X+JEkXS3okl98n6cp2GSFQZqgJIB81gVpUVA+UmXUxs5WSdkpaLGmjpN0ppcO5m2yVNKLAfeeZ2XIzW16KAQPlgJoA8lETqDVFTaBSSk0ppUmSTpI0VdKE6GYF7js/pTQlpTSl9cMEygs1AeSjJlBrMq3CSyntlvSspGmSBphZ19xfnSRpW2mHBpQ/agLIR02gVhSzCu9EMxuQ+7yXpEskrZW0RNJVuZvdKOnx9hokUE6oCSAfNYFaZCmF76j+zw3MzlRz818XNU+4Hkop/dzMxkr6b0mDJK2QNCeldKCF7/XpBwM6WErJst6HmkA1oyaAfIVqosUJVClRGCg3rXmxKCVqAuWGmgDyFaoJdiIHAADIiAkUAABARl1bvklJvSfprdznJ+S+rgacS3lq6VxGddRAPsUnNVFNP3epus6nls6Fmmg/1XQ+tXQuBWuiQ3ug8g5strxa9vzgXMpTJZ1LJY21GNV0PpxL56iksRajms6Hc2nGr/AAAAAyYgIFAACQUWdOoOZ34rFLjXMpT5V0LpU01mJU0/lwLp2jksZajGo6H85FndgDBQAAUKn4FR4AAEBGTKAAAAAy6vAJlJnNNLPXzWyDmd3a0cdvKzO718x2mtnqo7JBZrbYzNbnPg7szDEWy8xGmtkSM1trZmvM7OZcXnHnY2Y9zexFM3sldy7/nsvHmFlD7lweNLPunT3WY1ET5aGa6kGiJjpLtdSDRE20pEMnUGbWRdLvJM2SdJqka83stI4cQwkskDTzmOxWSU+nlOolPZ37uhIclnRLSmmCpGmS/jX371GJ53NA0sUppbMkTZI008ymSbpd0h25c/lA0txOHKNDTZSVaqoHiZroLAtUHfUgUROfqqPfgZoqaUNKaVNK6aCar9J9RQePoU1SSksl7TomvkLNVyJX7uOVHTqoVkopbU8pvZz7fK+ktZJGqALPJzXbl/uyW+5PknSxpEdyeTmeCzVRJqqpHiRqorNUSz1I1ERLOnoCNULS20d9vTWXVbqhKaXtUvMDTtKQTh5PZmY2WtJkSQ2q0PMxsy5mtlLSTkmLJW2UtDuldDh3k3J8vFETZaga6kGiJspIxT6GPkFNeB09gbIgYx+FTmZmfSU9Kul7KaXGzh5Pa6WUmlJKkySdpOb/xU6Ibtaxo2oRNVFmqqUeJGoCpUFNxDp6ArVV0sijvj5J0rYOHkN72GFmdZKU+7izk8dTNDPrpubC+H1K6Q+5uGLPR5JSSrslPavm39kPMLNPLppdjo83aqKMVGM9SNREGajYxxA1UVhHT6D+Iak+1/HeXdI1kp7o4DG0hyck3Zj7/EZJj3fiWIpmZibpHklrU0r/cdRfVdz5mNmJZjYg93kvSZeo+ff1SyRdlbtZOZ4LNVEmqqkeJGqizFTqY4ia+DQppQ79I+lySW+o+feO/6ejj1+C8T8gabukQ2r+n9JcSYPVvBJhfe7joM4eZ5Hn8lk1v1X5qqSVuT+XV+L5SDpT0orcuayW9H9z+VhJL0raIOlhST06e6zB2KmJMvhTTfWQOx9qonPGXhX1kDsXauJT/nApFwAAgIzYiRwAACAjJlAAAAAZMYECAADIiAkUAABARkygAAAAMmICBQAAkBETKAAAgIz+PwdDIIkiSi/TAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 720x1080 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "alpha=1\n",
    "beta = 1.2\n",
    "check(19,11, alpha, beta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Attack(nn.Module):\n",
    "#     def __init__(self, attack_digit=attack_digit, target_digit=target_digit, vae=model, classifier=classifier, avg_latent=l_sample_list):\n",
    "#         super(self, Attack).__init__()\n",
    "#         self.classifier = classifier\n",
    "#         self.classifier.eval()\n",
    "#         self.vae = vae\n",
    "#         self.vae.eval()\n",
    "#         self.avg_latent = avg_latent\n",
    "#         self.attack_digit = attack_digit\n",
    "#         self.target_digit = target_digit\n",
    "#         self.hidden_layers = hidden_layers\n",
    "#         self.hidden_layers.insert(0, latent_dim)\n",
    "#         self.hidden_layers.append(latent_dim)\n",
    "#         self.layers = []\n",
    "        \n",
    "#         for i in range(len(self.hidden_layers)-1):\n",
    "#             self.layers.append(nn.Linear(self.hidden_layers[i], self.hidden_layers[i+1]))\n",
    "        \n",
    "#         self.layers = nn.ModuleList(self.layers)\n",
    "        \n",
    "#     def forward(self, x, y):\n",
    "#         _, l_dist_x = self.vae(x)\n",
    "#         _, l_dist_y = self.vae(y)\n",
    "#         l_sample_x = self.vae.reparameterize(l_dist_x)\n",
    "#         l_sample_y = self.vae.reparameterize(l_dist_y)\n",
    "#         noised_sample = l_sample\n",
    "#         for layer in self.layers:\n",
    "#             noised_sample = layer(noised_sample)\n",
    "#         noised_images = self.vae.decoder(noised_sample)\n",
    "#         preds = self.classifier(F.upsample(noised_image, (28,28), mode='bilinear', align_corners=True))\n",
    "#         return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import exp\n",
    "\n",
    "def gaussian(window_size, sigma):\n",
    "    gauss = torch.Tensor([exp(-(x - window_size//2)**2/float(2*sigma**2)) for x in range(window_size)])\n",
    "    return gauss/gauss.sum()\n",
    "\n",
    "def create_window(window_size, channel):\n",
    "    _1D_window = gaussian(window_size, 1.5).unsqueeze(1)\n",
    "    _2D_window = _1D_window.mm(_1D_window.t()).float().unsqueeze(0).unsqueeze(0)\n",
    "    window = torch.Tensor(_2D_window.expand(channel, 1, window_size, window_size).contiguous())\n",
    "    return window\n",
    "\n",
    "def _ssim(img1, img2, window, window_size, channel, size_average = True):\n",
    "    mu1 = F.conv2d(img1, window, padding = window_size//2, groups = channel)\n",
    "    mu2 = F.conv2d(img2, window, padding = window_size//2, groups = channel)\n",
    "\n",
    "    mu1_sq = mu1.pow(2)\n",
    "    mu2_sq = mu2.pow(2)\n",
    "    mu1_mu2 = mu1*mu2\n",
    "\n",
    "    sigma1_sq = F.conv2d(img1*img1, window, padding = window_size//2, groups = channel) - mu1_sq\n",
    "    sigma2_sq = F.conv2d(img2*img2, window, padding = window_size//2, groups = channel) - mu2_sq\n",
    "    sigma12 = F.conv2d(img1*img2, window, padding = window_size//2, groups = channel) - mu1_mu2\n",
    "\n",
    "    C1 = 0.01**2\n",
    "    C2 = 0.03**2\n",
    "\n",
    "    ssim_map = ((2*mu1_mu2 + C1)*(2*sigma12 + C2))/((mu1_sq + mu2_sq + C1)*(sigma1_sq + sigma2_sq + C2))\n",
    "\n",
    "    if size_average:\n",
    "        return ssim_map.mean()\n",
    "    else:\n",
    "        return ssim_map.mean(1).mean(1).mean(1)\n",
    "\n",
    "class SSIM(nn.Module):\n",
    "    def __init__(self, window_size = 11, size_average = True):\n",
    "        super(SSIM, self).__init__()\n",
    "        self.window_size = window_size\n",
    "        self.size_average = size_average\n",
    "        self.channel = 1\n",
    "        self.window = create_window(window_size, self.channel)\n",
    "\n",
    "    def forward(self, img1, img2):\n",
    "        (_, channel, _, _) = img1.size()\n",
    "\n",
    "        if channel == self.channel and self.window.data.type() == img1.data.type():\n",
    "            window = self.window\n",
    "        else:\n",
    "            window = create_window(self.window_size, channel)\n",
    "            \n",
    "            if img1.is_cuda:\n",
    "                window = window.cuda(img1.get_device())\n",
    "            window = window.type_as(img1)\n",
    "            \n",
    "            self.window = window\n",
    "            self.channel = channel\n",
    "\n",
    "\n",
    "        return _ssim(img1, img2, window, self.window_size, channel, self.size_average)\n",
    "\n",
    "def ssim(img1, img2, window_size = 11, size_average = True):\n",
    "    (_, channel, _, _) = img1.size()\n",
    "    window = create_window(window_size, channel)\n",
    "    \n",
    "    if img1.is_cuda:\n",
    "        window = window.cuda(img1.get_device())\n",
    "    window = window.type_as(img1)\n",
    "    \n",
    "    return _ssim(img1, img2, window, window_size, channel, size_average)\n",
    "\n",
    "ssim_loss = SSIM(window_size = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from skimage.metrics import structural_similarity as ssim\n",
    "\n",
    "use_cuda = True\n",
    "device = torch.device('cuda' if use_cuda else 'cpu')\n",
    "\n",
    "avg_latent = torch.load('tensor/latent.pt')\n",
    "\n",
    "def create_logits(target_label, pred, confidence=0.58, device=device):\n",
    "    logits = torch.zeros(pred.shape, dtype=torch.float64)\n",
    "    pred_labels = [int(element.item()) for element in torch.argmax(pred, dim=1)]\n",
    "#     print(\"length\", len(pred_labels))\n",
    "#     print(\"pred_lables:\",pred_labels)\n",
    "#     print(logits.shape)\n",
    "    logits[range(logits.shape[0]), pred_labels] = torch.DoubleTensor([1-confidence]*pred.shape[0])\n",
    "    logits[range(logits.shape[0]), [target_label]*pred.shape[0]] += torch.DoubleTensor([confidence]*pred.shape[0])\n",
    "    return logits.to(device)\n",
    "\n",
    "def structural(org_image, noised_image):\n",
    "    batch_size, channels, width, height = org_image.shape\n",
    "    loss1 = 0\n",
    "    for b_ in range(batch_size):\n",
    "        ch_loss = 0\n",
    "        for ch_ in range(channels):\n",
    "            ch_loss += 1-ssim(org_image[b_][ch_].detach().cpu().numpy(), noised_image[b_][ch_].detach().cpu().numpy())\n",
    "        loss1 += ch_loss/channels\n",
    "    return loss1\n",
    "            \n",
    "class T_Loss(nn.Module):\n",
    "    def __init__(self, decoder=model.decode, classifier=classifier,latent_dim=20, avg_latent=avg_latent, classes=len(train_data.classes)):\n",
    "        super(T_Loss, self).__init__()\n",
    "        self.decoder = decoder\n",
    "        self.classifier = classifier\n",
    "        self.avg_latent = avg_latent\n",
    "        self.classes = classes\n",
    "        self.latent_dim = latent_dim\n",
    "    def forward(self, coff, org_x, target_label):\n",
    "        \n",
    "        org_image = self.decoder(org_x)\n",
    "        noise = 0.01*torch.transpose(coff[:,None].cuda()*self.avg_latent.T, 1, 2).sum(1)\n",
    "#         final = torch.zeros(org_x.shape)\n",
    "#         for i in range(final.shape[0]):\n",
    "#             z = torch.zeros((1, self.latent_dim))\n",
    "#             count = 0\n",
    "#             for j in coff[i,:].tolist():\n",
    "#                 z += j * self.avg_latent[count].cpu()\n",
    "#                 count += 1\n",
    "#             final[i] = z\n",
    "        noised_sample = org_x +  noise.cuda()\n",
    "        noised_image = self.decoder(noised_sample)\n",
    "        batch_size, channels, width, height = org_image.shape\n",
    "        loss1 = ssim_loss(org_image, noised_image)\n",
    "#         print(type(loss1))\n",
    "#         print(loss1)\n",
    "#         loss1 = torch.Tensor(loss1 / batch_size).to(device)\n",
    "#         print(\"loss1:\", loss1)\n",
    "        preds = self.classifier(F.upsample(noised_image, (28,28), mode='bilinear', align_corners=True))\n",
    "\n",
    "        target = create_logits(target_label, preds)\n",
    "\n",
    "        loss2 = nn.BCELoss(reduction='sum')(preds, target.float())\n",
    "\n",
    "        loss = 500*(1-loss1) + 2*loss2\n",
    "        \n",
    "        \n",
    "        out_labels = preds.argmax(dim=1, keepdim=True)\n",
    "#         print(out_labels)\n",
    "#         print(torch.empty(out_labels.shape).fill_(target_label))\n",
    "#         print(preds)\n",
    "#         correct = out_labels.eq(torch.Tensor([target_label]*out_labels.shape[0]).to(device)).sum()\n",
    "        correct = out_labels.eq(torch.empty(out_labels.shape).fill_(target_label).to(device)).sum()\n",
    "#         print(out_labels.shape)\n",
    "#         print(correct)\n",
    "#         print(torch.Tensor([target_label]*out_labels.shape[0]))\n",
    "#         print(out_labels)\n",
    "        return loss, correct.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cuda = True\n",
    "device = torch.device('cuda' if use_cuda else 'cpu')\n",
    "\n",
    "# Constrained Translator\n",
    "class Translator(nn.Module):\n",
    "    def __init__(self, hidden_layers=[5, 10, len(train_data.classes)], latent_dim=20):\n",
    "        super(Translator, self).__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "        self.hidden_layers = hidden_layers\n",
    "        self.hidden_layers.insert(0, latent_dim)\n",
    "        self.layers = []\n",
    "        \n",
    "        for i in range(len(self.hidden_layers)-1):\n",
    "            self.layers.append(nn.Linear(self.hidden_layers[i], self.hidden_layers[i+1]))\n",
    "        \n",
    "        self.layers = nn.ModuleList(self.layers)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        l_sample = x\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        coff = x\n",
    "#         final = torch.zeros((x.shape[0], self.latent_dim))\n",
    "#         for i in range(final.shape[0]):\n",
    "#             z = torch.zeros((1, self.latent_dim))\n",
    "#             count = 0\n",
    "#             for j in coff[i,:].tolist():\n",
    "#                 z += j * self.avg_latent[count].cpu()\n",
    "#                 count += 1\n",
    "#             final[i] = z\n",
    "#         noised_sample = l_sample +  final.cuda()\n",
    "#         print(coff)\n",
    "        return coff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "T_Loss(\n",
       "  (classifier): Classifier(\n",
       "    (conv1): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (dropout1): Dropout2d(p=0.25, inplace=False)\n",
       "    (dropout2): Dropout2d(p=0.5, inplace=False)\n",
       "    (fc1): Linear(in_features=9216, out_features=128, bias=True)\n",
       "    (fc2): Linear(in_features=128, out_features=10, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translator = Translator().to(device)\n",
    "tloss = T_Loss().to(device)\n",
    "tloss.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d967885a223749608279a6ad5ecb7839",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=50.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1\tLoss: 1499.971061\tCorrect: 5488\n",
      "Train Epoch: 2\tLoss: 1472.341813\tCorrect: 5490\n",
      "Train Epoch: 3\tLoss: 1443.841470\tCorrect: 5490\n"
     ]
    }
   ],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "attack_log_interval = 1\n",
    "alt_target = 5\n",
    "translator.train()\n",
    "optimizer = optim.Adam(translator.parameters(), lr=1e-5)\n",
    "for epoch in tqdm(range(50)):\n",
    "    epoch_loss = 0\n",
    "    epoch_correct = 0\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "#         print(target)\n",
    "        data = torch.FloatTensor(data).to(device)\n",
    "        \n",
    "        _, l_dist = model(data)\n",
    "        l_sample = model.reparameterize(l_dist)\n",
    "        \n",
    "        coff = translator(l_sample)\n",
    "        loss, correct = tloss(coff, l_sample, alt_target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "#         print(correct)\n",
    "        epoch_loss += loss.item()\n",
    "        epoch_correct += correct\n",
    "        \n",
    "    if (epoch+1) % attack_log_interval == 0:\n",
    "        print('Train Epoch: {}\\tLoss: {:.6f}\\tCorrect: {}'.format(\n",
    "            epoch+1, epoch_loss/batch_idx, epoch_correct))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6833\n",
      "Accuracy:  68.33\n"
     ]
    }
   ],
   "source": [
    "translator.eval()\n",
    "total_correct = 0\n",
    "total_test = 0\n",
    "for batch_idx, (data, target) in enumerate(test_loader):\n",
    "    total_test += data.shape[0]\n",
    "    data = torch.FloatTensor(data).to(device)\n",
    "\n",
    "    _, l_dist = model(data)\n",
    "    l_sample = model.reparameterize(l_dist)\n",
    "\n",
    "    coff = translator(l_sample)\n",
    "    loss, correct = tloss(coff, l_sample, alt_target)\n",
    "    total_correct += correct\n",
    "#     print(correct)\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "#         print(correct)\n",
    "#         epoch_loss += loss.item()\n",
    "    \n",
    "\n",
    "#     if (epoch+1) % attack_log_interval == 0:\n",
    "#         print('Train Epoch: \\tCorrect: {}'.format(\n",
    "#             epoch, epoch_correct))\n",
    "print(total_correct)\n",
    "print(\"Accuracy: \", 100*(total_correct/total_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(i):\n",
    "    _, l_dist = model(example_data[i].unsqueeze_(0).to(device))\n",
    "    l_sample = model.reparameterize(l_dist)\n",
    "    coff = translator(l_sample)\n",
    "    print(l_sample.shape)\n",
    "    print(torch.transpose(coff[:,None].cuda()*avg_latent.T, 1, 2).sum(1).shape)\n",
    "    noised_latent = l_sample + 0.2*torch.transpose(coff[:,None].cuda()*avg_latent.T, 1, 2).sum(1)\n",
    "    print(noised_latent.shape)\n",
    "#     print(noised_sample)\n",
    "#     print(l_sample)\n",
    "#     noised_sample = 1 * ((l_sample - l_sample.min())/(l_sample.max() - l_sample.min())) + 1e-2 * ((noised_sample - noised_sample.min())/(noised_sample.max() - noised_sample.min()))\n",
    "#     noised_sample = 1 * l_sample + 2e-2 * noised_sample\n",
    "#     noised_sample = l_sample + 1e-7 * noised_sample\n",
    "    final = model.decode(noised_latent)\n",
    "    pred_org = torch.argmax(classifier(F.upsample(example_data[i,:,:,:].unsqueeze(0).cuda(), (28,28), mode='bilinear', align_corners=True)))\n",
    "    pred = torch.argmax(classifier(F.upsample(final, (28,28), mode='bilinear', align_corners=True)))\n",
    "    print(\"Prediction: {}, {}\".format(pred_org.item(), pred.item()))\n",
    "    plt.figure(figsize=(10,10))\n",
    "    plt.subplot(1,2,1)\n",
    "    plt.imshow(example_data[i][0].detach().cpu().numpy(), cmap='gray', interpolation='none')\n",
    "    plt.subplot(1,2,2)\n",
    "    plt.imshow(final[0][0].detach().cpu().numpy(), cmap='gray', interpolation='none')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 20])\n",
      "torch.Size([1, 20])\n",
      "torch.Size([1, 20])\n",
      "Prediction: 4, 5\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "test(40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets, transforms\n",
    "all_transforms = transforms.Compose([\n",
    "        transforms.Resize(32),\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "train_data = datasets.MNIST('/home/data/bvaa/', train=True, download=True,\n",
    "                                transform=all_transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_index = {}\n",
    "for i in range(len(train_data)):\n",
    "    index = train_data[i][1]\n",
    "    if index not in list_index.keys():\n",
    "        list_index[index] = [i]\n",
    "    else:\n",
    "        list_index[index].append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7fa8d07ca5a545aa83bd45d12fd2ab18",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=10.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "l_sample_list = {}\n",
    "def get_average_latent_space(list_index):\n",
    "    for i in tqdm(list_index.keys()):\n",
    "        for j in list_index[i]:\n",
    "            output, l_dist = model(train_data[j][0].unsqueeze(0).cuda())\n",
    "            l_sample_x = model.reparameterize(l_dist)\n",
    "            if i not in l_sample_list.keys():\n",
    "                l_sample_list[i] = [l_sample_x]\n",
    "            else:\n",
    "                l_sample_list[i].append(l_sample_x)\n",
    "        l_sample_list[i] = torch.mean(torch.stack(l_sample_list[i]), dim=0)\n",
    "        \n",
    "get_average_latent_space(list_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = []\n",
    "for i in l_sample_list.keys():\n",
    "    l.append(l_sample_list[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "l_tensor = torch.stack(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = l_tensor.squeeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 20])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(l, 'tensor/latent.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "e = torch.load('tensor/latent.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "coff = torch.randn((2,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 10])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coff.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = coff[:,None].cuda()*l.T\n",
    "r = torch.transpose(m,1,2) \n",
    "r.sum(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = torch.transpose(m,1,2)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 10, 20])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 20])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r.sum(1).shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
